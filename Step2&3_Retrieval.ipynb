{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garima24112000/Scaling-Retrieval-System/blob/main/Step2%263_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 pandas==2.1.4 pyarrow==14.0.2 tqdm pyyaml -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install faiss-cpu>=1.12.0 -q\n",
        "!pip install pyspark==3.5.1 -q\n",
        "!pip install chromadb==0.4.24 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt1xODxWUwPp",
        "outputId": "b208231e-f530-4a51-847c-942d6344b31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.11.0 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires pyarrow>=15.0.0; platform_machine == \"x86_64\", but you have pyarrow 14.0.2 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "datasets 4.0.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.2 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "bigframes 2.29.1 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m145.2/145.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "datasets 4.0.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.2 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "bigframes 2.29.1 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "HqT7XwkeUxcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, pandas, pyarrow, faiss, pyspark\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "JTb5rVyZU0-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount Drive + set ONE project root**"
      ],
      "metadata": {
        "id": "xiBBGiUO4ubf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-6q20hX2BEr",
        "outputId": "f57f5a0a-2ceb-46f9-8c59-622776e4701b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/AMS 560 PROJECT\n",
            "Project root set to: /content/drive/MyDrive/AMS 560 PROJECT\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define ONE project root for everything (edit if your folder is different)\n",
        "ROOT = \"/content/drive/MyDrive/AMS 560 PROJECT\"\n",
        "\n",
        "# Move into your project root\n",
        "%cd $ROOT\n",
        "\n",
        "print(\"Project root set to:\", ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install required dependencies**"
      ],
      "metadata": {
        "id": "xBQuCr3i2dbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install only what's required for retrieval (no embedding, no Spark here)\n",
        "!pip install -q sentence-transformers faiss-cpu pandas pyarrow\n",
        "\n"
      ],
      "metadata": {
        "id": "-PhtNWSK2dzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create necessary folders**"
      ],
      "metadata": {
        "id": "U54T_ErW2wEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create config, results, and query folders if they don't already exist\n",
        "import os\n",
        "os.makedirs(os.path.join(ROOT, \"configs/index\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(ROOT, \"results\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(ROOT, \"data/queries\"), exist_ok=True)\n",
        "\n",
        "print(\"Folders ready under\", ROOT)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3vTcQAP2vXF",
        "outputId": "2b3c5968-99b3-42c5-9c6c-b3f839e4cb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders ready under /content/drive/MyDrive/AMS 560 PROJECT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write config.json (points to saved FAISS + meta)**"
      ],
      "metadata": {
        "id": "CFkAB8AN3AU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a config dictionary using ROOT so paths never break\n",
        "import os, json\n",
        "\n",
        "cfg = {\n",
        "  \"active_index\": \"multi_default\",\n",
        "  \"indices\": {\n",
        "    \"multi_default\": {\n",
        "      \"faiss_path\": os.path.join(ROOT, \"indices/multi_default/multi_hnsw.faiss\"),\n",
        "      \"meta_path\":  os.path.join(ROOT, \"data/embeddings/multi_default/multi_meta.parquet\"),\n",
        "      \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
        "      \"metric\": \"ip\",\n",
        "      \"normalize_query\": True\n",
        "    },\n",
        "    \"single_wiki\": {\n",
        "      # ğŸ‘‡ edit these if your wiki index/meta are elsewhere\n",
        "      \"faiss_path\": os.path.join(ROOT, \"indices/wikipedia/wiki_hnsw_M32.faiss\"),\n",
        "      \"meta_path\":  os.path.join(ROOT, \"data/embeddings/wikipedia/wiki_meta.parquet\"),\n",
        "      \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
        "      \"metric\": \"ip\",\n",
        "      \"normalize_query\": True\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "config_path = os.path.join(ROOT, \"configs/index/config.json\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(\" Wrote config to:\", config_path)\n",
        "print(json.dumps(cfg, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqiR1qV-3AwY",
        "outputId": "59537417-2541-4ada-e3eb-23cbea13f08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Wrote config to: /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n",
            "{\n",
            "  \"active_index\": \"multi_default\",\n",
            "  \"indices\": {\n",
            "    \"multi_default\": {\n",
            "      \"faiss_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\",\n",
            "      \"meta_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default/multi_meta.parquet\",\n",
            "      \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "      \"metric\": \"ip\",\n",
            "      \"normalize_query\": true\n",
            "    },\n",
            "    \"single_wiki\": {\n",
            "      \"faiss_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/indices/wikipedia/wiki_hnsw_M32.faiss\",\n",
            "      \"meta_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/wikipedia/wiki_meta.parquet\",\n",
            "      \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "      \"metric\": \"ip\",\n",
            "      \"normalize_query\": true\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat configs/index/config.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qr9sPsD3OCL",
        "outputId": "a3590ef9-05c2-4f7d-df9f-68850bdfc458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"active_index\": \"multi_default\",\n",
            "  \"indices\": {\n",
            "    \"multi_default\": {\n",
            "      \"faiss_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\",\n",
            "      \"meta_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default/multi_meta.parquet\",\n",
            "      \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "      \"metric\": \"ip\",\n",
            "      \"normalize_query\": true\n",
            "    },\n",
            "    \"single_wiki\": {\n",
            "      \"faiss_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/indices/wikipedia/wiki_hnsw_M32.faiss\",\n",
            "      \"meta_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/wikipedia/wiki_meta.parquet\",\n",
            "      \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "      \"metric\": \"ip\",\n",
            "      \"normalize_query\": true\n",
            "    }\n",
            "  }\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write retrieval_runner.py**"
      ],
      "metadata": {
        "id": "lLDl5qY45b6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This script loads model + FAISS + metadata and runs queries.\n",
        "# It does NOT re-embed or rebuild indices.\n",
        "%%writefile retrieval_runner.py\n",
        "import os, json, argparse, time\n",
        "import numpy as np, pandas as pd\n",
        "try:\n",
        "    import faiss\n",
        "except Exception:\n",
        "    import faiss_cpu as faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class RetrievalRunner:\n",
        "    \"\"\"Loads FAISS + metadata and returns search results.\"\"\"\n",
        "    def __init__(self, config_path: str, index_key: str | None = None):\n",
        "        # Load config\n",
        "        with open(config_path, \"r\") as f:\n",
        "            cfg = json.load(f)\n",
        "        index_key = index_key or cfg.get(\"active_index\")\n",
        "        if not index_key or index_key not in cfg[\"indices\"]:\n",
        "            raise ValueError(f\"Index key '{index_key}' missing or not found in config.\")\n",
        "        self.icfg = cfg[\"indices\"][index_key]\n",
        "\n",
        "        # 1) Load FAISS index\n",
        "        if not os.path.exists(self.icfg[\"faiss_path\"]):\n",
        "            raise FileNotFoundError(f\"FAISS index not found: {self.icfg['faiss_path']}\")\n",
        "        self.index = faiss.read_index(self.icfg[\"faiss_path\"])\n",
        "        print(f\"Loaded FAISS index: {self.icfg['faiss_path']}\")\n",
        "\n",
        "        # 2) Load metadata\n",
        "        if not os.path.exists(self.icfg[\"meta_path\"]):\n",
        "            raise FileNotFoundError(f\"Meta parquet not found: {self.icfg['meta_path']}\")\n",
        "        self.meta  = pd.read_parquet(self.icfg[\"meta_path\"])\n",
        "        print(f\"Loaded metadata rows: {len(self.meta)}\")\n",
        "\n",
        "        # 3) Load query encoder\n",
        "        self.model = SentenceTransformer(self.icfg.get(\"model_name\", \"BAAI/bge-small-en-v1.5\"))\n",
        "        print(f\"Loaded model: {self.icfg.get('model_name')}\")\n",
        "\n",
        "        # Search settings\n",
        "        self.normalize_query = bool(self.icfg.get(\"normalize_query\", True))\n",
        "        try:\n",
        "            faiss.ParameterSpace().set_index_parameter(self.index, \"efSearch\", 64)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def _encode(self, q: str) -> np.ndarray:\n",
        "        \"\"\"Encode text query to a vector.\"\"\"\n",
        "        if not q or not q.strip():\n",
        "            raise ValueError(\"Empty query.\")\n",
        "        vec = self.model.encode([q], normalize_embeddings=self.normalize_query).astype(\"float32\")\n",
        "        return vec\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5, filter_source: str | None = None) -> list[dict]:\n",
        "        \"\"\"\n",
        "        Returns list of dicts:\n",
        "        rank, score, id, source, title, url, text, chunk_id\n",
        "        \"\"\"\n",
        "        qv = self._encode(query)\n",
        "        overfetch = top_k * 4 if filter_source else top_k\n",
        "        D, I = self.index.search(qv, overfetch)\n",
        "        results, taken = [], 0\n",
        "\n",
        "        for idx, score in zip(I[0], D[0]):\n",
        "            if idx < 0:\n",
        "                continue\n",
        "            row = self.meta.iloc[idx]\n",
        "            source = row.get(\"domain\", \"\") or row.get(\"source\", \"\")\n",
        "            if filter_source and source != filter_source:\n",
        "                continue\n",
        "            results.append({\n",
        "                \"rank\": taken + 1,\n",
        "                \"score\": float(score),\n",
        "                \"id\": row.get(\"id\", \"\"),\n",
        "                \"source\": source,\n",
        "                \"title\": row.get(\"title\", \"\"),\n",
        "                \"url\": row.get(\"url\", \"\"),\n",
        "                \"text\": row.get(\"text\", row.get(\"chunk_text\", \"\")),\n",
        "                \"chunk_id\": row.get(\"chunk_id\", None)\n",
        "            })\n",
        "            taken += 1\n",
        "            if taken >= top_k:\n",
        "                break\n",
        "        return results\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--config\", required=True, help=\"Path to configs/index/config.json\")\n",
        "    ap.add_argument(\"--index\", default=None, help=\"Index key to use (overrides active_index)\")\n",
        "    ap.add_argument(\"--query\", default=None, help=\"Single query string\")\n",
        "    ap.add_argument(\"--qfile\", default=None, help=\"Optional newline-delimited queries file\")\n",
        "    ap.add_argument(\"--top_k\", type=int, default=5)\n",
        "    ap.add_argument(\"--filter_source\", default=None, help=\"e.g., wikipedia | pubmed | stackexchange\")\n",
        "    ap.add_argument(\"--out_csv\", default=None, help=\"If set, write results to CSV at this path\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    rr = RetrievalRunner(args.config, args.index)\n",
        "\n",
        "    # collect queries\n",
        "    queries = []\n",
        "    if args.query:\n",
        "        queries.append(args.query)\n",
        "    if args.qfile and os.path.exists(args.qfile):\n",
        "        with open(args.qfile) as f:\n",
        "            queries += [ln.strip() for ln in f if ln.strip()]\n",
        "    if not queries:\n",
        "        raise SystemExit(\"Provide --query or --qfile\")\n",
        "\n",
        "    # run\n",
        "    t0 = time.time()\n",
        "    rows = []\n",
        "    for q in queries:\n",
        "        hits = rr.search(q, top_k=args.top_k, filter_source=args.filter_source)\n",
        "        for h in hits:\n",
        "            rows.append({\"query\": q, **h})\n",
        "    dt = time.time() - t0\n",
        "    print(f\"ğŸ•’ Ran {len(queries)} queries in {dt:.3f}s; avg {dt/len(queries):.3f}s/query\")\n",
        "\n",
        "    # output\n",
        "    if args.out_csv:\n",
        "        os.makedirs(os.path.dirname(args.out_csv), exist_ok=True)\n",
        "        pd.DataFrame(rows).to_csv(args.out_csv, index=False)\n",
        "        print(\"ğŸ’¾ Saved:\", args.out_csv)\n",
        "    else:\n",
        "        for r in rows:\n",
        "            print(f\"\\nQ: {r['query']}\")\n",
        "            print(f\"{r['rank']}. [{r['source']}] {r['title']} (score={r['score']:.4f})\")\n",
        "            if r.get(\"url\"):\n",
        "                print(r[\"url\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhmjDkbK3SBg",
        "outputId": "51be6587-b1ea-44ad-c1ab-04c27f15ce79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting retrieval_runner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quick sanity check: show config and file presence**"
      ],
      "metadata": {
        "id": "LRN1-9d-3kt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "config_path = os.path.join(ROOT, \"configs/index/config.json\")\n",
        "print(\"Config exists:\", os.path.exists(config_path))\n",
        "print(\"Config path:\", config_path)\n",
        "with open(config_path) as f:\n",
        "    print(f.read()[:400], \"...\\n\")\n",
        "\n",
        "# Also confirm index/meta exist (prints True/False)\n",
        "import json\n",
        "cfg = json.load(open(config_path))\n",
        "icfg = cfg[\"indices\"][cfg[\"active_index\"]]\n",
        "print(\"Index exists:\", os.path.exists(icfg[\"faiss_path\"]), \"â†’\", icfg[\"faiss_path\"])\n",
        "print(\"Meta exists :\", os.path.exists(icfg[\"meta_path\"]), \"â†’\", icfg[\"meta_path\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6RzBY6O3lMs",
        "outputId": "25e8e230-488f-4c3e-8fdd-79c7a97c24ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config exists: True\n",
            "Config path: /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n",
            "{\n",
            "  \"active_index\": \"multi_default\",\n",
            "  \"indices\": {\n",
            "    \"multi_default\": {\n",
            "      \"faiss_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\",\n",
            "      \"meta_path\": \"/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default/multi_meta.parquet\",\n",
            "      \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "      \"metric\": \"ip\",\n",
            "      \"normalize_query\": true\n",
            "    },\n",
            "    \"sing ...\n",
            "\n",
            "Index exists: True â†’ /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\n",
            "Meta exists : True â†’ /content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default/multi_meta.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Verify artifacts + alignment (quick guardrails)**"
      ],
      "metadata": {
        "id": "-p19QgsY5wJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alignment check: FAISS vectors == meta rows\n",
        "import json, os, pandas as pd\n",
        "import faiss\n",
        "\n",
        "config_path = os.path.join(ROOT, \"configs/index/config.json\")\n",
        "cfg = json.load(open(config_path))\n",
        "icfg = cfg[\"indices\"][cfg[\"active_index\"]]\n",
        "\n",
        "index = faiss.read_index(icfg[\"faiss_path\"])\n",
        "meta  = pd.read_parquet(icfg[\"meta_path\"])\n",
        "\n",
        "print(\"FAISS ntotal:\", index.ntotal)\n",
        "print(\"META rows  :\", len(meta))\n",
        "assert index.ntotal == len(meta), \"Mismatch! Index and meta are not row-aligned.\"\n",
        "print(\"Alignment OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgT3GQ-g64Vq",
        "outputId": "d571ea2b-37f7-4ffb-b965-4839b89b1df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS ntotal: 79164\n",
            "META rows  : 79164\n",
            "Alignment OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run merged (no-filter) tests that hit all 3 sources**"
      ],
      "metadata": {
        "id": "d9yOP6Ng6DvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a balanced 12-query set likely to pull from wikipedia, pubmed, stackexchange\n",
        "import os, textwrap\n",
        "allq_path = os.path.join(ROOT, \"data/queries/all_sources_eval.txt\")\n",
        "qtext = textwrap.dedent(\"\"\"\\\n",
        "metformin mechanism of action\n",
        "adverse effects of statins\n",
        "type 2 diabetes lifestyle interventions evidence\n",
        "randomized controlled trial definition\n",
        "How to fix ModuleNotFoundError in Python\n",
        "SQL window function ROW_NUMBER example\n",
        "difference between git merge and rebase\n",
        "What is a memory leak and how to detect it\n",
        "Explain PageRank algorithm\n",
        "definition of entropy in information theory\n",
        "history of backpropagation\n",
        "What is overfitting vs underfitting\n",
        "\"\"\")\n",
        "open(allq_path, \"w\").write(qtext)\n",
        "print(\" Wrote:\", allq_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QvOCRq-6-75",
        "outputId": "bbf91e86-294a-421d-ece5-9787363194a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Wrote: /content/drive/MyDrive/AMS 560 PROJECT/data/queries/all_sources_eval.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run retrieval on merged index, save top-10 per query\n",
        "all_csv = os.path.join(ROOT, \"results/multi_default/all_sources_top10.csv\")\n",
        "!python retrieval_runner.py \\\n",
        "  --config \"configs/index/config.json\" \\\n",
        "  --index multi_default \\\n",
        "  --qfile \"data/queries/all_sources_eval.txt\" \\\n",
        "  --top_k 10 \\\n",
        "  --out_csv \"{all_csv}\"\n",
        "print(\" Saved:\", all_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvP4QGFl7GC7",
        "outputId": "e4c6b4ad-6587-4dd4-b106-b9209165c442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-04 16:51:43.710595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867103.731710    1596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867103.738214    1596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867103.754378    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867103.754405    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867103.754408    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867103.754410    1596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Loaded FAISS index: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\n",
            "Loaded metadata rows: 79164\n",
            "modules.json: 100% 349/349 [00:00<00:00, 2.57MB/s]\n",
            "config_sentence_transformers.json: 100% 124/124 [00:00<00:00, 803kB/s]\n",
            "README.md: 94.8kB [00:00, 120MB/s]\n",
            "sentence_bert_config.json: 100% 52.0/52.0 [00:00<00:00, 300kB/s]\n",
            "config.json: 100% 743/743 [00:00<00:00, 5.79MB/s]\n",
            "model.safetensors: 100% 133M/133M [00:01<00:00, 81.2MB/s]\n",
            "tokenizer_config.json: 100% 366/366 [00:00<00:00, 3.24MB/s]\n",
            "vocab.txt: 232kB [00:00, 78.3MB/s]\n",
            "tokenizer.json: 711kB [00:00, 160MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 1.14MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.36MB/s]\n",
            "Loaded model: BAAI/bge-small-en-v1.5\n",
            "ğŸ•’ Ran 12 queries in 0.874s; avg 0.073s/query\n",
            "ğŸ’¾ Saved: /content/drive/MyDrive/AMS 560 PROJECT/results/multi_default/all_sources_top10.csv\n",
            " Saved: /content/drive/MyDrive/AMS 560 PROJECT/results/multi_default/all_sources_top10.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#quick coverage report (proof all three domains appear)"
      ],
      "metadata": {
        "id": "Hr-NjyO37bSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, os\n",
        "all_csv = os.path.join(ROOT, \"results/multi_default/all_sources_top10.csv\")\n",
        "df = pd.read_csv(all_csv)\n",
        "\n",
        "print(\"Rows:\", len(df))\n",
        "print(\"\\nHits by source:\")\n",
        "print(df[\"source\"].value_counts(dropna=False))\n",
        "\n",
        "print(\"\\nFirst hit per query:\")\n",
        "firsts = (df.sort_values([\"query\",\"rank\"])\n",
        "            .groupby(\"query\")\n",
        "            .first()[[\"source\",\"title\",\"url\"]])\n",
        "display(firsts.head(20))\n"
      ],
      "metadata": {
        "id": "vcMim3C_7cN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "6c33813b-7808-4c99-b617-00b028bb3997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 120\n",
            "\n",
            "Hits by source:\n",
            "source\n",
            "stackexchange    46\n",
            "wikipedia        42\n",
            "pubmed           32\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First hit per query:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                         source  \\\n",
              "query                                                             \n",
              "Explain PageRank algorithm                            wikipedia   \n",
              "How to fix ModuleNotFoundError in Python          stackexchange   \n",
              "SQL window function ROW_NUMBER example            stackexchange   \n",
              "What is a memory leak and how to detect it            wikipedia   \n",
              "What is overfitting vs underfitting               stackexchange   \n",
              "adverse effects of statins                               pubmed   \n",
              "definition of entropy in information theory           wikipedia   \n",
              "difference between git merge and rebase           stackexchange   \n",
              "history of backpropagation                            wikipedia   \n",
              "metformin mechanism of action                            pubmed   \n",
              "randomized controlled trial definition                   pubmed   \n",
              "type 2 diabetes lifestyle interventions evidence         pubmed   \n",
              "\n",
              "                                                                                              title  \\\n",
              "query                                                                                                 \n",
              "Explain PageRank algorithm                                                            Search engine   \n",
              "How to fix ModuleNotFoundError in Python               Is my tkcalendar clashing with ttkbootstrap?   \n",
              "SQL window function ROW_NUMBER example            How to pass fieldnames containing # to createM...   \n",
              "What is a memory leak and how to detect it                                          Buffer overflow   \n",
              "What is overfitting vs underfitting               Why does my logistic curve_fit prediction retu...   \n",
              "adverse effects of statins                        Updated Evidence on the Protective Role of Sta...   \n",
              "definition of entropy in information theory                                   Kolmogorov complexity   \n",
              "difference between git merge and rebase           Is there any advantage to updating a branch th...   \n",
              "history of backpropagation                                                  Artificial intelligence   \n",
              "metformin mechanism of action                     The beneficial effects of empagliflozin-metfor...   \n",
              "randomized controlled trial definition            Lessons learned from two multicentre randomise...   \n",
              "type 2 diabetes lifestyle interventions evidence  The role of lifestyle factors in type 2 diabet...   \n",
              "\n",
              "                                                                                                url  \n",
              "query                                                                                                \n",
              "Explain PageRank algorithm                            https://en.wikipedia.org/wiki/Search%20engine  \n",
              "How to fix ModuleNotFoundError in Python          https://stackoverflow.com/questions/79751755/i...  \n",
              "SQL window function ROW_NUMBER example            https://stackoverflow.com/questions/79768985/h...  \n",
              "What is a memory leak and how to detect it          https://en.wikipedia.org/wiki/Buffer%20overflow  \n",
              "What is overfitting vs underfitting               https://stackoverflow.com/questions/79748043/w...  \n",
              "adverse effects of statins                                https://pubmed.ncbi.nlm.nih.gov/41170235/  \n",
              "definition of entropy in information theory       https://en.wikipedia.org/wiki/Kolmogorov%20com...  \n",
              "difference between git merge and rebase           https://stackoverflow.com/questions/79724991/i...  \n",
              "history of backpropagation                        https://en.wikipedia.org/wiki/Artificial%20int...  \n",
              "metformin mechanism of action                             https://pubmed.ncbi.nlm.nih.gov/41175191/  \n",
              "randomized controlled trial definition                    https://pubmed.ncbi.nlm.nih.gov/41174772/  \n",
              "type 2 diabetes lifestyle interventions evidence          https://pubmed.ncbi.nlm.nih.gov/41174629/  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1fb1f723-8651-4f41-9315-b62b7154b263\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>query</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Explain PageRank algorithm</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Search engine</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Search%20engine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>How to fix ModuleNotFoundError in Python</th>\n",
              "      <td>stackexchange</td>\n",
              "      <td>Is my tkcalendar clashing with ttkbootstrap?</td>\n",
              "      <td>https://stackoverflow.com/questions/79751755/i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SQL window function ROW_NUMBER example</th>\n",
              "      <td>stackexchange</td>\n",
              "      <td>How to pass fieldnames containing # to createM...</td>\n",
              "      <td>https://stackoverflow.com/questions/79768985/h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>What is a memory leak and how to detect it</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Buffer overflow</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Buffer%20overflow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>What is overfitting vs underfitting</th>\n",
              "      <td>stackexchange</td>\n",
              "      <td>Why does my logistic curve_fit prediction retu...</td>\n",
              "      <td>https://stackoverflow.com/questions/79748043/w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adverse effects of statins</th>\n",
              "      <td>pubmed</td>\n",
              "      <td>Updated Evidence on the Protective Role of Sta...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41170235/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>definition of entropy in information theory</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Kolmogorov complexity</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kolmogorov%20com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difference between git merge and rebase</th>\n",
              "      <td>stackexchange</td>\n",
              "      <td>Is there any advantage to updating a branch th...</td>\n",
              "      <td>https://stackoverflow.com/questions/79724991/i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>history of backpropagation</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Artificial intelligence</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Artificial%20int...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metformin mechanism of action</th>\n",
              "      <td>pubmed</td>\n",
              "      <td>The beneficial effects of empagliflozin-metfor...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41175191/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>randomized controlled trial definition</th>\n",
              "      <td>pubmed</td>\n",
              "      <td>Lessons learned from two multicentre randomise...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41174772/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>type 2 diabetes lifestyle interventions evidence</th>\n",
              "      <td>pubmed</td>\n",
              "      <td>The role of lifestyle factors in type 2 diabet...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41174629/</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fb1f723-8651-4f41-9315-b62b7154b263')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1fb1f723-8651-4f41-9315-b62b7154b263 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1fb1f723-8651-4f41-9315-b62b7154b263');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ae9a0d73-5719-461d-9f08-b5d8f285f7e9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae9a0d73-5719-461d-9f08-b5d8f285f7e9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ae9a0d73-5719-461d-9f08-b5d8f285f7e9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(firsts\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"randomized controlled trial definition\",\n          \"metformin mechanism of action\",\n          \"Explain PageRank algorithm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"wikipedia\",\n          \"stackexchange\",\n          \"pubmed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Lessons learned from two multicentre randomised controlled trials undertaken in pharmacies in community settings: a retrospective project management analysis.\",\n          \"The beneficial effects of empagliflozin-metformin combination on cardiovascular risk factors in type 2 diabetes mellitus patients.\",\n          \"Search engine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"https://pubmed.ncbi.nlm.nih.gov/41174772/\",\n          \"https://pubmed.ncbi.nlm.nih.gov/41175191/\",\n          \"https://en.wikipedia.org/wiki/Search%20engine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#single-query demos"
      ],
      "metadata": {
        "id": "RDVG70QP7hLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Biomedical-ish â†’ likely PubMed in mix\n",
        "!python retrieval_runner.py --config \"configs/index/config.json\" --index multi_default \\\n",
        "  --query \"GLP-1 receptor agonists weight loss evidence\" --top_k 8\n",
        "\n",
        "# Coding/how-to â†’ likely StackExchange in mix\n",
        "!python retrieval_runner.py --config \"configs/index/config.json\" --index multi_default \\\n",
        "  --query \"Best practice for SQL indexing composite keys\" --top_k 8\n",
        "\n",
        "# General CS/knowledge â†’ likely Wikipedia in mix\n",
        "!python retrieval_runner.py --config \"configs/index/config.json\" --index multi_default \\\n",
        "  --query \"Explain PageRank algorithm in simple terms\" --top_k 8\n"
      ],
      "metadata": {
        "id": "GLxGqkq57jcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17f4eca-9cff-43f7-d364-872a48f50e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-04 16:52:07.227708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867127.248575    1808 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867127.254954    1808 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867127.270564    1808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867127.270594    1808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867127.270599    1808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867127.270602    1808 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Loaded FAISS index: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\n",
            "Loaded metadata rows: 79164\n",
            "Loaded model: BAAI/bge-small-en-v1.5\n",
            "ğŸ•’ Ran 1 queries in 0.275s; avg 0.275s/query\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "1. [pubmed] Computational Modelling the Impact of GLP-1 Receptor Agonists on Botulinum Toxin A: Evidence for Reduced Treatment Durability Across Neurologic and Aesthetic Indications. (score=0.3327)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41173332/\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "2. [pubmed] Efficacy of GLP-1 Receptor Agonists in Obese Patients with Heart Failure with Preserved Ejection Fraction: A Systematic Review and Meta-analysis of Randomized Trials and Propensity Score-Matched Cohorts. (score=0.3392)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41173128/\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "3. [pubmed] Patient experiences with liraglutide for obesity and binge eating disorder-A qualitative study. (score=0.3516)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41171863/\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "4. [pubmed] Benefits of glucagon-like peptide-1 receptor agonists versus pioglitazone for cardio-hepatic outcomes: a territory-wide target trial emulation. (score=0.3911)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41174643/\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "5. [pubmed] Prophylactic and therapeutic role of Betulin in mitigating obesity-related metabolic dysfunction in rats fed on high-fat diet. (score=0.4337)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41172778/\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "6. [pubmed] Pharmacological therapies for type 2 diabetes: future approaches. (score=0.4482)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41174263/\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "7. [pubmed] Comprehensive impact of Intermittent Hypoxia Training and Intermittent Fasting on metabolic and cognitive health in adults with obesity: an umbrella systematic review and meta-analysis. (score=0.4504)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41170361/\n",
            "\n",
            "Q: GLP-1 receptor agonists weight loss evidence\n",
            "8. [pubmed] Association of Glucagon-Like Peptide-1 Receptor Agonist Use with Risk of Infections: A Systematic Review and Meta-analysis. (score=0.4580)\n",
            "https://pubmed.ncbi.nlm.nih.gov/41173399/\n",
            "2025-12-04 16:52:23.909275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867143.930503    1941 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867143.936943    1941 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867143.952630    1941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867143.952666    1941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867143.952669    1941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867143.952671    1941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Loaded FAISS index: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\n",
            "Loaded metadata rows: 79164\n",
            "Loaded model: BAAI/bge-small-en-v1.5\n",
            "ğŸ•’ Ran 1 queries in 0.280s; avg 0.280s/query\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "1. [stackexchange] Query with aggregate functions is very slow (score=0.4709)\n",
            "https://stackoverflow.com/questions/79760111/query-with-aggregate-functions-is-very-slow\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "2. [stackexchange] Grouping by and making flag based on patterns (score=0.4725)\n",
            "https://stackoverflow.com/questions/79805913/grouping-by-and-making-flag-based-on-patterns\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "3. [stackexchange] Query with aggregate functions is very slow (score=0.4957)\n",
            "https://stackoverflow.com/questions/79760111/query-with-aggregate-functions-is-very-slow\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "4. [stackexchange] Is a clustered primary key in InnoDB considered a covering index when selecting only PK columns? (score=0.5046)\n",
            "https://stackoverflow.com/questions/79763470/is-a-clustered-primary-key-in-innodb-considered-a-covering-index-when-selecting\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "5. [stackexchange] Create multi table index/composite key (score=0.5066)\n",
            "https://stackoverflow.com/questions/79738459/create-multi-table-index-composite-key\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "6. [stackexchange] Query with aggregate functions is very slow (score=0.5114)\n",
            "https://stackoverflow.com/questions/79760111/query-with-aggregate-functions-is-very-slow\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "7. [stackexchange] Do you need index on is_archived if not many rows are archived? (score=0.5162)\n",
            "https://stackoverflow.com/questions/79796876/do-you-need-index-on-is-archived-if-not-many-rows-are-archived\n",
            "\n",
            "Q: Best practice for SQL indexing composite keys\n",
            "8. [stackexchange] Do explicit locks in postgresql CTEs lock every row even with a cursor? (score=0.5249)\n",
            "https://stackoverflow.com/questions/79760645/do-explicit-locks-in-postgresql-ctes-lock-every-row-even-with-a-cursor\n",
            "2025-12-04 16:52:40.582598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867160.604628    2076 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867160.611234    2076 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867160.627799    2076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867160.627828    2076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867160.627831    2076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867160.627834    2076 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Loaded FAISS index: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\n",
            "Loaded metadata rows: 79164\n",
            "Loaded model: BAAI/bge-small-en-v1.5\n",
            "ğŸ•’ Ran 1 queries in 0.276s; avg 0.276s/query\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "1. [wikipedia] Search engine (score=0.5067)\n",
            "https://en.wikipedia.org/wiki/Search%20engine\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "2. [wikipedia] Search engine (score=0.5331)\n",
            "https://en.wikipedia.org/wiki/Search%20engine\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "3. [wikipedia] Search engine (score=0.5350)\n",
            "https://en.wikipedia.org/wiki/Search%20engine\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "4. [wikipedia] Search engine (score=0.5597)\n",
            "https://en.wikipedia.org/wiki/Search%20engine\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "5. [wikipedia] Analysis of algorithms (score=0.5844)\n",
            "https://en.wikipedia.org/wiki/Analysis%20of%20algorithms\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "6. [wikipedia] Search engine (score=0.5880)\n",
            "https://en.wikipedia.org/wiki/Search%20engine\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "7. [wikipedia] Search engine (score=0.5885)\n",
            "https://en.wikipedia.org/wiki/Search%20engine\n",
            "\n",
            "Q: Explain PageRank algorithm in simple terms\n",
            "8. [wikipedia] Algorithm (score=0.5921)\n",
            "https://en.wikipedia.org/wiki/Algorithm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#timing log (repeatable benchmark output)"
      ],
      "metadata": {
        "id": "oHTtp-9J794Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run the 12-query set and also write a small timing sidecar\n",
        "import time, subprocess, os, json\n",
        "start = time.time()\n",
        "all_csv = os.path.join(ROOT, \"results/multi_default/all_sources_top10.csv\")\n",
        "cmd = f'python retrieval_runner.py --config \"configs/index/config.json\" --index multi_default --qfile \"data/queries/all_sources_eval.txt\" --top_k 10 --out_csv \"{all_csv}\"'\n",
        "ret = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "print(ret.stdout)\n",
        "elapsed = time.time()-start\n",
        "\n",
        "timing_path = os.path.join(ROOT, \"results/multi_default/all_sources_timing.json\")\n",
        "json.dump({\"elapsed_seconds\": elapsed, \"queries\": 12, \"avg_per_query\": elapsed/12}, open(timing_path,\"w\"), indent=2)\n",
        "print(\"Timing saved:\", timing_path)\n"
      ],
      "metadata": {
        "id": "Vt54XARb7-2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d715a041-eeae-42ce-e2fc-993eaf6d0a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded FAISS index: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss\n",
            "Loaded metadata rows: 79164\n",
            "Loaded model: BAAI/bge-small-en-v1.5\n",
            "ğŸ•’ Ran 12 queries in 0.406s; avg 0.034s/query\n",
            "ğŸ’¾ Saved: /content/drive/MyDrive/AMS 560 PROJECT/results/multi_default/all_sources_top10.csv\n",
            "\n",
            "Timing saved: /content/drive/MyDrive/AMS 560 PROJECT/results/multi_default/all_sources_timing.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# efSearch configurable"
      ],
      "metadata": {
        "id": "yknQZkwX8N3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch: if you want to try a faster/more accurate HNSW search, bump efSearch to 128 in code.\n",
        "# (You can also add \"efSearch\": 128 inside config.json under each index to formalize it.)\n",
        "from json import load, dump\n",
        "cfg_path = os.path.join(ROOT, \"configs/index/config.json\")\n",
        "cfg = load(open(cfg_path))\n",
        "for k,v in cfg[\"indices\"].items():\n",
        "    v[\"efSearch\"] = 128\n",
        "dump(cfg, open(cfg_path, \"w\"), indent=2)\n",
        "print(\"Added efSearch=128 to config\")\n",
        "\n",
        "# Update retrieval_runner.py (one-line enhancement): use efSearch if present\n",
        "patch = \"\"\"\n",
        "        try:\n",
        "            from faiss import ParameterSpace\n",
        "            ps = ParameterSpace()\n",
        "            ps.set_index_parameter(self.index, \"efSearch\", int(self.icfg.get(\"efSearch\", 64)))\n",
        "        except Exception:\n",
        "            pass\n",
        "\"\"\"\n",
        "print(\"Add this block (replacing the old efSearch try/except) inside RetrievalRunner.__init__ if you want dynamic efSearch.\")\n"
      ],
      "metadata": {
        "id": "RS1J7UT78PQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23828a6f-e1cc-4f04-9cda-b193b1a13f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added efSearch=128 to config\n",
            "Add this block (replacing the old efSearch try/except) inside RetrievalRunner.__init__ if you want dynamic efSearch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-2 Retrieval Runner (Kashish README)\n",
        "**What this does**\n",
        "- Loads merged FAISS + merged meta (wikipedia+pubmed+stackexchange).\n",
        "- Encodes queries with BAAI/bge-small-en-v1.5.\n",
        "- Returns top-k results with title/url/source/text.\n",
        "- Saves CSV results for Team-3 benchmarking.\n",
        "\n",
        "**How to run**\n",
        "```bash\n",
        "python retrieval_runner.py \\\\\n",
        "  --config \"configs/index/config.json\" \\\\\n",
        "  --index multi_default \\\\\n",
        "  --qfile \"data/queries/all_sources_eval.txt\" \\\\\n",
        "  --top_k 10 \\\\\n",
        "  --out_csv \"results/multi_default/all_sources_top10.csv\"\n"
      ],
      "metadata": {
        "id": "7tfd408E8W4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval System Extension"
      ],
      "metadata": {
        "id": "axWkRD0kKXxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3.1 â€” Config loader + Backend interface**\n",
        "\n",
        "What this adds?\n",
        "\n",
        "pipeline/config.py â†’ loads JSON or YAML, validates schema, and returns a normalized config for a chosen index (single or sharded).\n",
        "\n",
        "pipeline/backends/base.py â†’ defines a pluggable VectorBackend interface so FAISS/Chroma/Spark-sharded implementations can slot in later.\n",
        "\n",
        "Why?\n",
        "\n",
        "Lets you switch backend / model / index (single vs shards) just by editing config"
      ],
      "metadata": {
        "id": "O2P0wWWDL9H_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folders (once)"
      ],
      "metadata": {
        "id": "kIkz1F7PMZXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make pipeline folders (safe if they already exist)\n",
        "import os\n",
        "os.makedirs(\"pipeline/backends\", exist_ok=True)\n",
        "os.makedirs(\"results/logs\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "oTVe0NU_MH9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Hide TensorFlowâ€™s C++ logs\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "# Tell Transformers to NOT import TensorFlow/Flax at all\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\""
      ],
      "metadata": {
        "id": "tLdSaCFR7UZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write pipeline/config.py"
      ],
      "metadata": {
        "id": "uUnvNKKEMW1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/config.py\n",
        "\"\"\"\n",
        "Config loader for retrieval pipeline (scaling-ready).\n",
        "\n",
        "- Supports JSON (.json) and YAML (.yml/.yaml)\n",
        "- Validates index blocks (single-index or sharded)\n",
        "- Normalizes keys so backends see a consistent shape\n",
        "- Adds schema versioning and metric/normalization semantics\n",
        "- Supports remote paths (s3://, gs://, hdfs://) and optional path checks\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import yaml  # type: ignore\n",
        "except Exception as e:\n",
        "    raise ImportError(\n",
        "        \"pyyaml is required. Install with: pip install pyyaml\"\n",
        "    ) from e\n",
        "\n",
        "\n",
        "class ConfigError(Exception):\n",
        "    \"\"\"Raised when configuration is missing/invalid.\"\"\"\n",
        "\n",
        "\n",
        "# Accepted config schema versions (simple gate for now)\n",
        "_ACCEPTED_CONFIG_VERSIONS = {\"1\"}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class NormalizedIndexConfig:\n",
        "    # core knobs\n",
        "    backend: str\n",
        "    model_name: str\n",
        "    normalize_query: bool\n",
        "    metric: str\n",
        "    efSearch: int\n",
        "\n",
        "    # union of single vs sharded\n",
        "    faiss_path: Optional[str] = None\n",
        "    meta_path: Optional[str] = None\n",
        "    index_paths: Optional[List[str]] = None\n",
        "    meta_paths: Optional[List[str]] = None\n",
        "\n",
        "    # semantics (for cross-backend parity & validation)\n",
        "    index_metric: Optional[str] = None  # 'ip' | 'l2' | 'cosine'\n",
        "    vector_normed: Optional[bool] = None  # True if stored vectors are unit-normalized\n",
        "\n",
        "    # passthrough\n",
        "    extras: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    # loader context\n",
        "    index_key: Optional[str] = None\n",
        "    config_version: Optional[str] = None\n",
        "    skip_path_checks: bool = False\n",
        "\n",
        "\n",
        "def _load_raw_config(path: str) -> Dict[str, Any]:\n",
        "    if not os.path.exists(path):\n",
        "        raise ConfigError(f\"Config file not found: {path}\")\n",
        "\n",
        "    _, ext = os.path.splitext(path.lower())\n",
        "\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            if ext == \".json\":\n",
        "                return json.load(f)\n",
        "            if ext in (\".yml\", \".yaml\"):\n",
        "                return yaml.safe_load(f)\n",
        "\n",
        "            # unknown extension â†’ try JSON then YAML\n",
        "            try:\n",
        "                f.seek(0)\n",
        "                return json.load(f)\n",
        "            except Exception:\n",
        "                f.seek(0)\n",
        "                return yaml.safe_load(f)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise ConfigError(f\"Failed to parse config {path}: {e}\") from e\n",
        "\n",
        "\n",
        "def _require(d: Dict[str, Any], key: str, ctx: str) -> Any:\n",
        "    if key not in d:\n",
        "        raise ConfigError(f\"Missing required key '{key}' in {ctx}\")\n",
        "    return d[key]\n",
        "\n",
        "\n",
        "def _as_bool(v: Any, default: bool) -> bool:\n",
        "    if v is None:\n",
        "        return default\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if isinstance(v, (int, float)):\n",
        "        return bool(v)\n",
        "    if isinstance(v, str):\n",
        "        s = v.strip().lower()\n",
        "        if s in {\"1\", \"true\", \"yes\", \"y\", \"on\"}:\n",
        "            return True\n",
        "        if s in {\"0\", \"false\", \"no\", \"n\", \"off\"}:\n",
        "            return False\n",
        "    raise ConfigError(f\"Expected boolean-like value, got: {v!r}\")\n",
        "\n",
        "\n",
        "def _as_int(v: Any, default: int) -> int:\n",
        "    if v is None:\n",
        "        return default\n",
        "    if isinstance(v, int):\n",
        "        return v\n",
        "    if isinstance(v, float):\n",
        "        return int(v)\n",
        "    if isinstance(v, str):\n",
        "        try:\n",
        "            return int(float(v.strip()))\n",
        "        except Exception:\n",
        "            pass\n",
        "    raise ConfigError(f\"Expected integer-like value, got: {v!r}\")\n",
        "\n",
        "\n",
        "def _is_remote_path(p: str) -> bool:\n",
        "    p = (p or \"\").lower()\n",
        "    return p.startswith(\"s3://\") or p.startswith(\"gs://\") or p.startswith(\"hdfs://\")\n",
        "\n",
        "\n",
        "def _validate_paths_exist(paths: List[str], ctx: str, *, skip_checks: bool) -> None:\n",
        "    if skip_checks:\n",
        "        return\n",
        "    missing = [p for p in paths if (not _is_remote_path(p)) and (not os.path.exists(p))]\n",
        "    if missing:\n",
        "        raise ConfigError(f\"{ctx}: missing files:\\n - \" + \"\\n - \".join(missing))\n",
        "\n",
        "\n",
        "def _normalize_index_block(\n",
        "    block: Dict[str, Any],\n",
        "    ctx: str,\n",
        "    index_key: str,\n",
        "    top_level_skip_checks: bool,\n",
        ") -> NormalizedIndexConfig:\n",
        "\n",
        "    # knobs\n",
        "    backend = (block.get(\"backend\") or \"faiss\").lower()\n",
        "    model_name = block.get(\"model_name\", \"BAAI/bge-small-en-v1.5\")\n",
        "    metric = (block.get(\"metric\") or \"ip\").lower()\n",
        "    efSearch = _as_int(block.get(\"efSearch\"), 64)\n",
        "    normalize_query = _as_bool(block.get(\"normalize_query\"), True)\n",
        "\n",
        "    # semantics\n",
        "    index_metric = (block.get(\"index_metric\") or metric or \"\").lower() or None\n",
        "\n",
        "    vector_normed = block.get(\"vector_normed\")\n",
        "    if vector_normed is not None:\n",
        "        vector_normed = _as_bool(vector_normed, False)\n",
        "\n",
        "    # single vs sharded\n",
        "    faiss_path = block.get(\"faiss_path\")\n",
        "    meta_path = block.get(\"meta_path\")\n",
        "    index_paths = block.get(\"index_paths\")\n",
        "    meta_paths = block.get(\"meta_paths\")\n",
        "\n",
        "    single_ok = bool(faiss_path and meta_path)\n",
        "    shard_ok = bool(index_paths and meta_paths)\n",
        "\n",
        "    if single_ok and shard_ok:\n",
        "        raise ConfigError(\n",
        "            f\"{ctx}: Provide EITHER single (faiss_path/meta_path) OR sharded \"\n",
        "            f\"(index_paths/meta_paths), not both.\"\n",
        "        )\n",
        "\n",
        "    if not (single_ok or shard_ok):\n",
        "        raise ConfigError(\n",
        "            f\"{ctx}: Must provide single (faiss_path+meta_path) OR sharded \"\n",
        "            f\"(index_paths+meta_paths).\"\n",
        "        )\n",
        "\n",
        "    # skip checks: block-level overrides top-level\n",
        "    skip_path_checks = _as_bool(block.get(\"skip_path_checks\"), top_level_skip_checks)\n",
        "\n",
        "    # path validation\n",
        "    if single_ok:\n",
        "        _validate_paths_exist(\n",
        "            [faiss_path, meta_path], f\"{ctx} (single)\", skip_checks=skip_path_checks\n",
        "        )\n",
        "    else:\n",
        "        if not isinstance(index_paths, list) or not isinstance(meta_paths, list):\n",
        "            raise ConfigError(\n",
        "                f\"{ctx}: index_paths/meta_paths must be lists for sharded mode.\"\n",
        "            )\n",
        "        if len(index_paths) != len(meta_paths):\n",
        "            raise ConfigError(\n",
        "                f\"{ctx}: index_paths and meta_paths length mismatch \"\n",
        "                f\"({len(index_paths)} vs {len(meta_paths)}).\"\n",
        "            )\n",
        "\n",
        "        _validate_paths_exist(\n",
        "            index_paths, f\"{ctx} index_paths\", skip_checks=skip_path_checks\n",
        "        )\n",
        "        _validate_paths_exist(\n",
        "            meta_paths, f\"{ctx} meta_paths\", skip_checks=skip_path_checks\n",
        "        )\n",
        "\n",
        "    # extras\n",
        "    extras = {\n",
        "        k: v\n",
        "        for k, v in block.items()\n",
        "        if k\n",
        "        not in {\n",
        "            \"backend\",\n",
        "            \"model_name\",\n",
        "            \"normalize_query\",\n",
        "            \"metric\",\n",
        "            \"efSearch\",\n",
        "            \"faiss_path\",\n",
        "            \"meta_path\",\n",
        "            \"index_paths\",\n",
        "            \"meta_paths\",\n",
        "            \"index_metric\",\n",
        "            \"vector_normed\",\n",
        "            \"skip_path_checks\",\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return NormalizedIndexConfig(\n",
        "        backend=backend,\n",
        "        model_name=model_name,\n",
        "        normalize_query=normalize_query,\n",
        "        metric=metric,\n",
        "        efSearch=efSearch,\n",
        "        faiss_path=faiss_path,\n",
        "        meta_path=meta_path,\n",
        "        index_paths=index_paths,\n",
        "        meta_paths=meta_paths,\n",
        "        index_metric=index_metric,\n",
        "        vector_normed=vector_normed,\n",
        "        extras=extras,\n",
        "        index_key=index_key,\n",
        "        config_version=None,  # filled by loader\n",
        "        skip_path_checks=skip_path_checks,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_config(\n",
        "    path: str, index_key: Optional[str] = None\n",
        ") -> Tuple[Dict[str, Any], NormalizedIndexConfig]:\n",
        "    \"\"\"\n",
        "    Returns (raw_config_dict, normalized_index_cfg) for the chosen index key.\n",
        "\n",
        "    - If index_key is None, uses raw['active_index'].\n",
        "    - Validates the selected index block.\n",
        "    - Enforces schema version and remote-path/skip-check behavior.\n",
        "    \"\"\"\n",
        "\n",
        "    raw = _load_raw_config(path)\n",
        "    if not isinstance(raw, dict):\n",
        "        raise ConfigError(\"Top-level config must be an object/dict.\")\n",
        "\n",
        "    # schema version\n",
        "    cfg_version = str(raw.get(\"config_version\", \"1\")).strip()\n",
        "    if cfg_version not in _ACCEPTED_CONFIG_VERSIONS:\n",
        "        raise ConfigError(\n",
        "            f\"Unsupported config_version='{cfg_version}'. \"\n",
        "            f\"Accepted versions: {_ACCEPTED_CONFIG_VERSIONS}\"\n",
        "        )\n",
        "\n",
        "    indices = _require(raw, \"indices\", \"root\")\n",
        "    if not isinstance(indices, dict) or not indices:\n",
        "        raise ConfigError(\"'indices' must be a non-empty object.\")\n",
        "\n",
        "    if index_key is None:\n",
        "        index_key = raw.get(\"active_index\")\n",
        "        if not index_key:\n",
        "            raise ConfigError(\"Provide index_key or set 'active_index' in config.\")\n",
        "\n",
        "    if index_key not in indices:\n",
        "        raise ConfigError(f\"Index key '{index_key}' not found in 'indices'.\")\n",
        "\n",
        "    block = indices[index_key]\n",
        "    if not isinstance(block, dict):\n",
        "        raise ConfigError(f\"Index '{index_key}' must be an object.\")\n",
        "\n",
        "    top_level_skip_checks = _as_bool(raw.get(\"skip_path_checks\"), False)\n",
        "\n",
        "    nic = _normalize_index_block(\n",
        "        block, f\"index '{index_key}'\", index_key, top_level_skip_checks\n",
        "    )\n",
        "    nic.config_version = cfg_version\n",
        "\n",
        "    # warn on metric mismatch\n",
        "    if nic.index_metric and nic.metric and nic.index_metric != nic.metric:\n",
        "        import warnings\n",
        "\n",
        "        warnings.warn(\n",
        "            f\"Requested metric '{nic.metric}' differs from index_metric '{nic.index_metric}'. \"\n",
        "            \"Ensure query normalization / FAISS setup matches.\"\n",
        "        )\n",
        "\n",
        "    return raw, nic\n"
      ],
      "metadata": {
        "id": "kkW3X-ZRMOzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1626169a-3358-48e0-e4bd-00e9aa4f666c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write pipeline/backends/base.py"
      ],
      "metadata": {
        "id": "fC_3VBnUMczD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/backends/base.py\n",
        "\"\"\"\n",
        "Backend interface for retrieval pipeline.\n",
        "\n",
        "Concrete backends must implement:\n",
        "- load(cfg): prepare model, index/connection, metadata\n",
        "- encode(text_or_list): return float32 numpy array (n, d)\n",
        "- search(qvec, top_k): return list[dict] with keys:\n",
        "    rank, score, id, source, title, url, text, chunk_id\n",
        "- close(): free resources\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "from typing import Any, Dict, List, Protocol, runtime_checkable\n",
        "\n",
        "\n",
        "Result = Dict[str, Any]\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class VectorBackend(Protocol):\n",
        "    def load(self, cfg: Any) -> None:  # cfg is NormalizedIndexConfig or similar\n",
        "        ...\n",
        "\n",
        "    def encode(self, text_or_list: Any) -> Any:\n",
        "        \"\"\"\n",
        "        Accepts a string or list[str] and returns np.ndarray[float32] of shape (n, d).\n",
        "        Implementations should respect 'normalize_query' if applicable.\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    def search(self, qvec: Any, top_k: int = 5, filter_source: str | None = None) -> List[Result]:\n",
        "        \"\"\"\n",
        "        Accepts a single query vector (1, d) and returns a list of result dicts:\n",
        "        {\n",
        "          'rank': int,\n",
        "          'score': float,\n",
        "          'id': str,\n",
        "          'source': str,   # domain\n",
        "          'title': str,\n",
        "          'url': str,\n",
        "          'text': str,     # chunk_text or raw text\n",
        "          'chunk_id': int|None\n",
        "        }\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    def close(self) -> None:\n",
        "        ...\n"
      ],
      "metadata": {
        "id": "bagL0tlXMR8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4d3013-2156-4386-ca4c-cfc1e958ac5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/backends/base.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick sanity tests (run these)"
      ],
      "metadata": {
        "id": "G9p5zFPMMhQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from pipeline.config import load_config, NormalizedIndexConfig, ConfigError\n",
        "    print(\"Imports OK\")\n",
        "    raw, nic = load_config(\"configs/index/config.json\", index_key=None)\n",
        "    print(\"Active index normalized:\", nic)\n",
        "    try:\n",
        "        _ = load_config(\"configs/index/config.json\", index_key=\"does_not_exist\")\n",
        "    except ConfigError as e:\n",
        "        print(\"Expected error (bad index key):\", e)"
      ],
      "metadata": {
        "id": "EcbXF1ZLhUBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdffcefb-495b-4547-d950-04d77457ed91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports OK\n",
            "Active index normalized: NormalizedIndexConfig(backend='faiss', model_name='BAAI/bge-small-en-v1.5', normalize_query=True, metric='ip', efSearch=128, faiss_path='/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default/multi_hnsw.faiss', meta_path='/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default/multi_meta.parquet', index_paths=None, meta_paths=None, index_metric='ip', vector_normed=None, extras={}, index_key='multi_default', config_version='1', skip_path_checks=False)\n",
            "Expected error (bad index key): Index key 'does_not_exist' not found in 'indices'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pipeline.config import load_config, ConfigError\n",
        "try:\n",
        "    # Temporarily point to a non-existent file in a copy of the config and load it\n",
        "    import json, shutil\n",
        "    shutil.copy(\"configs/index/config.json\", \"configs/index/config_tmp.json\")\n",
        "    cfg = json.load(open(\"configs/index/config_tmp.json\"))\n",
        "    cfg[\"indices\"][\"multi_default\"][\"faiss_path\"] = \"/not/a/real/path.faiss\"\n",
        "    json.dump(cfg, open(\"configs/index/config_tmp.json\",\"w\"), indent=2)\n",
        "    _ = load_config(\"configs/index/config_tmp.json\", index_key=\"multi_default\")\n",
        "except ConfigError as e:\n",
        "    print(\"Path validation works â†’\", e)\n"
      ],
      "metadata": {
        "id": "mYg_3CmbNYYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43721899-f84b-4e67-e96b-b51586263b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path validation works â†’ index 'multi_default' (single): missing files:\n",
            " - /not/a/real/path.faiss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.2: FAISS backend + pipeline runner"
      ],
      "metadata": {
        "id": "IMY531zWPTlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this adds?\n",
        "\n",
        "pipeline/backends/faiss_backend.py â€” production-ready backend that:\n",
        "*   loads single FAISS index or sharded indices (with heap-merge),\n",
        "*   honors efSearch, normalize_query, metric,\n",
        "*   supports filter_source (wikipedia/pubmed/stackexchange).\n",
        "\n",
        "pipeline/runner.py â€” tiny faÃ§ade:\n",
        "*   load_pipeline(cfg_path, index_key=None, backend_override=None)\n",
        "*   search_one(query, top_k, filter_source=None)\n",
        "*   search_many(qfile, top_k, filter_source=None)\n",
        "\n",
        "Why?\n",
        "\n",
        "This replaces ad-hoc scripts with a pluggable, config-driven pipeline scaling can reuse for bigger stores."
      ],
      "metadata": {
        "id": "Pp8evj6PPkVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write pipeline/backends/faiss_backend.py"
      ],
      "metadata": {
        "id": "khOmeP2VQKUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/backends/faiss_backend.py\n",
        "from __future__ import annotations\n",
        "import heapq, os\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import faiss  # faiss-cpu\n",
        "except Exception:  # pragma: no cover\n",
        "    import faiss_cpu as faiss  # fallback if aliased\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pipeline.backends.base import VectorBackend, Result\n",
        "from pipeline.config import NormalizedIndexConfig\n",
        "\n",
        "\n",
        "class FaissBackend(VectorBackend):\n",
        "    \"\"\"\n",
        "    FAISS backend supporting:\n",
        "      â€¢ Single index (faiss_path + meta_path)\n",
        "      â€¢ Sharded indices (index_paths + meta_paths) with global top-k merge\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.cfg: Optional[NormalizedIndexConfig] = None\n",
        "        self.model: Optional[SentenceTransformer] = None\n",
        "        self.normalize_query: bool = True\n",
        "        self.metric: str = \"ip\"\n",
        "        self._higher_is_better: bool = True  # derived from index/metric (ip/cosine=True, l2=False)\n",
        "\n",
        "        # single-index\n",
        "        self.index = None\n",
        "        self.meta_df: Optional[pd.DataFrame] = None\n",
        "\n",
        "        # sharded\n",
        "        self.shard_indices: List[Any] = []\n",
        "        self.shard_metas: List[pd.DataFrame] = []\n",
        "        self.sharded: bool = False\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_efsearch(index, ef: int) -> None:\n",
        "        try:\n",
        "            ps = faiss.ParameterSpace()\n",
        "            ps.set_index_parameter(index, \"efSearch\", int(ef))\n",
        "        except Exception:\n",
        "            # Some FAISS builds or index types don't support this param (e.g., non-HNSW)\n",
        "            pass\n",
        "\n",
        "    @staticmethod\n",
        "    def _set_nprobe(index, nprobe: Optional[int]) -> None:\n",
        "        if nprobe is None:\n",
        "            return\n",
        "        try:\n",
        "            ps = faiss.ParameterSpace()\n",
        "            ps.set_index_parameter(index, \"nprobe\", int(nprobe))\n",
        "        except Exception:\n",
        "            # Silently ignore if index is not IVF or FAISS build lacks ParameterSpace\n",
        "            pass\n",
        "\n",
        "    @staticmethod\n",
        "    def _norm_metric_name(name: Optional[str]) -> str:\n",
        "        if not name:\n",
        "            return \"\"\n",
        "        n = str(name).strip().lower()\n",
        "        if n in {\"cos\", \"cosine\", \"cos_sim\"}:\n",
        "            return \"cosine\"\n",
        "        if n in {\"ip\", \"inner\", \"inner_product\"}:\n",
        "            return \"ip\"\n",
        "        if n in {\"l2\", \"euclidean\"}:\n",
        "            return \"l2\"\n",
        "        return n\n",
        "\n",
        "    def _derive_metric_direction(self) -> None:\n",
        "        \"\"\"\n",
        "        Decide whether larger score = better. We normalize so that the internal 'score'\n",
        "        always follows higher-is-better across metrics.\n",
        "        \"\"\"\n",
        "        # Prefer the index's actual metric if provided; otherwise requested metric.\n",
        "        m = self._norm_metric_name(getattr(self.cfg, \"index_metric\", None) or self.metric)\n",
        "        self._higher_is_better = (m in {\"ip\", \"cosine\"})\n",
        "\n",
        "    @staticmethod\n",
        "    def _src_of_row(row: pd.Series) -> str:\n",
        "        \"\"\"Return a normalized (lowercased) source/domain string; empty if missing.\"\"\"\n",
        "        s = row.get(\"domain\", \"\") or row.get(\"source\", \"\") or \"\"\n",
        "        return str(s).strip().lower()\n",
        "\n",
        "    def _encode_one(self, text: str) -> np.ndarray:\n",
        "        assert self.model is not None\n",
        "        v = self.model.encode([text], normalize_embeddings=self.normalize_query)\n",
        "        return np.asarray(v, dtype=\"float32\")\n",
        "\n",
        "    def _row_to_result(self, row: pd.Series, score: float, rank: int) -> Result:\n",
        "        # choose best available text field\n",
        "        txt = (row.get(\"chunk_text\") or row.get(\"text\") or row.get(\"preview\") or row.get(\"content\") or \"\")\n",
        "        cid = row.get(\"chunk_id\")\n",
        "        try:\n",
        "            cid = int(cid) if cid is not None and cid == cid else None\n",
        "        except Exception:\n",
        "            cid = None\n",
        "        uid = f\"{row.get('id','')}::c{cid if cid is not None else -1}\"\n",
        "\n",
        "        return {\n",
        "            \"rank\": rank,\n",
        "            \"score\": float(score),\n",
        "            \"id\": row.get(\"id\", \"\"),\n",
        "            \"uid\": uid,\n",
        "            \"source\": row.get(\"domain\", \"\") or row.get(\"source\", \"\"),\n",
        "            \"title\": row.get(\"title\", \"\"),\n",
        "            \"url\": row.get(\"url\", \"\"),\n",
        "            \"text\": txt,\n",
        "            \"chunk_id\": cid,\n",
        "        }\n",
        "\n",
        "    # ---------- VectorBackend API ----------\n",
        "\n",
        "    def load(self, cfg: NormalizedIndexConfig) -> None:\n",
        "        self.cfg = cfg\n",
        "        self.normalize_query = bool(cfg.normalize_query)\n",
        "        self.metric = cfg.metric\n",
        "        self._derive_metric_direction()\n",
        "\n",
        "        # threading control (optional knob)\n",
        "        threads = int(cfg.extras.get(\"faiss_num_threads\", os.cpu_count() or 8))\n",
        "        try:\n",
        "            faiss.omp_set_num_threads(threads)\n",
        "            print(f\"[FAISS] Using {threads} threads\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # model\n",
        "        self.model = SentenceTransformer(cfg.model_name)\n",
        "\n",
        "        # optional IVF knob\n",
        "        nprobe = cfg.extras.get(\"nprobe\")\n",
        "\n",
        "        # load single or sharded indices\n",
        "        if cfg.faiss_path and cfg.meta_path:\n",
        "            self.sharded = False\n",
        "            self.index = faiss.read_index(cfg.faiss_path)\n",
        "            self._set_efsearch(self.index, cfg.efSearch)\n",
        "            self._set_nprobe(self.index, nprobe)\n",
        "            self.meta_df = pd.read_parquet(cfg.meta_path).reset_index(drop=True)\n",
        "        else:\n",
        "            self.sharded = True\n",
        "            for ipath, mpath in zip(cfg.index_paths or [], cfg.meta_paths or []):\n",
        "                idx = faiss.read_index(ipath)\n",
        "                self._set_efsearch(idx, cfg.efSearch)\n",
        "                self._set_nprobe(idx, nprobe)\n",
        "                self.shard_indices.append(idx)\n",
        "                self.shard_metas.append(pd.read_parquet(mpath).reset_index(drop=True))\n",
        "\n",
        "        # sanity checks\n",
        "        if not self.sharded:\n",
        "            if self.index.ntotal != len(self.meta_df):\n",
        "                raise ValueError(f\"FAISS ntotal={self.index.ntotal} != meta rows={len(self.meta_df)}\")\n",
        "        else:\n",
        "            nvec = sum(ix.ntotal for ix in self.shard_indices)\n",
        "            nmeta = sum(len(md) for md in self.shard_metas)\n",
        "            if nvec != nmeta:\n",
        "                raise ValueError(f\"[sharded] vectors {nvec} != meta rows {nmeta}\")\n",
        "\n",
        "    def encode(self, text_or_list: Any) -> np.ndarray:\n",
        "        if isinstance(text_or_list, str):\n",
        "            return self._encode_one(text_or_list)\n",
        "        elif isinstance(text_or_list, list):\n",
        "            assert self.model is not None\n",
        "            v = self.model.encode(text_or_list, normalize_embeddings=self.normalize_query)\n",
        "            return np.asarray(v, dtype=\"float32\")\n",
        "        raise TypeError(\"encode() expects str or list[str]\")\n",
        "\n",
        "    # ---- search helpers ----\n",
        "    def _search_single(self, qvec: np.ndarray, top_k: int, filter_source: Optional[str]) -> List[Result]:\n",
        "        norm_filter = filter_source.strip().lower() if filter_source else None\n",
        "        dedup_on = bool(getattr(self.cfg, \"extras\", {}).get(\"dedup_by_uid\", False))\n",
        "        seen_uids: set[str] = set()\n",
        "\n",
        "        overfetch = top_k * 4 if norm_filter else top_k\n",
        "        D, I = self.index.search(qvec, overfetch)\n",
        "        out: List[Result] = []\n",
        "        taken = 0\n",
        "        for idx, raw in zip(I[0], D[0]):\n",
        "            if idx < 0:\n",
        "                continue\n",
        "            row = self.meta_df.iloc[idx]\n",
        "            if norm_filter:\n",
        "                if self._src_of_row(row) != norm_filter:\n",
        "                    continue\n",
        "            # Normalize score so \"higher is better\" regardless of metric.\n",
        "            score = float(raw if self._higher_is_better else -raw)\n",
        "            res = self._row_to_result(row, score, taken + 1)\n",
        "\n",
        "            if dedup_on:\n",
        "                uid = res[\"uid\"]\n",
        "                if uid in seen_uids:\n",
        "                    continue\n",
        "                seen_uids.add(uid)\n",
        "\n",
        "            taken += 1\n",
        "            out.append(res)\n",
        "            if taken >= top_k:\n",
        "                break\n",
        "        return out\n",
        "\n",
        "    def _search_sharded(self, qvec: np.ndarray, top_k: int, filter_source: Optional[str]) -> List[Result]:\n",
        "        norm_filter = filter_source.strip().lower() if filter_source else None\n",
        "        dedup_on = bool(getattr(self.cfg, \"extras\", {}).get(\"dedup_by_uid\", False))\n",
        "        seen_uids: set[str] = set()\n",
        "\n",
        "        alpha = int(self.cfg.extras.get(\"alpha\", 3))\n",
        "        per = max(top_k * alpha, top_k)\n",
        "        heap: List[Tuple[float, int, int, str]] = []\n",
        "\n",
        "        for s_id, idx in enumerate(self.shard_indices):\n",
        "            D, I = idx.search(qvec, per)\n",
        "            for raw, li in zip(D[0], I[0]):\n",
        "                if li < 0:\n",
        "                    continue\n",
        "                # Normalize score; store as a max-heap via negative key.\n",
        "                score = float(raw if self._higher_is_better else -raw)\n",
        "                uid = str(li)\n",
        "                heapq.heappush(heap, (-score, s_id, int(li), uid))\n",
        "\n",
        "        out: List[Result] = []\n",
        "        seen = 0\n",
        "        while heap and seen < top_k:\n",
        "            neg, s_id, li, _ = heapq.heappop(heap)\n",
        "            score = -neg\n",
        "            row = self.shard_metas[s_id].iloc[li]\n",
        "            if norm_filter:\n",
        "                if self._src_of_row(row) != norm_filter:\n",
        "                    continue\n",
        "\n",
        "            res = self._row_to_result(row, score, seen + 1)\n",
        "\n",
        "            if dedup_on:\n",
        "                uid = res[\"uid\"]\n",
        "                if uid in seen_uids:\n",
        "                    continue\n",
        "                seen_uids.add(uid)\n",
        "\n",
        "            seen += 1\n",
        "            out.append(res)\n",
        "        return out\n",
        "\n",
        "    def search(self, qvec: Any, top_k: int = 5, filter_source: str | None = None) -> List[Result]:\n",
        "        if not isinstance(qvec, np.ndarray):\n",
        "            raise TypeError(\"search() expects numpy array (1,d)\")\n",
        "        if qvec.ndim != 2 or qvec.shape[0] != 1:\n",
        "            raise ValueError(\"search() expects shape (1,d)\")\n",
        "        return (\n",
        "            self._search_sharded(qvec, top_k, filter_source)\n",
        "            if self.sharded\n",
        "            else self._search_single(qvec, top_k, filter_source)\n",
        "        )\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.index = None\n",
        "        self.shard_indices.clear()\n",
        "        self.shard_metas.clear()\n",
        "        self.meta_df = None\n",
        "        self.model = None"
      ],
      "metadata": {
        "id": "pZyLcvvyQIby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b2f121-2788-442b-dff1-cb080636529b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/backends/faiss_backend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write pipeline/runner.py"
      ],
      "metadata": {
        "id": "ZyPMWjbEQMj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/runner.py\n",
        "from __future__ import annotations\n",
        "from typing import Optional, List, Dict, Any\n",
        "from pipeline.config import load_config, NormalizedIndexConfig\n",
        "from pipeline.backends.base import VectorBackend\n",
        "from pipeline.backends.faiss_backend import FaissBackend\n",
        "\n",
        "# registry of backends\n",
        "_REGISTRY = {\n",
        "    \"faiss\": FaissBackend,\n",
        "    # future: \"chroma\": ChromaBackend\n",
        "}\n",
        "\n",
        "class Pipeline:\n",
        "    def __init__(self, backend: VectorBackend, nicfg: NormalizedIndexConfig):\n",
        "        self.backend = backend\n",
        "        self.nicfg = nicfg\n",
        "\n",
        "    def search_one(self, query: str, top_k: int = 5, filter_source: str | None = None):\n",
        "        qvec = self.backend.encode(query)\n",
        "        return self.backend.search(qvec, top_k=top_k, filter_source=filter_source)\n",
        "\n",
        "    def search_many(self, queries: List[str], top_k: int = 5, filter_source: str | None = None):\n",
        "        qvecs = self.backend.encode(queries)\n",
        "        out = {}\n",
        "        for i, q in enumerate(queries):\n",
        "            out[q] = self.backend.search(qvecs[i:i+1], top_k=top_k, filter_source=filter_source)\n",
        "        return out\n",
        "\n",
        "    def close(self):\n",
        "        self.backend.close()\n",
        "\n",
        "\n",
        "def load_pipeline(cfg_path: str, index_key: Optional[str] = None, backend_override: Optional[str] = None) -> Pipeline:\n",
        "    raw, nic = load_config(cfg_path, index_key=index_key)\n",
        "    backend_key = (backend_override or nic.backend or \"faiss\").lower()\n",
        "    if backend_key not in _REGISTRY:\n",
        "        raise ValueError(f\"Unknown backend '{backend_key}'. Available: {list(_REGISTRY)}\")\n",
        "    backend = _REGISTRY[backend_key]()  # type: ignore\n",
        "    backend.load(nic)\n",
        "    return Pipeline(backend, nic)"
      ],
      "metadata": {
        "id": "9ES9sqZ6QO3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6dab8bd-0403-4c70-a422-b30956b6e050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/runner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "patch the sharded index block (indices.multi_sharded)"
      ],
      "metadata": {
        "id": "2FiZSK43I6Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/AMS 560 PROJECT\"\n",
        "CFG_PATH = f\"{PROJECT_ROOT}/configs/index/config.json\"\n",
        "FAISS_PATH = f\"{PROJECT_ROOT}/indices/multi_default/multi_hnsw.faiss\"\n",
        "META_PATH  = f\"{PROJECT_ROOT}/data/embeddings/multi_default/multi_meta.parquet\"\n",
        "\n",
        "cfg = {\n",
        "    \"config_version\": \"1\",\n",
        "    \"active_index\": \"multi_default\",\n",
        "    \"indices\": {\n",
        "        \"multi_default\": {\n",
        "            \"backend\": \"faiss\",\n",
        "            \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
        "            \"normalize_query\": True,\n",
        "            \"metric\": \"ip\",\n",
        "            \"index_metric\": \"ip\",\n",
        "            \"vector_normed\": True,\n",
        "            \"efSearch\": 128,\n",
        "            \"faiss_path\": FAISS_PATH,\n",
        "            \"meta_path\": META_PATH,\n",
        "            \"extras\": {\n",
        "                \"alpha\": 3,\n",
        "                \"dedup_by_uid\": True,\n",
        "                \"faiss_num_threads\": os.cpu_count() or 8,\n",
        "                \"text_fields\": [\"chunk_text\", \"text\", \"preview\"],\n",
        "                \"nprobe\": 32\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "os.makedirs(os.path.dirname(CFG_PATH), exist_ok=True)\n",
        "json.dump(cfg, open(CFG_PATH, \"w\"), indent=2)\n",
        "print(\"âœ… Wrote config:\", CFG_PATH)"
      ],
      "metadata": {
        "id": "03iRXLZZI7K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bc8b6c-2139-42d0-8bcd-9ffee22996fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Wrote config: /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hard-reload + quick sanity"
      ],
      "metadata": {
        "id": "mzGUjhMhQRcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”„ Hard-reload pipeline modules so the new FaissBackend is used\n",
        "import sys, inspect\n",
        "\n",
        "for m in list(sys.modules):\n",
        "    if m.startswith(\"pipeline.\"):\n",
        "        del sys.modules[m]\n",
        "\n",
        "from pipeline.runner import load_pipeline\n",
        "from pipeline.backends.faiss_backend import FaissBackend\n",
        "\n",
        "print(\"row_to_result signature:\", inspect.signature(FaissBackend._row_to_result))\n",
        "print(\"Source snippet:\\n\", \"\\n\".join(inspect.getsource(FaissBackend._row_to_result).splitlines()[:6]))"
      ],
      "metadata": {
        "id": "cyMpJiQYaHGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6249e19-0afd-4974-e83d-e1a3a1860383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row_to_result signature: (self, row: 'pd.Series', score: 'float', rank: 'int') -> 'Result'\n",
            "Source snippet:\n",
            "     def _row_to_result(self, row: pd.Series, score: float, rank: int) -> Result:\n",
            "        # choose best available text field\n",
            "        txt = (row.get(\"chunk_text\") or row.get(\"text\") or row.get(\"preview\") or row.get(\"content\") or \"\")\n",
            "        cid = row.get(\"chunk_id\")\n",
            "        try:\n",
            "            cid = int(cid) if cid is not None and cid == cid else None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smoke test"
      ],
      "metadata": {
        "id": "DZ94KI5OGndJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "for m in list(sys.modules):\n",
        "    if m.startswith(\"pipeline.\"):\n",
        "        del sys.modules[m]\n",
        "\n",
        "from pipeline.runner import load_pipeline\n",
        "\n",
        "P = load_pipeline(\"configs/index/config.json\")  # uses active_index\n",
        "hits = P.search_one(\"Explain PageRank algorithm\", top_k=5)\n",
        "print(f\"Got {len(hits)} hits. First title:\", hits[0][\"title\"], \"| source:\", hits[0][\"source\"])\n",
        "\n",
        "hits_pubmed = P.search_one(\"metformin mechanism of action\", top_k=5, filter_source=\"pubmed\")\n",
        "print(\"pubmed first hit:\", hits_pubmed[0][\"source\"], hits_pubmed[0][\"title\"])\n",
        "\n",
        "P.close()"
      ],
      "metadata": {
        "id": "X132DbL8J_dJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd7ba3a-4be2-415e-91dc-e4c7c6b30aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAISS] Using 12 threads\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 5 hits. First title: Search engine | source: wikipedia\n",
            "pubmed first hit: pubmed The beneficial effects of empagliflozin-metformin combination on cardiovascular risk factors in type 2 diabetes mellitus patients.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.3 â€” Sharded FAISS + Global Top-K Merge (â€œLocal Fan-Outâ€)"
      ],
      "metadata": {
        "id": "kvl8IejbRmBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective\n",
        "\n",
        "Splits the multi-domain FAISS datastore into multiple shards (e.g., 8), then extend the pipeline so that a single query is:\n",
        "\n",
        "broadcast to each shard,\n",
        "\n",
        "searched locally (with efSearch â‰ˆ 64 - 128),\n",
        "\n",
        "merged globally using a heap-based top-K aggregator."
      ],
      "metadata": {
        "id": "dTLuIv_XRpAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shard the Existing Multi Index**\n",
        "\n",
        "Script: scripts/shard_index.py\n",
        "\n",
        "This takes:\n",
        "\n",
        "\n",
        "```\n",
        "data/embeddings/multi_default/multi_meta.parquet\n",
        "data/embeddings/multi_default/multi_vectors.npy\n",
        "```\n",
        "and produces:\n",
        "\n",
        "```\n",
        "data/embeddings/multi_default_sharded/meta_shard_000.parquet â€¦\n",
        "indices/multi_default_sharded/shard_000.faiss â€¦\n",
        "indices/multi_default_sharded/manifest.json\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "z5VQoFdsSTrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make folders\n",
        "!mkdir -p scripts\n",
        "!mkdir -p \"/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded\"\n",
        "!mkdir -p \"/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default_sharded\""
      ],
      "metadata": {
        "id": "Yg_YzBK9SzOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/shard_index.py\n",
        "\"\"\"\n",
        "Shard the merged FAISS dataset into N shards with metric parity + manifest.\n",
        "\n",
        "Usage (typical):\n",
        "    python scripts/shard_index.py --n_shards 8 \\\n",
        "        --metric ip --normalize 1 --M 32 --efC 200 --split hash --seed 42 \\\n",
        "        --save_vectors 0\n",
        "\n",
        "Inputs (fixed paths under project_root):\n",
        "    data/embeddings/multi_default/multi_meta.parquet\n",
        "    data/embeddings/multi_default/multi_vectors.npy\n",
        "\n",
        "Outputs:\n",
        "    data/embeddings/multi_default_sharded/meta_shard_000.parquet ...\n",
        "    indices/multi_default_sharded/shard_000.faiss ...\n",
        "    indices/multi_default_sharded/manifest.json\n",
        "    # (optional, only if --save_vectors 1)\n",
        "    data/embeddings/multi_default_sharded/vecs_shard_000.npy ...\n",
        "\"\"\"\n",
        "\n",
        "import os, json, argparse, time, hashlib, sys\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "\n",
        "\n",
        "project_root = \"/content/drive/MyDrive/AMS 560 PROJECT\"\n",
        "EMB_DIR  = f\"{project_root}/data/embeddings/multi_default\"\n",
        "OUT_EMB  = f\"{project_root}/data/embeddings/multi_default_sharded\"\n",
        "OUT_IDX  = f\"{project_root}/indices/multi_default_sharded\"\n",
        "os.makedirs(OUT_EMB, exist_ok=True)\n",
        "os.makedirs(OUT_IDX, exist_ok=True)\n",
        "\n",
        "\n",
        "def _normalize_rows(x: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(x, axis=1, keepdims=True)\n",
        "    return x / np.maximum(norms, 1e-12)\n",
        "\n",
        "\n",
        "def _hnsw_index(dim: int, M: int, metric: str):\n",
        "    \"\"\"\n",
        "    Build IndexHNSWFlat with the requested metric.\n",
        "    For FAISS versions without metric in constructor, fall back & try to set it.\n",
        "    \"\"\"\n",
        "    metric = metric.lower()\n",
        "    if metric not in {\"ip\", \"l2\"}:\n",
        "        raise ValueError(\"metric must be 'ip' or 'l2'\")\n",
        "    METRIC = faiss.METRIC_INNER_PRODUCT if metric == \"ip\" else faiss.METRIC_L2\n",
        "\n",
        "    # Try modern signature (d, M, metric)\n",
        "    try:\n",
        "        return faiss.IndexHNSWFlat(dim, M, METRIC)\n",
        "    except TypeError:\n",
        "        idx = faiss.IndexHNSWFlat(dim, M)\n",
        "        # Some builds expose metric_type; if not, accept default (L2)\n",
        "        try:\n",
        "            idx.metric_type = METRIC  # may no-op on older builds\n",
        "        except Exception:\n",
        "            if metric == \"ip\":\n",
        "                print(\"[WARN] Index metric could not be set to IP; \"\n",
        "                      \"ensure vectors are L2-normalized and use IP-compatible search.\")\n",
        "        return idx\n",
        "\n",
        "\n",
        "def _hash_bucket(val: str, n: int) -> int:\n",
        "    \"\"\"Stable shard id via SHA1(id) % n for better topic balance.\"\"\"\n",
        "    h = hashlib.sha1(val.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "    return int(h[:8], 16) % n\n",
        "\n",
        "\n",
        "def _value_counts_int(series: pd.Series) -> dict:\n",
        "    \"\"\"Return a plain dict of str->int counts (safe for JSON).\"\"\"\n",
        "    vc = series.value_counts(dropna=False)\n",
        "    # Convert NaN to \"(null)\" for readability\n",
        "    keys = [(\"(null)\" if (isinstance(k, float) and np.isnan(k)) else str(k)) for k in vc.index.tolist()]\n",
        "    vals = [int(v) for v in vc.values.tolist()]\n",
        "    return dict(zip(keys, vals))\n",
        "\n",
        "\n",
        "def shard_index(\n",
        "    n_shards: int = 8,\n",
        "    metric: str = \"ip\",\n",
        "    normalize: int = 1,\n",
        "    M: int = 32,\n",
        "    efC: int = 200,\n",
        "    split: str = \"hash\",  # 'hash' or 'contiguous'\n",
        "    seed: int = 42,\n",
        "    save_vectors: int = 0,  # 1 = save vecs_shard_XXX.npy (optional)\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    meta_pq = f\"{EMB_DIR}/multi_meta.parquet\"\n",
        "    vec_np  = f\"{EMB_DIR}/multi_vectors.npy\"\n",
        "\n",
        "    if not os.path.exists(meta_pq) or not os.path.exists(vec_np):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing inputs:\\n  {meta_pq if os.path.exists(meta_pq) else '(not found)'}\\n\"\n",
        "            f\"  {vec_np if os.path.exists(vec_np) else '(not found)'}\"\n",
        "        )\n",
        "\n",
        "    print(\"Loading meta & vectors ...\")\n",
        "    meta = pd.read_parquet(meta_pq)\n",
        "    vecs = np.load(vec_np).astype(\"float32\")\n",
        "    if len(meta) != len(vecs):\n",
        "        raise ValueError(f\"meta rows {len(meta)} != vectors {len(vecs)}\")\n",
        "\n",
        "    N, dim = vecs.shape\n",
        "    print(f\"Loaded {N:,} vectors (dim={dim})\")\n",
        "\n",
        "    # Decide domain column (for diagnostics)\n",
        "    domain_col = \"domain\" if \"domain\" in meta.columns else (\"source\" if \"source\" in meta.columns else None)\n",
        "    if domain_col is None:\n",
        "        print(\"[INFO] No 'domain' or 'source' column found; domain counts will be omitted.\")\n",
        "\n",
        "    # Optional normalization (recommended for IP/cosine semantics)\n",
        "    do_norm = bool(int(normalize))\n",
        "    if do_norm:\n",
        "        print(\"Normalizing vectors (L2) ...\")\n",
        "        vecs = _normalize_rows(vecs)\n",
        "\n",
        "    # Compute shard assignment\n",
        "    print(f\"Sharding strategy: {split}  |  n_shards={n_shards}\")\n",
        "    if split == \"hash\":\n",
        "        if \"id\" not in meta.columns:\n",
        "            print(\"[WARN] meta has no 'id' column; using contiguous splits.\")\n",
        "            split = \"contiguous\"\n",
        "        else:\n",
        "            assign = meta[\"id\"].astype(str).map(lambda s: _hash_bucket(s, n_shards)).to_numpy()\n",
        "    if split == \"contiguous\":\n",
        "        shard_size = int(np.ceil(N / n_shards))\n",
        "        assign = np.repeat(np.arange(n_shards), shard_size)[:N]\n",
        "\n",
        "    # Global domain counts (pre-sharding)\n",
        "    global_domain_counts = None\n",
        "    if domain_col is not None:\n",
        "        global_domain_counts = _value_counts_int(meta[domain_col])\n",
        "\n",
        "    # Manifest skeleton\n",
        "    manifest = {\n",
        "        \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"faiss_version\": getattr(faiss, \"__version__\", \"unknown\"),\n",
        "        \"n_total\": int(N),\n",
        "        \"n_shards\": int(n_shards),\n",
        "        \"dim\": int(dim),\n",
        "        \"normalized\": bool(do_norm),\n",
        "        \"index_kind\": \"hnsw_flat\",\n",
        "        \"index_params\": {\"M\": int(M), \"efConstruction\": int(efC)},\n",
        "        \"index_metric\": metric.lower(),  # logical metric used when building\n",
        "        \"split_strategy\": split,\n",
        "        \"seed\": int(seed),\n",
        "        \"save_vectors\": bool(int(save_vectors)),\n",
        "        \"domain_counts_total\": global_domain_counts if global_domain_counts is not None else {},\n",
        "        \"shards\": [],\n",
        "    }\n",
        "\n",
        "    # Build each shard\n",
        "    pbar = tqdm(range(n_shards), desc=\"Building shards\")\n",
        "    for i in pbar:\n",
        "        mask = (assign == i)\n",
        "        n_i = int(mask.sum())\n",
        "        if n_i == 0:\n",
        "            sub_meta = meta.iloc[0:0].copy()\n",
        "            sub_vecs = np.zeros((0, dim), dtype=\"float32\")\n",
        "        else:\n",
        "            sub_meta = meta.loc[mask].reset_index(drop=True)\n",
        "            sub_vecs = vecs[mask].copy()\n",
        "\n",
        "        shard_id = f\"{i:03d}\"\n",
        "        meta_p = f\"{OUT_EMB}/meta_shard_{shard_id}.parquet\"\n",
        "        idx_p  = f\"{OUT_IDX}/shard_{shard_id}.faiss\"\n",
        "        vec_p  = f\"{OUT_EMB}/vecs_shard_{shard_id}.npy\"\n",
        "\n",
        "        # Save meta\n",
        "        sub_meta.to_parquet(meta_p)\n",
        "\n",
        "        # Build FAISS HNSW\n",
        "        index = _hnsw_index(dim, M, metric)\n",
        "        try:\n",
        "            index.hnsw.efConstruction = int(efC)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if n_i > 0:\n",
        "            index.add(sub_vecs)\n",
        "\n",
        "        # Persist index\n",
        "        faiss.write_index(index, idx_p)\n",
        "\n",
        "        # Optional: save vectors for this shard\n",
        "        vectors_path = None\n",
        "        if int(save_vectors) and n_i > 0:\n",
        "            np.save(vec_p, sub_vecs)\n",
        "            vectors_path = vec_p\n",
        "\n",
        "        # Domain counts for this shard\n",
        "        shard_domain_counts = {}\n",
        "        if (domain_col is not None) and (n_i > 0):\n",
        "            shard_domain_counts = _value_counts_int(sub_meta[domain_col])\n",
        "\n",
        "        manifest[\"shards\"].append({\n",
        "            \"id\": shard_id,\n",
        "            \"n\": n_i,\n",
        "            \"meta_path\": meta_p,\n",
        "            \"index_path\": idx_p,\n",
        "            **({\"vectors_path\": vectors_path} if vectors_path else {}),\n",
        "            \"domain_counts\": shard_domain_counts\n",
        "        })\n",
        "        pbar.set_postfix({\"shard\": shard_id, \"rows\": n_i})\n",
        "\n",
        "    # Write manifest\n",
        "    man_path = f\"{OUT_IDX}/manifest.json\"\n",
        "    with open(man_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    print(\"Manifest written:\", man_path)\n",
        "    print(\"Sum(ntotal) =\", sum(s[\"n\"] for s in manifest[\"shards\"]))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--n_shards\", type=int, default=8)\n",
        "    ap.add_argument(\"--metric\", type=str, default=\"ip\", choices=[\"ip\", \"l2\"])\n",
        "    ap.add_argument(\"--normalize\", type=int, default=1)   # 1=True (normalize rows)\n",
        "    ap.add_argument(\"--M\", type=int, default=32)\n",
        "    ap.add_argument(\"--efC\", type=int, default=200)\n",
        "    ap.add_argument(\"--split\", type=str, default=\"hash\", choices=[\"hash\", \"contiguous\"])\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--save_vectors\", type=int, default=0)  # 1 to save vecs_shard_XXX.npy\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    shard_index(\n",
        "        n_shards=args.n_shards,\n",
        "        metric=args.metric,\n",
        "        normalize=args.normalize,\n",
        "        M=args.M,\n",
        "        efC=args.efC,\n",
        "        split=args.split,\n",
        "        seed=args.seed,\n",
        "        save_vectors=args.save_vectors,\n",
        "    )"
      ],
      "metadata": {
        "id": "BtOmTDcCSWqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3479f422-c0b7-4727-ef54-d523bc201f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/shard_index.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the sharder (balanced, IP, normalized)"
      ],
      "metadata": {
        "id": "VhUpIAZXNSWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/shard_index.py --n_shards 8 --metric ip --normalize 1 --M 32 --efC 200 --split hash --seed 42 --save_vectors 1"
      ],
      "metadata": {
        "id": "vgSiZc30S4Cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55c9dcf-e6f3-4d07-e6ab-3e6446fc4092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta & vectors ...\n",
            "Loaded 79,164 vectors (dim=384)\n",
            "Normalizing vectors (L2) ...\n",
            "Sharding strategy: hash  |  n_shards=8\n",
            "Building shards: 100% 8/8 [00:42<00:00,  5.37s/it, shard=007, rows=9858]\n",
            "Manifest written: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded/manifest.json\n",
            "Sum(ntotal) = 79164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update configs/index/config.json using the manifest (with scaling knobs)"
      ],
      "metadata": {
        "id": "zH9McxBVT-9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/AMS 560 PROJECT\"\n",
        "CONFIG_PATH = os.path.join(ROOT, \"configs/index/config.json\")\n",
        "MANIFEST_PATH = os.path.join(ROOT, \"indices/multi_default_sharded/manifest.json\")\n",
        "\n",
        "# Helpers\n",
        "def load_json(p):\n",
        "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def save_json(p, obj):\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def ensure_cfg_skeleton(cfg):\n",
        "    if \"indices\" not in cfg or not isinstance(cfg[\"indices\"], dict):\n",
        "        cfg[\"indices\"] = {}\n",
        "    if \"config_version\" not in cfg:\n",
        "        cfg[\"config_version\"] = \"1\"\n",
        "    return cfg\n",
        "\n",
        "# Read manifest\n",
        "man = load_json(MANIFEST_PATH)\n",
        "idx_paths = [s[\"index_path\"] for s in man[\"shards\"]]\n",
        "meta_paths = [s[\"meta_path\"]  for s in man[\"shards\"]]\n",
        "\n",
        "assert len(idx_paths) == len(meta_paths) > 0, \"manifest index/meta length mismatch\"\n",
        "\n",
        "# Load or create config\n",
        "cfg = load_json(CONFIG_PATH) if os.path.exists(CONFIG_PATH) else {}\n",
        "cfg = ensure_cfg_skeleton(cfg)\n",
        "\n",
        "# Build/replace the multi_sharded block with knobs aligned to Step 3.2\n",
        "cfg[\"indices\"][\"multi_sharded\"] = {\n",
        "    \"backend\": \"faiss\",\n",
        "    \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
        "    \"normalize_query\": True,               # queries normalized at search time\n",
        "    \"metric\": \"ip\",                        # requested similarity metric (runtime)\n",
        "    \"index_metric\": man.get(\"index_metric\", \"ip\"),  # metric used at build time\n",
        "    \"vector_normed\": bool(man.get(\"normalized\", True)),\n",
        "    \"efSearch\": 128,\n",
        "    \"index_paths\": idx_paths,\n",
        "    \"meta_paths\": meta_paths,\n",
        "    \"extras\": {\n",
        "        \"alpha\": 3,\n",
        "        \"dedup_by_uid\": True,\n",
        "        \"faiss_num_threads\": os.cpu_count() or 8,\n",
        "        \"text_fields\": [\"chunk_text\", \"text\", \"preview\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Make sharded active\n",
        "cfg[\"active_index\"] = \"multi_sharded\"\n",
        "cfg[\"config_version\"] = \"1\"\n",
        "\n",
        "save_json(CONFIG_PATH, cfg)\n",
        "\n",
        "print(\"âœ“ Updated config at:\", CONFIG_PATH)\n",
        "print(\"\\n--- multi_sharded summary ---\")\n",
        "print(\"active_index:\", cfg.get(\"active_index\"))\n",
        "print(\"#index_paths:\", len(cfg[\"indices\"][\"multi_sharded\"][\"index_paths\"]))\n",
        "print(\"#meta_paths :\", len(cfg[\"indices\"][\"multi_sharded\"][\"meta_paths\"]))\n",
        "print(\"index_metric:\", cfg[\"indices\"][\"multi_sharded\"][\"index_metric\"], \"| vector_normed:\", cfg[\"indices\"][\"multi_sharded\"][\"vector_normed\"])"
      ],
      "metadata": {
        "id": "BzZ7pjDeUBMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d2fc06-e69a-4473-b3a9-3d4468106eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Updated config at: /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n",
            "\n",
            "--- multi_sharded summary ---\n",
            "active_index: multi_sharded\n",
            "#index_paths: 8\n",
            "#meta_paths : 8\n",
            "index_metric: ip | vector_normed: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation"
      ],
      "metadata": {
        "id": "NYbWbED7UIm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pipeline.runner import load_pipeline\n",
        "\n",
        "CONFIG_PATH = \"/content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\"\n",
        "\n",
        "P = load_pipeline(CONFIG_PATH)  # uses active_index=multi_sharded\n",
        "hits = P.search_one(\"Explain PageRank algorithm\", top_k=5)\n",
        "print(f\"Got {len(hits)} hits (sharded). First title:\", hits[0][\"title\"], \"| source:\", hits[0][\"source\"])\n",
        "P.close()"
      ],
      "metadata": {
        "id": "b-ZCvjD9UGlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a41af8-1867-405d-969b-7f91b6b81abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAISS] Using 12 threads\n",
            "Got 5 hits (sharded). First title: Search engine | source: wikipedia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Sharded Retrieval"
      ],
      "metadata": {
        "id": "ihp5kGivUSVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pipeline.runner import load_pipeline\n",
        "import json\n",
        "\n",
        "P = load_pipeline(\"/content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\", index_key=\"multi_sharded\")\n",
        "hits = P.search_one(\"explain PageRank algorithm\", top_k=5)\n",
        "print(f\"Got {len(hits)} hits (sharded). First:\", hits[0])\n",
        "P.close()\n",
        "\n",
        "manifest = json.load(open(\"/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded/manifest.json\"))\n",
        "print(\"Sum(ntotal) =\", sum(s[\"n\"] for s in manifest[\"shards\"]))"
      ],
      "metadata": {
        "id": "ACpXB7VoUU_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2398c16d-e8cf-4ab3-abb2-1002f5de2f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAISS] Using 12 threads\n",
            "Got 5 hits (sharded). First: {'rank': 1, 'score': 0.7310386300086975, 'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528', 'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c13', 'source': 'wikipedia', 'title': 'Search engine', 'url': 'https://en.wikipedia.org/wiki/Search%20engine', 'text': '', 'chunk_id': 13}\n",
            "Sum(ntotal) = 79164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set project root so paths resolve"
      ],
      "metadata": {
        "id": "HXe6GPSzOe_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pathlib\n",
        "\n",
        "# ğŸ‘‡ change this if your project lives somewhere else\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/AMS 560 PROJECT\"\n",
        "os.makedirs(f\"{PROJECT_ROOT}/configs/index\", exist_ok=True)\n",
        "\n",
        "CFG_PATH      = f\"{PROJECT_ROOT}/configs/index/config.json\"\n",
        "MANIFEST_PATH = f\"{PROJECT_ROOT}/indices/multi_default_sharded/manifest.json\"  # from Step 3.3/3.6\n",
        "print(\"Config path:\", CFG_PATH)\n",
        "print(\"Manifest:\", MANIFEST_PATH, \"exists?\", os.path.exists(MANIFEST_PATH))"
      ],
      "metadata": {
        "id": "S52ZXLwMOZYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8c20a8-03b0-4331-f7db-f4778ebb4430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config path: /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n",
            "Manifest: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded/manifest.json exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "helper: load/patch/save config JSON"
      ],
      "metadata": {
        "id": "wB6jxl6_Pr9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(p):\n",
        "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def save_json(p, obj):\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "    print(\"âœ… wrote\", p)\n",
        "\n",
        "def ensure_cfg_skeleton(cfg):\n",
        "    if \"indices\" not in cfg: cfg[\"indices\"] = {}\n",
        "    if \"config_version\" not in cfg: cfg[\"config_version\"] = \"1\"\n",
        "    return cfg"
      ],
      "metadata": {
        "id": "2t2kBW6HOhJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Read manifest (handles both formats)\n",
        "man = load_json(MANIFEST_PATH)\n",
        "\n",
        "if \"index_paths\" in man and \"meta_paths\" in man:\n",
        "    # Flat list format (already there)\n",
        "    index_paths = man[\"index_paths\"]\n",
        "    meta_paths  = man[\"meta_paths\"]\n",
        "else:\n",
        "    # Shard list format â†’ extract\n",
        "    shards = man.get(\"shards\", [])\n",
        "    if not shards:\n",
        "        raise ValueError(\"Manifest missing 'shards' array.\")\n",
        "    index_paths = [s[\"index_path\"] for s in shards]\n",
        "    meta_paths  = [s[\"meta_path\"]  for s in shards]\n",
        "\n",
        "assert len(index_paths) == len(meta_paths) > 0, \"manifest index/meta length mismatch\"\n",
        "\n",
        "# --- Load or create config\n",
        "cfg = load_json(CFG_PATH) if os.path.exists(CFG_PATH) else {}\n",
        "cfg = ensure_cfg_skeleton(cfg)\n",
        "\n",
        "# --- Fill the multi_sharded block with the new knobs\n",
        "cfg[\"indices\"][\"multi_sharded\"] = {\n",
        "    \"backend\": \"faiss\",\n",
        "    \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
        "    \"normalize_query\": True,         # queries normalized at search time\n",
        "    \"metric\": \"ip\",                  # requested similarity metric\n",
        "    \"index_metric\": \"ip\",            # actual FAISS index semantics\n",
        "    \"vector_normed\": True,           # your corpus vectors were L2-normalized at build time\n",
        "    \"efSearch\": 128,\n",
        "    \"index_paths\": index_paths,\n",
        "    \"meta_paths\": meta_paths,\n",
        "    \"extras\": {\n",
        "        \"alpha\": 3,\n",
        "        \"dedup_by_uid\": True,\n",
        "        \"faiss_num_threads\": os.cpu_count() or 8,\n",
        "        \"text_fields\": [\"chunk_text\", \"text\", \"preview\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "cfg[\"active_index\"]   = \"multi_sharded\"\n",
        "cfg[\"config_version\"] = \"1\"\n",
        "\n",
        "save_json(CFG_PATH, cfg)\n",
        "\n",
        "print(\"âœ“ Patched config:\", CFG_PATH)\n",
        "print(\"  shards:\", len(index_paths))\n",
        "print(\"  sample index:\", index_paths[0])\n",
        "print(\"  sample meta :\", meta_paths[0])"
      ],
      "metadata": {
        "id": "3LMkFPn6Ojdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a859873-861e-4dd1-a49a-080ecc2a16b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… wrote /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n",
            "âœ“ Patched config: /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n",
            "  shards: 8\n",
            "  sample index: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded/shard_000.faiss\n",
            "  sample meta : /content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default_sharded/meta_shard_000.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "patch the sharded index block (indices.multi_sharded)"
      ],
      "metadata": {
        "id": "h2-1hRsiPuec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FAISS_SINGLE = f\"{PROJECT_ROOT}/indices/multi_default/multi_hnsw.faiss\"\n",
        "META_SINGLE  = f\"{PROJECT_ROOT}/data/embeddings/multi_default/multi_meta.parquet\"\n",
        "\n",
        "cfg = load_json(CFG_PATH)\n",
        "cfg = ensure_cfg_skeleton(cfg)\n",
        "\n",
        "cfg[\"indices\"][\"multi_default\"] = {\n",
        "    \"backend\": \"faiss\",\n",
        "    \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
        "    \"normalize_query\": True,\n",
        "    \"metric\": \"ip\",\n",
        "    \"index_metric\": \"ip\",      # leave 'ip' since corpus is normalized\n",
        "    \"vector_normed\": True,     # âœ… your corpus was normalized\n",
        "    \"efSearch\": 128,\n",
        "    \"faiss_path\": FAISS_SINGLE,\n",
        "    \"meta_path\": META_SINGLE,\n",
        "    \"extras\": {\n",
        "        \"faiss_num_threads\": os.cpu_count() or 8,\n",
        "        \"text_fields\": [\"chunk_text\", \"text\", \"preview\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# (Optional) switch active_index:\n",
        "# cfg[\"active_index\"] = \"multi_default\"\n",
        "\n",
        "save_json(CFG_PATH, cfg)"
      ],
      "metadata": {
        "id": "g5xbPk_XPh11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e04bd2-de59-43d0-8b3e-3aa9b9531548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… wrote /content/drive/MyDrive/AMS 560 PROJECT/configs/index/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "verify config loads cleanly with your loader"
      ],
      "metadata": {
        "id": "lOo7pwtXP0tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses your Step 3.1 loader (already in your repo)\n",
        "from pipeline.config import load_config\n",
        "raw, nic = load_config(CFG_PATH)  # uses active_index\n",
        "print(\"Active index:\", raw.get(\"active_index\"))\n",
        "print(\"Index metric:\", nic.index_metric, \"| vector_normed:\", nic.vector_normed, \"| normalize_query:\", nic.normalize_query)\n",
        "print(\"Sharded?\" , bool(nic.index_paths))\n",
        "if nic.index_paths:\n",
        "    print(\"#shards:\", len(nic.index_paths))"
      ],
      "metadata": {
        "id": "4u6-Q25kPkq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d255217-d031-4592-cddf-295832834fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active index: multi_sharded\n",
            "Index metric: ip | vector_normed: True | normalize_query: True\n",
            "Sharded? True\n",
            "#shards: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.4 â€” Chroma Backend Integration"
      ],
      "metadata": {
        "id": "7wNz9GhGVg3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective**\n",
        "\n",
        "Create a Chroma collection (multi_default) using merged embeddings and metadata,\n",
        "and add a backend implementation compatible with existing pipeline abstraction."
      ],
      "metadata": {
        "id": "OsfG4ig4VowR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "FpB2sgZRlhdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p scripts"
      ],
      "metadata": {
        "id": "HH-mxlufVocM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"configs/index\", exist_ok=True)\n",
        "os.makedirs(\"pipeline/backends\", exist_ok=True)\n",
        "os.makedirs(\"scripts\", exist_ok=True)\n",
        "os.makedirs(\"indices/chroma_store\", exist_ok=True)  # persistent Chroma dir on Drive"
      ],
      "metadata": {
        "id": "JWuXdHitlGJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config loader that permits non-FAISS backends"
      ],
      "metadata": {
        "id": "NbeDEyQrlRHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/config.py\n",
        "\"\"\"\n",
        "Config loader for retrieval pipeline (scaling-ready).\n",
        "\n",
        "- Supports JSON (.json) and YAML (.yml/.yaml)\n",
        "- Validates index blocks (single-index or sharded) for FAISS backends\n",
        "- Normalizes keys so backends see a consistent shape\n",
        "- Adds schema versioning and metric/normalization semantics\n",
        "- Supports remote paths (s3://, gs://, hdfs://) and optional path checks\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import yaml  # type: ignore\n",
        "except Exception as e:\n",
        "    raise ImportError(\n",
        "        \"pyyaml is required. Install with: pip install pyyaml\"\n",
        "    ) from e\n",
        "\n",
        "\n",
        "class ConfigError(Exception):\n",
        "    \"\"\"Raised when configuration is missing/invalid.\"\"\"\n",
        "\n",
        "\n",
        "# Accepted config schema versions (simple gate for now)\n",
        "_ACCEPTED_CONFIG_VERSIONS = {\"1\"}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class NormalizedIndexConfig:\n",
        "    # core knobs\n",
        "    backend: str\n",
        "    model_name: str\n",
        "    normalize_query: bool\n",
        "    metric: str\n",
        "    efSearch: int\n",
        "\n",
        "    # union of single vs sharded (for FAISS backends)\n",
        "    faiss_path: Optional[str] = None\n",
        "    meta_path: Optional[str] = None\n",
        "    index_paths: Optional[List[str]] = None\n",
        "    meta_paths: Optional[List[str]] = None\n",
        "\n",
        "    # semantics (for cross-backend parity & validation)\n",
        "    index_metric: Optional[str] = None         # 'ip' | 'l2' | 'cosine'\n",
        "    vector_normed: Optional[bool] = None       # True if stored vectors are unit-normalized\n",
        "\n",
        "    # free-form passthrough (if backends need extras)\n",
        "    extras: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    # loader context (useful for validation/telemetry)\n",
        "    index_key: Optional[str] = None\n",
        "    config_version: Optional[str] = None\n",
        "    skip_path_checks: bool = False\n",
        "\n",
        "\n",
        "def _load_raw_config(path: str) -> Dict[str, Any]:\n",
        "    if not os.path.exists(path):\n",
        "        raise ConfigError(f\"Config file not found: {path}\")\n",
        "    _, ext = os.path.splitext(path.lower())\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            if ext == \".json\":\n",
        "                return json.load(f)\n",
        "            if ext in (\".yml\", \".yaml\"):\n",
        "                return yaml.safe_load(f)\n",
        "            # try JSON first then YAML if unknown extension\n",
        "            try:\n",
        "                f.seek(0)\n",
        "                return json.load(f)\n",
        "            except Exception:\n",
        "                f.seek(0)\n",
        "                return yaml.safe_load(f)\n",
        "    except Exception as e:\n",
        "        raise ConfigError(f\"Failed to parse config {path}: {e}\") from e\n",
        "\n",
        "\n",
        "def _require(d: Dict[str, Any], key: str, ctx: str) -> Any:\n",
        "    if key not in d:\n",
        "        raise ConfigError(f\"Missing required key '{key}' in {ctx}\")\n",
        "    return d[key]\n",
        "\n",
        "\n",
        "def _as_bool(v: Any, default: bool) -> bool:\n",
        "    if v is None:\n",
        "        return default\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if isinstance(v, (int, float)):\n",
        "        return bool(v)\n",
        "    if isinstance(v, str):\n",
        "        s = v.strip().lower()\n",
        "        if s in {\"1\", \"true\", \"yes\", \"y\", \"on\"}:\n",
        "            return True\n",
        "        if s in {\"0\", \"false\", \"no\", \"n\", \"off\"}:\n",
        "            return False\n",
        "    raise ConfigError(f\"Expected boolean-like value, got: {v!r}\")\n",
        "\n",
        "\n",
        "def _as_int(v: Any, default: int) -> int:\n",
        "    if v is None:\n",
        "        return default\n",
        "    if isinstance(v, int):\n",
        "        return v\n",
        "    if isinstance(v, float):\n",
        "        return int(v)\n",
        "    if isinstance(v, str):\n",
        "        s = v.strip()\n",
        "        try:\n",
        "            # tolerate \"128 \", \"1e2\", etc.\n",
        "            return int(float(s))\n",
        "        except Exception:\n",
        "            pass\n",
        "    raise ConfigError(f\"Expected integer-like value, got: {v!r}\")\n",
        "\n",
        "\n",
        "def _is_remote_path(p: str) -> bool:\n",
        "    p = (p or \"\").lower()\n",
        "    return p.startswith(\"s3://\") or p.startswith(\"gs://\") or p.startswith(\"hdfs://\")\n",
        "\n",
        "\n",
        "def _validate_paths_exist(paths: List[str], ctx: str, *, skip_checks: bool) -> None:\n",
        "    if skip_checks:\n",
        "        return\n",
        "    missing = [\n",
        "        p for p in paths\n",
        "        if (not _is_remote_path(p)) and (not os.path.exists(p))\n",
        "    ]\n",
        "    if missing:\n",
        "        raise ConfigError(f\"{ctx}: missing files:\\n  - \" + \"\\n  - \".join(missing))\n",
        "\n",
        "\n",
        "def _normalize_index_block(\n",
        "    block: Dict[str, Any],\n",
        "    ctx: str,\n",
        "    index_key: str,\n",
        "    top_level_skip_checks: bool,\n",
        ") -> NormalizedIndexConfig:\n",
        "    # knobs\n",
        "    backend = (block.get(\"backend\") or \"faiss\").lower()\n",
        "    model_name = block.get(\"model_name\", \"BAAI/bge-small-en-v1.5\")\n",
        "    metric = (block.get(\"metric\") or \"ip\").lower()\n",
        "    efSearch = _as_int(block.get(\"efSearch\"), 64)\n",
        "    normalize_query = _as_bool(block.get(\"normalize_query\"), True)\n",
        "\n",
        "    # semantics (extra flags that help enforce parity across backends)\n",
        "    index_metric = (block.get(\"index_metric\") or metric or \"\").lower() or None\n",
        "    vector_normed = block.get(\"vector_normed\")\n",
        "    if vector_normed is not None:\n",
        "        vector_normed = _as_bool(vector_normed, False)\n",
        "\n",
        "    # support single-index OR sharded-index (for FAISS backends)\n",
        "    faiss_path = block.get(\"faiss_path\")\n",
        "    meta_path = block.get(\"meta_path\")\n",
        "    index_paths = block.get(\"index_paths\")\n",
        "    meta_paths = block.get(\"meta_paths\")\n",
        "\n",
        "    # per-block override for skipping path checks (e.g. cluster/remote mode)\n",
        "    skip_path_checks = _as_bool(block.get(\"skip_path_checks\"), top_level_skip_checks)\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # Only enforce single/sharded + path checks for FAISS backends.\n",
        "    # For other backends (e.g., \"chroma\"), faiss_path/index_paths\n",
        "    # are optional and can be omitted entirely.\n",
        "    # -------------------------------------------------------------\n",
        "    if backend == \"faiss\":\n",
        "        single_ok = bool(faiss_path and meta_path)\n",
        "        shard_ok = bool(index_paths and meta_paths)\n",
        "\n",
        "        if single_ok and shard_ok:\n",
        "            raise ConfigError(\n",
        "                f\"{ctx}: Provide EITHER single (faiss_path/meta_path) OR \"\n",
        "                f\"sharded (index_paths/meta_paths), not both.\"\n",
        "            )\n",
        "        if not (single_ok or shard_ok):\n",
        "            raise ConfigError(\n",
        "                f\"{ctx}: Must provide single (faiss_path+meta_path) OR \"\n",
        "                f\"sharded (index_paths+meta_paths).\"\n",
        "            )\n",
        "\n",
        "        # path existence checks\n",
        "        if single_ok:\n",
        "            _validate_paths_exist(\n",
        "                [faiss_path, meta_path],\n",
        "                f\"{ctx} (single)\",\n",
        "                skip_checks=skip_path_checks,\n",
        "            )\n",
        "        else:\n",
        "            if not isinstance(index_paths, list) or not isinstance(meta_paths, list):\n",
        "                raise ConfigError(\n",
        "                    f\"{ctx}: index_paths/meta_paths must be lists for sharded mode.\"\n",
        "                )\n",
        "            if len(index_paths) != len(meta_paths):\n",
        "                raise ConfigError(\n",
        "                    f\"{ctx}: index_paths and meta_paths length mismatch \"\n",
        "                    f\"({len(index_paths)} vs {len(meta_paths)}).\"\n",
        "                )\n",
        "            _validate_paths_exist(\n",
        "                index_paths,\n",
        "                f\"{ctx} index_paths\",\n",
        "                skip_checks=skip_path_checks,\n",
        "            )\n",
        "            _validate_paths_exist(\n",
        "                meta_paths,\n",
        "                f\"{ctx} meta_paths\",\n",
        "                skip_checks=skip_path_checks,\n",
        "            )\n",
        "\n",
        "    # everything not core becomes extras\n",
        "    extras = {\n",
        "        k: v\n",
        "        for k, v in block.items()\n",
        "        if k\n",
        "        not in {\n",
        "            \"backend\",\n",
        "            \"model_name\",\n",
        "            \"normalize_query\",\n",
        "            \"metric\",\n",
        "            \"efSearch\",\n",
        "            \"faiss_path\",\n",
        "            \"meta_path\",\n",
        "            \"index_paths\",\n",
        "            \"meta_paths\",\n",
        "            \"index_metric\",\n",
        "            \"vector_normed\",\n",
        "            \"skip_path_checks\",\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return NormalizedIndexConfig(\n",
        "        backend=backend,\n",
        "        model_name=model_name,\n",
        "        normalize_query=normalize_query,\n",
        "        metric=metric,\n",
        "        efSearch=efSearch,\n",
        "        faiss_path=faiss_path,\n",
        "        meta_path=meta_path,\n",
        "        index_paths=index_paths,\n",
        "        meta_paths=meta_paths,\n",
        "        index_metric=index_metric,\n",
        "        vector_normed=vector_normed,\n",
        "        extras=extras,\n",
        "        index_key=index_key,\n",
        "        config_version=None,         # filled by load_config\n",
        "        skip_path_checks=skip_path_checks,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_config(path: str, index_key: Optional[str] = None) -> Tuple[Dict[str, Any], NormalizedIndexConfig]:\n",
        "    \"\"\"\n",
        "    Returns (raw_config_dict, normalized_index_cfg) for the chosen index key.\n",
        "    - If index_key is None, uses raw['active_index'].\n",
        "    - Validates presence and correctness of the selected index block.\n",
        "    - Enforces schema version and provides remote-path/skip-check behavior.\n",
        "    \"\"\"\n",
        "    raw = _load_raw_config(path)\n",
        "    if not isinstance(raw, dict):\n",
        "        raise ConfigError(\"Top-level config must be an object/dict.\")\n",
        "\n",
        "    # Schema versioning\n",
        "    cfg_version = str(raw.get(\"config_version\", \"1\")).strip()\n",
        "    if cfg_version not in _ACCEPTED_CONFIG_VERSIONS:\n",
        "        raise ConfigError(\n",
        "            f\"Unsupported config_version='{cfg_version}'. \"\n",
        "            f\"Accepted versions: {_ACCEPTED_CONFIG_VERSIONS}\"\n",
        "        )\n",
        "\n",
        "    indices = _require(raw, \"indices\", \"root\")\n",
        "    if not isinstance(indices, dict) or not indices:\n",
        "        raise ConfigError(\"'indices' must be a non-empty object.\")\n",
        "\n",
        "    if index_key is None:\n",
        "        index_key = raw.get(\"active_index\")\n",
        "        if not index_key:\n",
        "            raise ConfigError(\"Provide index_key or set 'active_index' in config.\")\n",
        "\n",
        "    if index_key not in indices:\n",
        "        raise ConfigError(f\"Index '{index_key}' not found in 'indices'.\")\n",
        "\n",
        "    block = indices[index_key]\n",
        "    if not isinstance(block, dict):\n",
        "        raise ConfigError(f\"Index '{index_key}' must be an object.\")\n",
        "\n",
        "    # allow a top-level default for skipping path checks (e.g., cluster mode)\n",
        "    top_level_skip_checks = _as_bool(raw.get(\"skip_path_checks\"), False)\n",
        "\n",
        "    nic = _normalize_index_block(\n",
        "        block,\n",
        "        f\"index '{index_key}'\",\n",
        "        index_key,\n",
        "        top_level_skip_checks,\n",
        "    )\n",
        "    # fill loader context fields\n",
        "    nic.config_version = cfg_version\n",
        "\n",
        "    if nic.index_metric and nic.metric and nic.index_metric != nic.metric:\n",
        "        import warnings\n",
        "        warnings.warn(\n",
        "            f\"Requested metric '{nic.metric}' differs from index_metric '{nic.index_metric}'. \"\n",
        "            \"Ensure query normalization / FAISS setup matches.\"\n",
        "        )\n",
        "\n",
        "    return raw, nic\n"
      ],
      "metadata": {
        "id": "RBPThkg2lTSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856ec736-ae0f-4ce0-926f-374b6a8873f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backend interface"
      ],
      "metadata": {
        "id": "i1-14Q12llEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/backends/base.py\n",
        "from __future__ import annotations\n",
        "from typing import Any, Dict, List, Protocol, runtime_checkable\n",
        "Result = Dict[str, Any]\n",
        "\n",
        "@runtime_checkable\n",
        "class VectorBackend(Protocol):\n",
        "    def load(self, cfg: Any) -> None: ...\n",
        "    def encode(self, text_or_list: Any) -> Any: ...\n",
        "    def search(self, qvec: Any, top_k: int = 5, filter_source: str | None = None) -> List[Result]: ...\n",
        "    def close(self) -> None: ..."
      ],
      "metadata": {
        "id": "QiOZkReYllfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab46452f-b3d7-4ed8-dd5f-7341244d0533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/backends/base.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chroma ingestion (build persistent collection)"
      ],
      "metadata": {
        "id": "6Fmk2mSYlnHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/chroma_ingest.py\n",
        "import os, argparse, json\n",
        "import numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
        "VECTORS_ARE_NORMALIZED = True  # âœ… you already normalized corpus vectors at build time\n",
        "\n",
        "def _build_unique_ids(meta: pd.DataFrame) -> pd.Series:\n",
        "    if \"chunk_id\" in meta.columns:\n",
        "        ch = meta[\"chunk_id\"].fillna(-1).astype(\"int64\").astype(str)\n",
        "        uids = meta[\"id\"].astype(str) + \"::c\" + ch\n",
        "    else:\n",
        "        uids = meta[\"id\"].astype(str) + \"::row\" + pd.Series(range(len(meta)), index=meta.index).astype(str)\n",
        "    if not uids.is_unique:\n",
        "        uids = uids + \"::\" + pd.Series(range(len(uids)), index=uids.index).astype(str)\n",
        "    return uids\n",
        "\n",
        "def _sanitize_metadata(meta: pd.DataFrame) -> list[dict]:\n",
        "    m = meta.copy()\n",
        "\n",
        "    # Normalize domain/source consistently (lowercase, strip)\n",
        "    if \"domain\" in m.columns:\n",
        "        m[\"domain\"] = m[\"domain\"].astype(str).str.strip().str.lower()\n",
        "    if \"source\" in m.columns:\n",
        "        m[\"source\"] = m[\"source\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Ensure common string fields are strings\n",
        "    for col in [\"title\", \"url\", \"source_id\", \"id\"]:\n",
        "        if col in m.columns:\n",
        "            m[col] = m[col].astype(str)\n",
        "\n",
        "    # chunk_id as int or None\n",
        "    if \"chunk_id\" in m.columns:\n",
        "        m[\"chunk_id\"] = m[\"chunk_id\"].apply(lambda x: int(x) if pd.notna(x) else None)\n",
        "\n",
        "    return m.to_dict(orient=\"records\")\n",
        "\n",
        "def ingest_chroma(project_root: str, collection_name: str, persist_dir: str):\n",
        "    emb_dir = f\"{project_root}/data/embeddings/multi_default\"\n",
        "    meta_pq = f\"{emb_dir}/multi_meta.parquet\"\n",
        "    vec_np  = f\"{emb_dir}/multi_vectors.npy\"\n",
        "    if not (os.path.exists(meta_pq) and os.path.exists(vec_np)):\n",
        "        raise FileNotFoundError(f\"Missing merged files:\\n- {meta_pq}\\n- {vec_np}\")\n",
        "\n",
        "    meta = pd.read_parquet(meta_pq)\n",
        "    vecs = np.load(vec_np).astype(\"float32\")\n",
        "    if len(meta) != len(vecs):\n",
        "        raise ValueError(f\"meta/vector length mismatch: {len(meta)} vs {len(vecs)}\")\n",
        "\n",
        "    # Normalize domain/source columns BEFORE batching (helps server-side filtering)\n",
        "    if \"domain\" in meta.columns:\n",
        "        meta[\"domain\"] = meta[\"domain\"].astype(str).str.strip().str.lower()\n",
        "    elif \"source\" in meta.columns:\n",
        "        meta[\"source\"] = meta[\"source\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "    os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "    client = chromadb.PersistentClient(path=persist_dir, settings=Settings(anonymized_telemetry=False))\n",
        "\n",
        "    # Clean create the collection\n",
        "    try:\n",
        "        if collection_name in [c.name for c in client.list_collections()]:\n",
        "            client.delete_collection(collection_name)\n",
        "    except Exception:\n",
        "        pass\n",
        "    coll = client.create_collection(name=collection_name)\n",
        "\n",
        "    ids = _build_unique_ids(meta)\n",
        "    print(f\"Ingesting {len(meta):,} into '{collection_name}' @ {persist_dir}\")\n",
        "    bs = 1000\n",
        "    for i in tqdm(range(0, len(meta), bs)):\n",
        "        batch = meta.iloc[i:i+bs]\n",
        "        coll.add(\n",
        "            ids=ids.iloc[i:i+bs].tolist(),\n",
        "            embeddings=vecs[i:i+bs].tolist(),\n",
        "            metadatas=_sanitize_metadata(batch)\n",
        "        )\n",
        "\n",
        "    info = {\n",
        "        \"collection\": collection_name,\n",
        "        \"persist_dir\": persist_dir,\n",
        "        \"n_records\": len(meta),\n",
        "        \"fields\": list(meta.columns),\n",
        "        \"id_format\": \"uid = id::c<chunk_id> (or ::row<i>)\",\n",
        "        \"model_name\": MODEL_NAME,\n",
        "        \"vector_normed\": VECTORS_ARE_NORMALIZED,\n",
        "        \"metric\": \"cosine\",  # Chroma uses cosine distance\n",
        "        \"score_semantics\": \"distance=1-cosine_similarity; report similarity=1-distance\"\n",
        "    }\n",
        "    man_path = f\"{project_root}/indices/{collection_name}_chroma_manifest.json\"\n",
        "    with open(man_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(info, f, indent=2)\n",
        "    print(\"âœ“ Manifest:\", man_path)\n",
        "    print(\"âœ“ Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--collection\", default=\"multi_default\")\n",
        "    ap.add_argument(\"--persist_dir\", required=True)\n",
        "    args = ap.parse_args()\n",
        "    ingest_chroma(\"/content/drive/MyDrive/AMS 560 PROJECT\", args.collection, args.persist_dir)"
      ],
      "metadata": {
        "id": "NZiJwKoPlpB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb898d0-c88c-4529-f350-ae0f7bae38c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/chroma_ingest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run ingestion"
      ],
      "metadata": {
        "id": "Efxp8lXEp6-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/chroma_ingest.py --collection multi_default --persist_dir \"indices/chroma_store\""
      ],
      "metadata": {
        "id": "B1EnlSMmp7gG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05508b76-a6aa-460c-bc5d-5e72623a446c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "Ingesting 79,164 into 'multi_default' @ indices/chroma_store\n",
            "  0% 0/80 [00:00<?, ?it/s]Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
            "100% 80/80 [04:10<00:00,  3.13s/it]\n",
            "âœ“ Manifest: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_chroma_manifest.json\n",
            "âœ“ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chroma backend"
      ],
      "metadata": {
        "id": "yHd1xdcXrDsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/backends/chroma_backend.py\n",
        "\"\"\"\n",
        "Chroma backend implementation matching VectorBackend interface.\n",
        "- Converts Chroma cosine distance â†’ similarity (score = 1 - distance)\n",
        "- Validates collection/model/normalization parity via manifest\n",
        "- Adaptive overfetch when filtering; de-dup by uid\n",
        "- Stable uid mapping: returns both 'uid' and base 'id'\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from typing import Any, List, Dict, Optional\n",
        "\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from pipeline.backends.base import VectorBackend, Result\n",
        "\n",
        "\n",
        "class ChromaBackend(VectorBackend):\n",
        "    def __init__(self):\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "        self.model: Optional[SentenceTransformer] = None\n",
        "        self.cfg = None\n",
        "        self.text_fields_pref: List[str] = [\"chunk_text\", \"text\", \"preview\"]\n",
        "        self.alpha: int = 3\n",
        "        self.dedup_by_uid: bool = True\n",
        "        self.collection_name: str = \"multi_default\"\n",
        "        self.persist_dir: Optional[str] = None\n",
        "        self.manifest: Dict[str, Any] = {}\n",
        "\n",
        "    def _resolve_extras(self, cfg) -> dict:\n",
        "        ex = getattr(cfg, \"extras\", {}) or {}\n",
        "        if isinstance(ex, dict) and \"persist_dir\" not in ex and \"extras\" in ex and isinstance(ex[\"extras\"], dict):\n",
        "            ex = ex[\"extras\"]\n",
        "        return ex\n",
        "\n",
        "    def _load_manifest_if_exists(self):\n",
        "        # Try to read indices/<collection>_chroma_manifest.json for parity checks\n",
        "        if not self.persist_dir:\n",
        "            return\n",
        "        man_path = os.path.join(os.path.dirname(self.persist_dir.rstrip(\"/\")), f\"{self.collection_name}_chroma_manifest.json\")\n",
        "        # If persist_dir is \".../indices/chroma_store\", manifest path above points to \".../indices/multi_default_chroma_manifest.json\"\n",
        "        alt_path = os.path.join(self.persist_dir, f\"{self.collection_name}_chroma_manifest.json\")\n",
        "        for p in [man_path, alt_path]:\n",
        "            if os.path.exists(p):\n",
        "                try:\n",
        "                    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "                        self.manifest = json.load(f)\n",
        "                except Exception:\n",
        "                    self.manifest = {}\n",
        "                break\n",
        "\n",
        "    def _assert_parity(self):\n",
        "        # Only assert if we found a manifest\n",
        "        if not self.manifest:\n",
        "            return\n",
        "        want_model = getattr(self.cfg, \"model_name\", None)\n",
        "        have_model = self.manifest.get(\"model_name\")\n",
        "        if want_model and have_model and want_model != have_model:\n",
        "            raise RuntimeError(\n",
        "                f\"Model mismatch for Chroma collection '{self.collection_name}': \"\n",
        "                f\"config.model_name='{want_model}' vs manifest.model_name='{have_model}'. \"\n",
        "                f\"Re-ingest with scripts/chroma_ingest.py or switch config.\"\n",
        "            )\n",
        "        # Vector normalization parity\n",
        "        vec_normed = self.manifest.get(\"vector_normed\")\n",
        "        if vec_normed is not None and not bool(vec_normed):\n",
        "            # You said your corpus is normalized; if manifest claims otherwise, warn/raise.\n",
        "            raise RuntimeError(\n",
        "                f\"Manifest indicates vector_normed={vec_normed} but config expects normalized corpus vectors. \"\n",
        "                f\"Rebuild or verify ingestion.\"\n",
        "            )\n",
        "        # Count sanity (best-effort)\n",
        "        try:\n",
        "            coll_count = self.collection.count()\n",
        "            man_count = int(self.manifest.get(\"n_records\", coll_count))\n",
        "            if abs(coll_count - man_count) > 0:\n",
        "                raise RuntimeError(\n",
        "                    f\"Collection count ({coll_count}) != manifest n_records ({man_count}). \"\n",
        "                    f\"Re-ingest to synchronize.\"\n",
        "                )\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def load(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        ex = self._resolve_extras(cfg)\n",
        "\n",
        "        self.persist_dir = ex.get(\"persist_dir\")\n",
        "        if not self.persist_dir:\n",
        "            raise RuntimeError(\"Chroma backend requires extras.persist_dir in config.\")\n",
        "\n",
        "        self.collection_name = ex.get(\"collection\", \"multi_default\")\n",
        "        self.alpha = int(ex.get(\"alpha\", 3))\n",
        "        self.dedup_by_uid = bool(ex.get(\"dedup_by_uid\", True))\n",
        "        # optional text field preference\n",
        "        if isinstance(ex.get(\"text_fields\"), list) and ex[\"text_fields\"]:\n",
        "            self.text_fields_pref = list(ex[\"text_fields\"])\n",
        "\n",
        "        # Persistent client\n",
        "        os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
        "        self.client = chromadb.PersistentClient(path=self.persist_dir, settings=Settings(anonymized_telemetry=False))\n",
        "\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(self.collection_name)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                f\"Chroma collection '{self.collection_name}' not found in '{self.persist_dir}'. \"\n",
        "                f\"Run: python scripts/chroma_ingest.py --collection {self.collection_name} --persist_dir '{self.persist_dir}'\"\n",
        "            ) from e\n",
        "\n",
        "        # Model\n",
        "        self.model = SentenceTransformer(cfg.model_name)\n",
        "\n",
        "        # Parity checks\n",
        "        self._load_manifest_if_exists()\n",
        "        self._assert_parity()\n",
        "\n",
        "        print(f\"Loaded Chroma collection: {self.collection_name} | count={self.collection.count()} | store={self.persist_dir}\")\n",
        "\n",
        "    def encode(self, text_or_list):\n",
        "        if isinstance(text_or_list, str):\n",
        "            texts = [text_or_list]\n",
        "        else:\n",
        "            texts = list(text_or_list)\n",
        "        vecs = self.model.encode(\n",
        "            texts,\n",
        "            normalize_embeddings=getattr(self.cfg, \"normalize_query\", True)\n",
        "        ).astype(\"float32\")\n",
        "        return vecs\n",
        "\n",
        "    def _pick_text(self, meta: Dict[str, Any]) -> str:\n",
        "        for k in self.text_fields_pref:\n",
        "            if k in meta and meta[k]:\n",
        "                return meta[k]\n",
        "        return \"\"\n",
        "\n",
        "    def _split_uid(self, uid: str) -> (str, Optional[int]):\n",
        "        # uid format: \"<id>::c<chunk_id>\" or \"<id>::row<i>\"\n",
        "        if \"::c\" in uid:\n",
        "            base, suf = uid.split(\"::c\", 1)\n",
        "            try:\n",
        "                return base, int(suf)\n",
        "            except Exception:\n",
        "                return base, None\n",
        "        if \"::row\" in uid:\n",
        "            base, _ = uid.split(\"::row\", 1)\n",
        "            return base, None\n",
        "        return uid, None\n",
        "\n",
        "    def search(self, qvec, top_k: int = 5, filter_source: str | None = None) -> List[Result]:\n",
        "    # Chroma returns cosine distances; convert to similarity for consistency\n",
        "      overfetch = top_k * (self.alpha if filter_source else 1)\n",
        "      out: List[Result] = []\n",
        "      seen_uid = set()\n",
        "\n",
        "      # --- Normalize filter_source for case-insensitive matching ---\n",
        "      fs = filter_source.strip().lower() if filter_source else None\n",
        "      where = {\"domain\": fs} if fs else None\n",
        "\n",
        "      query = qvec[0].tolist()\n",
        "      results = self.collection.query(\n",
        "          query_embeddings=[query],\n",
        "          n_results=overfetch,\n",
        "          where=where\n",
        "      )\n",
        "\n",
        "      ids = results.get(\"ids\", [[]])[0]\n",
        "      dists = results.get(\"distances\", [[]])[0]  # cosine distance\n",
        "      metas = results.get(\"metadatas\", [[]])[0]\n",
        "\n",
        "      for uid, dist, md in zip(ids, dists, metas):\n",
        "          if uid is None:\n",
        "              continue\n",
        "          sim = 1.0 - float(dist)  # similarity = 1 - distance\n",
        "          meta = md or {}\n",
        "\n",
        "          # Extract source robustly and compare case-insensitively\n",
        "          source = (meta.get(\"domain\") or meta.get(\"source\") or \"\").strip().lower()\n",
        "          if fs and source != fs:\n",
        "              continue\n",
        "\n",
        "          if self.dedup_by_uid:\n",
        "              if uid in seen_uid:\n",
        "                  continue\n",
        "              seen_uid.add(uid)\n",
        "\n",
        "          base_id, chunk_id = self._split_uid(uid)\n",
        "\n",
        "          out.append({\n",
        "              \"rank\": len(out) + 1,\n",
        "              \"score\": sim,                          # report similarity (higher is better)\n",
        "              \"id\": base_id,                         # base document id\n",
        "              \"uid\": uid,                            # stable chunk-level id\n",
        "              \"source\": source,                      # already normalized lower-case\n",
        "              \"title\": meta.get(\"title\", \"\"),\n",
        "              \"url\": meta.get(\"url\", \"\"),\n",
        "              \"text\": self._pick_text(meta),\n",
        "              \"chunk_id\": chunk_id\n",
        "          })\n",
        "          if len(out) >= top_k:\n",
        "              break\n",
        "      return out\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        try:\n",
        "            if self.client and hasattr(self.client, \"reset\"):\n",
        "                self.client.reset()\n",
        "        except Exception:\n",
        "            pass\n",
        "        self.client = None\n",
        "        self.collection = None\n",
        "        self.model = None\n"
      ],
      "metadata": {
        "id": "4pQVKZGprFRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0bcdff-b449-4ed0-c11f-33494f38ce84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/backends/chroma_backend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimal runner (register Chroma)"
      ],
      "metadata": {
        "id": "9YCrPqB_rJsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/runner.py\n",
        "from __future__ import annotations\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "from pipeline.config import load_config, NormalizedIndexConfig\n",
        "from pipeline.backends.base import VectorBackend\n",
        "from pipeline.backends.chroma_backend import ChromaBackend\n",
        "\n",
        "_REGISTRY = {\n",
        "    \"chroma\": ChromaBackend,\n",
        "    # \"faiss\": FaissBackend,  # plug later if needed\n",
        "}\n",
        "\n",
        "class Pipeline:\n",
        "    def __init__(self, backend: VectorBackend, nicfg: NormalizedIndexConfig):\n",
        "        self.backend = backend\n",
        "        self.nicfg = nicfg\n",
        "\n",
        "    def search_one(self, query: str, top_k: int = 5, filter_source: str | None = None) -> List[Dict[str, Any]]:\n",
        "        qvec = self.backend.encode(query)\n",
        "        return self.backend.search(qvec, top_k=top_k, filter_source=filter_source)\n",
        "\n",
        "    def search_many(self, queries: List[str], top_k: int = 5, filter_source: str | None = None) -> Dict[str, List[Dict[str, Any]]]:\n",
        "        qvecs = self.backend.encode(queries)\n",
        "        out = {}\n",
        "        for i, q in enumerate(queries):\n",
        "            out[q] = self.backend.search(qvecs[i:i+1], top_k=top_k, filter_source=filter_source)\n",
        "        return out\n",
        "\n",
        "    def close(self):\n",
        "        self.backend.close()\n",
        "\n",
        "def load_pipeline(cfg_path: str, index_key: Optional[str] = None, backend_override: Optional[str] = None) -> Pipeline:\n",
        "    raw, nic = load_config(cfg_path, index_key=index_key)\n",
        "    bkey = (backend_override or nic.backend or \"chroma\").lower()\n",
        "    if bkey not in _REGISTRY:\n",
        "        raise ValueError(f\"Unknown backend '{bkey}'. Available: {list(_REGISTRY)}\")\n",
        "    backend = _REGISTRY[bkey]()  # type: ignore\n",
        "    backend.load(nic)\n",
        "    return Pipeline(backend, nic)"
      ],
      "metadata": {
        "id": "mpvo3UBkrKs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050e0974-549e-40fa-ef58-6f05658b9ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/runner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config JSON for Chroma (make it active)"
      ],
      "metadata": {
        "id": "EkWLZ80JrNSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "cfg_path = \"configs/index/config.json\"\n",
        "with open(cfg_path) as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "# Ensure indices exists\n",
        "cfg.setdefault(\"indices\", {})\n",
        "\n",
        "# Ensure a multi_chroma block exists and has the required fields\n",
        "mc = cfg[\"indices\"].get(\"multi_chroma\", {})\n",
        "mc.setdefault(\"backend\", \"chroma\")\n",
        "mc.setdefault(\"model_name\", \"BAAI/bge-small-en-v1.5\")\n",
        "mc.setdefault(\"normalize_query\", True)\n",
        "mc.setdefault(\"metric\", \"ip\")\n",
        "mc.setdefault(\"efSearch\", 64)\n",
        "\n",
        "# >>>>> THIS IS THE IMPORTANT PART <<<<<\n",
        "extras = mc.get(\"extras\", {})\n",
        "extras.setdefault(\"collection\", \"multi_default\")\n",
        "extras[\"persist_dir\"] = \"indices/chroma_store\"   # REQUIRED\n",
        "extras.setdefault(\"alpha\", 3)                    # optional overfetch for filtered queries\n",
        "extras.setdefault(\"dedup_by_uid\", True)          # optional de-dup behavior\n",
        "extras.setdefault(\"text_fields\", [\"chunk_text\",\"text\",\"preview\"])  # optional field preference\n",
        "mc[\"extras\"] = extras\n",
        "\n",
        "cfg[\"indices\"][\"multi_chroma\"] = mc\n",
        "cfg[\"active_index\"] = \"multi_chroma\"\n",
        "\n",
        "with open(cfg_path, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(\"Patched config at:\", cfg_path)\n",
        "print(json.dumps(cfg[\"indices\"][\"multi_chroma\"], indent=2))"
      ],
      "metadata": {
        "id": "QjGSYy14rOfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "accef37d-64e5-428b-a628-b57c38f42cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched config at: configs/index/config.json\n",
            "{\n",
            "  \"backend\": \"chroma\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"normalize_query\": true,\n",
            "  \"metric\": \"ip\",\n",
            "  \"efSearch\": 64,\n",
            "  \"extras\": {\n",
            "    \"collection\": \"multi_default\",\n",
            "    \"persist_dir\": \"indices/chroma_store\",\n",
            "    \"alpha\": 3,\n",
            "    \"dedup_by_uid\": true,\n",
            "    \"text_fields\": [\n",
            "      \"chunk_text\",\n",
            "      \"text\",\n",
            "      \"preview\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload & sanity test (Chroma)"
      ],
      "metadata": {
        "id": "fyeWFLq2rQ3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/pipeline"
      ],
      "metadata": {
        "id": "5m-Gm5N4S-id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/AMS 560 PROJECT\""
      ],
      "metadata": {
        "id": "rU583QmJS_hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13aa423c-7489-45cd-d08d-02ba09d4727b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AMS 560 PROJECT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pipeline.config\n",
        "print(pipeline.config.__file__)"
      ],
      "metadata": {
        "id": "3l2UDHPGTbDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc715db-60d5-461e-d56f-96710dcc5d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AMS 560 PROJECT/pipeline/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "for m in list(sys.modules):\n",
        "    if m.startswith(\"pipeline.\"):\n",
        "        del sys.modules[m]\n",
        "\n",
        "from pipeline.runner import load_pipeline\n",
        "\n",
        "P = load_pipeline(\"configs/index/config.json\")  # active_index=multi_chroma\n",
        "hits = P.search_one(\"Explain PageRank algorithm\", top_k=5)\n",
        "print(f\"Got {len(hits)} hits (Chroma). First:\", hits[0][\"title\"], \"| source:\", hits[0][\"source\"])\n",
        "\n",
        "hits_pubmed = P.search_one(\"metformin mechanism of action\", top_k=5, filter_source=\"pubmed\")\n",
        "print(\"pubmed first:\", hits_pubmed[0][\"source\"], \"|\", hits_pubmed[0][\"title\"])\n",
        "\n",
        "P.close()"
      ],
      "metadata": {
        "id": "PRoDe65trR5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919f3549-a2d6-4931-e109-310e98d9762d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Chroma collection: multi_default | count=79164 | store=indices/chroma_store\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 5 hits (Chroma). First: Search engine | source: wikipedia\n",
            "pubmed first: pubmed | The beneficial effects of empagliflozin-metformin combination on cardiovascular risk factors in type 2 diabetes mellitus patients.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3.5 â€” Benchmark harness + logging**"
      ],
      "metadata": {
        "id": "mmU1MuCLzPZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure both backends are registered in the runner"
      ],
      "metadata": {
        "id": "3JmtU1gFzaJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pipeline/runner.py\n",
        "from __future__ import annotations\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "from pipeline.config import load_config, NormalizedIndexConfig\n",
        "from pipeline.backends.base import VectorBackend\n",
        "from pipeline.backends.chroma_backend import ChromaBackend\n",
        "from pipeline.backends.faiss_backend import FaissBackend  # ensure this file exists\n",
        "\n",
        "_REGISTRY = {\n",
        "    \"chroma\": ChromaBackend,\n",
        "    \"faiss\": FaissBackend,\n",
        "}\n",
        "\n",
        "class Pipeline:\n",
        "    def __init__(self, backend: VectorBackend, nicfg: NormalizedIndexConfig):\n",
        "        self.backend = backend\n",
        "        self.nicfg = nicfg\n",
        "\n",
        "    def search_one(self, query: str, top_k: int = 5, filter_source: str | None = None) -> List[Dict[str, Any]]:\n",
        "        qvec = self.backend.encode(query)\n",
        "        return self.backend.search(qvec, top_k=top_k, filter_source=filter_source)\n",
        "\n",
        "    def search_many(self, queries: List[str], top_k: int = 5, filter_source: str | None = None) -> Dict[str, List[Dict[str, Any]]]:\n",
        "        qvecs = self.backend.encode(queries)\n",
        "        out = {}\n",
        "        for i, q in enumerate(queries):\n",
        "            out[q] = self.backend.search(qvecs[i:i+1], top_k=top_k, filter_source=filter_source)\n",
        "        return out\n",
        "\n",
        "    def close(self):\n",
        "        self.backend.close()\n",
        "\n",
        "def load_pipeline(cfg_path: str, index_key: Optional[str] = None, backend_override: Optional[str] = None) -> Pipeline:\n",
        "    raw, nic = load_config(cfg_path, index_key=index_key)\n",
        "    bkey = (backend_override or nic.backend or \"faiss\").lower()\n",
        "    if bkey not in _REGISTRY:\n",
        "        raise ValueError(f\"Unknown backend '{bkey}'. Available: {list(_REGISTRY)}\")\n",
        "    backend = _REGISTRY[bkey]()  # type: ignore\n",
        "    backend.load(nic)\n",
        "    return Pipeline(backend, nic)"
      ],
      "metadata": {
        "id": "01zU29rnzcLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd6401ab-12b3-4cfa-a113-f0a84898b94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pipeline/runner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bench.py (timestamped outputs, env capture, score semantics, optional quality)"
      ],
      "metadata": {
        "id": "-b9m5YrUzfSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf results/multi_sharded"
      ],
      "metadata": {
        "id": "_mJG_9wsNe4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bench.py\n",
        "\n",
        "import argparse, os, sys, json, time, csv, math, datetime, hashlib, platform\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Optional libs for env capture\n",
        "try:\n",
        "    import faiss\n",
        "    _FAISS_OK = True\n",
        "except Exception:\n",
        "    _FAISS_OK = False\n",
        "\n",
        "try:\n",
        "    import chromadb\n",
        "    _CHROMA_VER = getattr(chromadb, \"__version__\", \"unknown\")\n",
        "except Exception:\n",
        "    _CHROMA_VER = \"unknown\"\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import __version__ as _ST_VER\n",
        "except Exception:\n",
        "    _ST_VER = \"unknown\"\n",
        "\n",
        "from pipeline.runner import load_pipeline\n",
        "from pipeline.config import load_config, ConfigError\n",
        "\n",
        "# ---------------- util ---------------- #\n",
        "\n",
        "def read_queries(qfile: str) -> List[str]:\n",
        "    qs = []\n",
        "    with open(qfile, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            ln = ln.strip()\n",
        "            if ln:\n",
        "                qs.append(ln)\n",
        "    if not qs:\n",
        "        raise SystemExit(f\"No queries found in {qfile}\")\n",
        "    return qs\n",
        "\n",
        "def sha1_of_json(obj: Any) -> str:\n",
        "    s = json.dumps(obj, sort_keys=True, ensure_ascii=False)\n",
        "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n",
        "\n",
        "def percentile(values: List[float], p: float) -> float:\n",
        "    if not values:\n",
        "        return 0.0\n",
        "    values_sorted = sorted(values)\n",
        "    k = (len(values_sorted)-1) * (p/100.0)\n",
        "    f = math.floor(k); c = math.ceil(k)\n",
        "    if f == c: return values_sorted[int(k)]\n",
        "    d0 = values_sorted[f] * (c-k); d1 = values_sorted[c] * (k-f)\n",
        "    return d0 + d1\n",
        "\n",
        "def flat_hits_rows(query, hits, iter_id):\n",
        "    rows = []\n",
        "    for h in hits:\n",
        "        rows.append({\n",
        "            \"query\": query,\n",
        "            \"iter\": iter_id,\n",
        "            \"rank\": h.get(\"rank\"),\n",
        "            \"score\": h.get(\"score\"),\n",
        "            \"id\": h.get(\"id\"),\n",
        "            \"uid\": h.get(\"uid\"),\n",
        "            \"source\": h.get(\"source\"),\n",
        "            \"title\": h.get(\"title\"),\n",
        "            \"url\": h.get(\"url\"),\n",
        "            \"chunk_id\": h.get(\"chunk_id\"),\n",
        "            \"text\": h.get(\"text\"),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def load_truth_map(path: Optional[str]) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Optional gold file (JSON) mapping query -> list of relevant uids/ids.\n",
        "    Example:\n",
        "      {\"Explain PageRank algorithm\": [\"doc123::c0\",\"doc456::c2\", ...], ...}\n",
        "    \"\"\"\n",
        "    if not path: return {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def compute_recall_mrr(hits_by_query: Dict[str, List[Dict[str, Any]]], gold: Dict[str, List[str]]) -> Tuple[Optional[float], Optional[float]]:\n",
        "    \"\"\"\n",
        "    Computes recall@k and MRR using uid if present, else falls back to id.\n",
        "    \"\"\"\n",
        "    recall_list = []\n",
        "    rr_list = []\n",
        "    for q, hits in hits_by_query.items():\n",
        "        if q not in gold:\n",
        "            continue\n",
        "        gold_set = set(gold[q])\n",
        "        if not gold_set:\n",
        "            continue\n",
        "        seq = [h.get(\"uid\") or h.get(\"id\") for h in hits]\n",
        "        seq = [x for x in seq if x]\n",
        "        hit_any = any(s in gold_set for s in seq)\n",
        "        if hit_any:\n",
        "            rank = None\n",
        "            for i, s in enumerate(seq, start=1):\n",
        "                if s in gold_set:\n",
        "                    rank = i\n",
        "                    break\n",
        "            rr_list.append(1.0 / rank if rank else 0.0)\n",
        "        else:\n",
        "            rr_list.append(0.0)\n",
        "        retrieved = set(seq)\n",
        "        inter = len(retrieved.intersection(gold_set))\n",
        "        recall_list.append(inter / max(1, len(gold_set)))\n",
        "    rec = round(float(np.mean(recall_list)), 4) if recall_list else None\n",
        "    mrr = round(float(np.mean(rr_list)), 4) if rr_list else None\n",
        "    return rec, mrr\n",
        "\n",
        "def _dedup_and_rerank(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Keep the best-scoring occurrence per UID (or per (id,chunk_id) fallback), then re-rank.\n",
        "    \"\"\"\n",
        "    best: Dict[str, Dict[str, Any]] = {}\n",
        "    for h in hits:\n",
        "        uid = h.get(\"uid\")\n",
        "        if not uid:\n",
        "            cid = h.get(\"chunk_id\")\n",
        "            uid = f\"{h.get('id','')}::c{int(cid) if cid is not None else -1}\"\n",
        "        prev = best.get(uid)\n",
        "        if (prev is None) or (float(h.get(\"score\", -1e30)) > float(prev.get(\"score\", -1e30))):\n",
        "            best[uid] = {**h, \"uid\": uid}\n",
        "    # Re-rank by (score desc, uid asc)\n",
        "    merged = list(best.values())\n",
        "    merged.sort(key=lambda r: (-float(r.get(\"score\", 0.0)), str(r.get(\"uid\",\"\"))))\n",
        "    for i, h in enumerate(merged, start=1):\n",
        "        h[\"rank\"] = i\n",
        "    return merged\n",
        "\n",
        "# ---------------- main bench logic ---------------- #\n",
        "\n",
        "def run_bench(\n",
        "    config_path: str,\n",
        "    index_key: str | None,\n",
        "    backend: str | None,\n",
        "    qfile: str,\n",
        "    top_k: int,\n",
        "    iters: int,\n",
        "    warmup: int,\n",
        "    filter_source: str | None,\n",
        "    out_format: str,\n",
        "    results_dir: str,\n",
        "    truth_json: Optional[str] = None,\n",
        "):\n",
        "    # Load raw config to determine effective index_key and some knobs for env capture\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_cfg = json.load(f)\n",
        "    effective_index = index_key or raw_cfg.get(\"active_index\")\n",
        "    if not effective_index:\n",
        "        raise SystemExit(\"No index specified and no active_index in config.\")\n",
        "    icfg = raw_cfg[\"indices\"][effective_index]\n",
        "    cfg_hash = sha1_of_json(icfg)\n",
        "\n",
        "    # Figure out score semantics (for summary)\n",
        "    metric = (icfg.get(\"metric\") or \"ip\").lower()\n",
        "    # FAISS returns similarity for ip/cosine; Chroma backend converts distance->similarity\n",
        "    score_type = \"similarity\" if metric in (\"ip\", \"cosine\") else \"distance\"\n",
        "\n",
        "    # Identify shard count (if sharded FAISS)\n",
        "    shard_count = None\n",
        "    if isinstance(icfg.get(\"index_paths\"), list):\n",
        "        shard_count = len(icfg.get(\"index_paths\"))\n",
        "\n",
        "    # Pull common knobs\n",
        "    ef_search = icfg.get(\"efSearch\")\n",
        "    extras = icfg.get(\"extras\", {})\n",
        "    alpha = extras.get(\"alpha\")\n",
        "    model_name = icfg.get(\"model_name\")\n",
        "\n",
        "    # Prepare timestamped run dir: results/<index_key>/<YYYYMMDD-HHMMSS>-<cfg_hash>/\n",
        "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    run_dir = Path(results_dir) / effective_index / f\"{ts}-{cfg_hash}\"\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "    logs_dir = run_dir / \"logs\"\n",
        "    logs_dir.mkdir(parents=True, exist_ok=True)\n",
        "    log_path = logs_dir / \"run.log\"\n",
        "\n",
        "    def log(msg: str):\n",
        "        print(msg)\n",
        "        with open(log_path, \"a\", encoding=\"utf-8\") as fh:\n",
        "            fh.write(msg + \"\\n\")\n",
        "\n",
        "    # Save a config snapshot and args for reproducibility\n",
        "    with open(run_dir / \"config_effective.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(icfg, f, indent=2)\n",
        "    args_snapshot = {\n",
        "        \"config\": config_path,\n",
        "        \"index\": index_key,\n",
        "        \"backend\": backend,\n",
        "        \"qfile\": qfile,\n",
        "        \"top_k\": top_k,\n",
        "        \"iters\": iters,\n",
        "        \"warmup\": warmup,\n",
        "        \"filter_source\": filter_source,\n",
        "        \"results_dir\": results_dir,\n",
        "        \"truth_json\": truth_json,\n",
        "    }\n",
        "    with open(run_dir / \"args.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(args_snapshot, f, indent=2)\n",
        "\n",
        "    # Echo basic setup\n",
        "    queries = read_queries(qfile)\n",
        "    n_q = len(queries)\n",
        "    log(f\"[bench] config={config_path} index={effective_index} backend={backend or '(config)'}\")\n",
        "    log(f\"[bench] queries={n_q} top_k={top_k} iters={iters} warmup={warmup} filter_source={filter_source or 'None'}\")\n",
        "    log(f\"[bench] run_dir={run_dir}\")\n",
        "\n",
        "    # Load pipeline once (measure)\n",
        "    t_load0 = time.perf_counter()\n",
        "    P = load_pipeline(config_path, index_key=effective_index, backend_override=backend)\n",
        "    t_load1 = time.perf_counter()\n",
        "    load_ms = (t_load1 - t_load0) * 1000.0\n",
        "    log(f\"[bench] pipeline loaded in {load_ms:.1f} ms\")\n",
        "\n",
        "    # Warmup\n",
        "    if warmup > 0:\n",
        "        log(f\"[bench] warmup passes={warmup}\")\n",
        "        for _ in range(warmup):\n",
        "            for q in queries:\n",
        "                _ = P.search_one(q, top_k=top_k, filter_source=filter_source)\n",
        "\n",
        "    # Measured runs\n",
        "    per_query_ms: List[float] = []\n",
        "    all_hits_for_mix: List[Dict[str, Any]] = []\n",
        "    hits_rows_accum: List[Dict[str, Any]] = []\n",
        "    hits_by_query_last_iter: Dict[str, List[Dict[str, Any]]] = {}\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    for it in range(iters):\n",
        "        log(f\"[bench] iter={it+1}/{iters}\")\n",
        "        for q in queries:\n",
        "            t_q0 = time.perf_counter()\n",
        "            raw_hits = P.search_one(q, top_k=top_k, filter_source=filter_source)\n",
        "            # de-dup + re-rank to ensure apples-to-apples\n",
        "            hits = _dedup_and_rerank(raw_hits)[:top_k]\n",
        "            t_q1 = time.perf_counter()\n",
        "\n",
        "            per_query_ms.append((t_q1 - t_q0) * 1000.0)\n",
        "            all_hits_for_mix.extend(hits)\n",
        "            hits_by_query_last_iter[q] = hits  # keep last iter (already deduped)\n",
        "            if out_format in (\"csv\", \"jsonl\"):\n",
        "                hits_rows_accum.extend(flat_hits_rows(q, hits, iter_id=it+1))\n",
        "    t1 = time.perf_counter()\n",
        "    P.close()\n",
        "\n",
        "    total_ms = (t1 - t0) * 1000.0\n",
        "    total_queries = n_q * iters\n",
        "    avg_ms = (sum(per_query_ms) / len(per_query_ms)) if per_query_ms else 0.0\n",
        "    p50_ms = percentile(per_query_ms, 50.0)\n",
        "    p90_ms = percentile(per_query_ms, 90.0)\n",
        "    qps = (total_queries / (total_ms / 1000.0)) if total_ms > 0 else 0.0\n",
        "\n",
        "    # Source mix (normalize)\n",
        "    mix_counts: Dict[str, int] = {}\n",
        "    for h in all_hits_for_mix:\n",
        "        src = (h.get(\"source\") or \"\").strip().lower() or \"unknown\"\n",
        "        mix_counts[src] = mix_counts.get(src, 0) + 1\n",
        "    total_hits = sum(mix_counts.values()) or 1\n",
        "    source_mix = {k: round(v / total_hits, 4) for k, v in sorted(mix_counts.items())}\n",
        "\n",
        "    # Quality probes (optional)\n",
        "    truth = load_truth_map(truth_json)\n",
        "    recall_at_k, mrr = compute_recall_mrr(hits_by_query_last_iter, truth) if truth else (None, None)\n",
        "\n",
        "    # Environment capture\n",
        "    env = {\n",
        "        \"python\": sys.version.split()[0],\n",
        "        \"platform\": platform.platform(),\n",
        "        \"numpy\": np.__version__,\n",
        "        \"faiss\": getattr(faiss, \"__version__\", \"unknown\") if _FAISS_OK else \"not_installed\",\n",
        "        \"faiss_threads\": int(getattr(faiss, \"omp_get_max_threads\", lambda: 0)()) if _FAISS_OK else None,\n",
        "        \"chromadb\": _CHROMA_VER,\n",
        "        \"sentence_transformers\": _ST_VER,\n",
        "        \"cpu_count\": os.cpu_count(),\n",
        "    }\n",
        "\n",
        "    summary = {\n",
        "        \"backend\": (backend or \"config.default\"),\n",
        "        \"index_key\": effective_index,\n",
        "        \"config_hash\": cfg_hash,\n",
        "        \"model_name\": model_name,\n",
        "        \"metric\": metric,\n",
        "        \"score_type\": score_type,                 # similarity|distance\n",
        "        \"efSearch\": ef_search,\n",
        "        \"alpha\": alpha,\n",
        "        \"shard_count\": shard_count,\n",
        "        \"n_queries\": n_q,\n",
        "        \"iters\": iters,\n",
        "        \"top_k\": top_k,\n",
        "        \"avg_ms\": round(avg_ms, 2),\n",
        "        \"p50_ms\": round(p50_ms, 2),\n",
        "        \"p90_ms\": round(p90_ms, 2),\n",
        "        \"qps\": round(qps, 2),\n",
        "        \"source_mix\": source_mix,\n",
        "        \"filter_source\": (filter_source.strip().lower() if filter_source else None),\n",
        "        \"total_elapsed_ms\": round(total_ms, 1),\n",
        "        \"quality\": {\n",
        "            \"recall_at_k\": recall_at_k,\n",
        "            \"mrr\": mrr\n",
        "        },\n",
        "        \"environment\": env,\n",
        "        \"run_dir\": str(run_dir),\n",
        "        \"hits_written\": bool(out_format in (\"csv\",\"jsonl\")),\n",
        "    }\n",
        "\n",
        "    # Write summary to timestamped run_dir\n",
        "    out_summary_path = run_dir / \"bench_summary.json\"\n",
        "    with open(out_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    log(f\"[bench] wrote summary â†’ {out_summary_path}\")\n",
        "\n",
        "    # Optional detailed hits (also under run_dir)\n",
        "    if out_format == \"jsonl\":\n",
        "        out_hits = run_dir / \"hits.jsonl\"\n",
        "        with open(out_hits, \"w\", encoding=\"utf-8\") as f:\n",
        "            for row in hits_rows_accum:\n",
        "                f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "        log(f\"[bench] wrote hits jsonl â†’ {out_hits}\")\n",
        "    elif out_format == \"csv\":\n",
        "        out_hits = run_dir / \"hits.csv\"\n",
        "        if hits_rows_accum:\n",
        "            fieldnames = list(hits_rows_accum[0].keys())\n",
        "            with open(out_hits, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                w.writeheader()\n",
        "                w.writerows(hits_rows_accum)\n",
        "        log(f\"[bench] wrote hits csv   â†’ {out_hits}\")\n",
        "\n",
        "    # Echo summary\n",
        "    print(json.dumps(summary, indent=2))\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--config\", default=\"configs/index/config.json\")\n",
        "    ap.add_argument(\"--index\", default=None, help=\"Index key in config (defaults to active_index)\")\n",
        "    ap.add_argument(\"--backend\", default=None, choices=[\"faiss\",\"chroma\"], help=\"Override backend\")\n",
        "    ap.add_argument(\"--qfile\", required=True, help=\"Newline-delimited queries file\")\n",
        "    ap.add_argument(\"--top_k\", type=int, default=10)\n",
        "    ap.add_argument(\"--iters\", type=int, default=1)\n",
        "    ap.add_argument(\"--warmup\", type=int, default=0)\n",
        "    ap.add_argument(\"--filter_source\", default=None, help=\"wikipedia | pubmed | stackexchange\")\n",
        "    ap.add_argument(\"--format\", default=\"jsonl\", choices=[\"jsonl\",\"csv\",\"none\"])\n",
        "    ap.add_argument(\"--results_dir\", default=\"results\")\n",
        "    ap.add_argument(\"--truth_json\", default=None, help=\"Optional JSON with gold uids/ids per query for recall/MRR\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    run_bench(\n",
        "        config_path=args.config,\n",
        "        index_key=args.index,\n",
        "        backend=args.backend,\n",
        "        qfile=args.qfile,\n",
        "        top_k=args.top_k,\n",
        "        iters=args.iters,\n",
        "        warmup=args.warmup,\n",
        "        filter_source=args.filter_source,\n",
        "        out_format=args.format if args.format != \"none\" else \"\",\n",
        "        results_dir=args.results_dir,\n",
        "        truth_json=args.truth_json,\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TzhVHuw2zheD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68950341-ee84-4497-a46c-c20cc9b0d3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bench.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bench.py \\\n",
        "  --config configs/index/config.json \\\n",
        "  --backend faiss \\\n",
        "  --index multi_sharded \\\n",
        "  --qfile data/queries/all_sources_eval.txt \\\n",
        "  --top_k 10 \\\n",
        "  --iters 1 \\\n",
        "  --warmup 0 \\\n",
        "  --format jsonl \\\n",
        "  --results_dir results"
      ],
      "metadata": {
        "id": "ZdoExNtaNouT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa64cb5-82c8-42af-e0bd-64a988fde897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867576.547531   13536 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867576.554042   13536 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867576.570345   13536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867576.570378   13536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867576.570382   13536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867576.570386   13536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[bench] config=configs/index/config.json index=multi_sharded backend=faiss\n",
            "[bench] queries=12 top_k=10 iters=1 warmup=0 filter_source=None\n",
            "[bench] run_dir=results/multi_sharded/20251204-165941-b807e0e73009\n",
            "[FAISS] Using 12 threads\n",
            "[bench] pipeline loaded in 3656.5 ms\n",
            "[bench] iter=1/1\n",
            "[bench] wrote summary â†’ results/multi_sharded/20251204-165941-b807e0e73009/bench_summary.json\n",
            "[bench] wrote hits jsonl â†’ results/multi_sharded/20251204-165941-b807e0e73009/hits.jsonl\n",
            "{\n",
            "  \"backend\": \"faiss\",\n",
            "  \"index_key\": \"multi_sharded\",\n",
            "  \"config_hash\": \"b807e0e73009\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 128,\n",
            "  \"alpha\": 3,\n",
            "  \"shard_count\": 8,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 1,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 36.9,\n",
            "  \"p50_ms\": 14.82,\n",
            "  \"p90_ms\": 16.19,\n",
            "  \"qps\": 26.97,\n",
            "  \"source_mix\": {\n",
            "    \"pubmed\": 0.2667,\n",
            "    \"stackexchange\": 0.3833,\n",
            "    \"wikipedia\": 0.35\n",
            "  },\n",
            "  \"filter_source\": null,\n",
            "  \"total_elapsed_ms\": 444.9,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_sharded/20251204-165941-b807e0e73009\",\n",
            "  \"hits_written\": true\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "quick sanity"
      ],
      "metadata": {
        "id": "_0WTL47oYE-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hard-reload pipeline modules\n",
        "import sys\n",
        "for m in list(sys.modules):\n",
        "    if m.startswith(\"pipeline.\"):\n",
        "        del sys.modules[m]\n",
        "\n",
        "from pipeline.runner import load_pipeline\n",
        "\n",
        "P = load_pipeline(\"configs/index/config.json\", index_key=\"multi_sharded\", backend_override=\"faiss\")\n",
        "hits = P.search_one(\"Explain PageRank algorithm\", top_k=5)\n",
        "print(\"Sharded OK, first title:\", hits[0][\"title\"], \"| source:\", hits[0][\"source\"])\n",
        "P.close()"
      ],
      "metadata": {
        "id": "ZNC0z1fM3OXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe926bed-2777-4684-9900-97b77f148bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAISS] Using 12 threads\n",
            "Sharded OK, first title: Search engine | source: wikipedia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run benchmarks (FAISS single / sharded / Chroma)"
      ],
      "metadata": {
        "id": "JRxRWMo0zuKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FAISS single (no filter)\n",
        "!python bench.py --config configs/index/config.json --backend faiss --index multi_default \\\n",
        "  --qfile data/queries/all_sources_eval.txt --top_k 10 --iters 3 --warmup 1 --results_dir results\n",
        "\n",
        "# FAISS sharded (if configured)\n",
        "!python bench.py --config configs/index/config.json --backend faiss --index multi_sharded \\\n",
        "  --qfile data/queries/all_sources_eval.txt --top_k 10 --iters 3 --warmup 1 --results_dir results\n",
        "\n",
        "# Chroma\n",
        "!python bench.py --config configs/index/config.json --backend chroma --index multi_chroma \\\n",
        "  --qfile data/queries/all_sources_eval.txt --top_k 10 --iters 3 --warmup 1 --results_dir results"
      ],
      "metadata": {
        "id": "NhSlB5btzwMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f876e806-0936-45d4-d2e8-2135a1c8652e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867598.848931   13726 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867598.855359   13726 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867598.871340   13726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867598.871369   13726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867598.871373   13726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867598.871375   13726 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[bench] config=configs/index/config.json index=multi_default backend=faiss\n",
            "[bench] queries=12 top_k=10 iters=3 warmup=1 filter_source=None\n",
            "[bench] run_dir=results/multi_default/20251204-170003-c96953b290c4\n",
            "[FAISS] Using 12 threads\n",
            "[bench] pipeline loaded in 3610.7 ms\n",
            "[bench] warmup passes=1\n",
            "[bench] iter=1/3\n",
            "[bench] iter=2/3\n",
            "[bench] iter=3/3\n",
            "[bench] wrote summary â†’ results/multi_default/20251204-170003-c96953b290c4/bench_summary.json\n",
            "[bench] wrote hits jsonl â†’ results/multi_default/20251204-170003-c96953b290c4/hits.jsonl\n",
            "{\n",
            "  \"backend\": \"faiss\",\n",
            "  \"index_key\": \"multi_default\",\n",
            "  \"config_hash\": \"c96953b290c4\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 128,\n",
            "  \"alpha\": null,\n",
            "  \"shard_count\": null,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 3,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 11.3,\n",
            "  \"p50_ms\": 11.21,\n",
            "  \"p90_ms\": 11.52,\n",
            "  \"qps\": 86.29,\n",
            "  \"source_mix\": {\n",
            "    \"pubmed\": 0.2667,\n",
            "    \"stackexchange\": 0.3833,\n",
            "    \"wikipedia\": 0.35\n",
            "  },\n",
            "  \"filter_source\": null,\n",
            "  \"total_elapsed_ms\": 417.2,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_default/20251204-170003-c96953b290c4\",\n",
            "  \"hits_written\": true\n",
            "}\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867616.674002   13864 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867616.680345   13864 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867616.695835   13864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867616.695861   13864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867616.695864   13864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867616.695867   13864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[bench] config=configs/index/config.json index=multi_sharded backend=faiss\n",
            "[bench] queries=12 top_k=10 iters=3 warmup=1 filter_source=None\n",
            "[bench] run_dir=results/multi_sharded/20251204-170021-b807e0e73009\n",
            "[FAISS] Using 12 threads\n",
            "[bench] pipeline loaded in 3656.9 ms\n",
            "[bench] warmup passes=1\n",
            "[bench] iter=1/3\n",
            "[bench] iter=2/3\n",
            "[bench] iter=3/3\n",
            "[bench] wrote summary â†’ results/multi_sharded/20251204-170021-b807e0e73009/bench_summary.json\n",
            "[bench] wrote hits jsonl â†’ results/multi_sharded/20251204-170021-b807e0e73009/hits.jsonl\n",
            "{\n",
            "  \"backend\": \"faiss\",\n",
            "  \"index_key\": \"multi_sharded\",\n",
            "  \"config_hash\": \"b807e0e73009\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 128,\n",
            "  \"alpha\": 3,\n",
            "  \"shard_count\": 8,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 3,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 14.55,\n",
            "  \"p50_ms\": 14.42,\n",
            "  \"p90_ms\": 15.01,\n",
            "  \"qps\": 67.53,\n",
            "  \"source_mix\": {\n",
            "    \"pubmed\": 0.2667,\n",
            "    \"stackexchange\": 0.3833,\n",
            "    \"wikipedia\": 0.35\n",
            "  },\n",
            "  \"filter_source\": null,\n",
            "  \"total_elapsed_ms\": 533.1,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_sharded/20251204-170021-b807e0e73009\",\n",
            "  \"hits_written\": true\n",
            "}\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867634.762270   14007 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867634.768686   14007 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867634.784488   14007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867634.784519   14007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867634.784522   14007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867634.784524   14007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[bench] config=configs/index/config.json index=multi_chroma backend=chroma\n",
            "[bench] queries=12 top_k=10 iters=3 warmup=1 filter_source=None\n",
            "[bench] run_dir=results/multi_chroma/20251204-170039-89c498df4b7d\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "Loaded Chroma collection: multi_default | count=79164 | store=indices/chroma_store\n",
            "[bench] pipeline loaded in 3902.0 ms\n",
            "[bench] warmup passes=1\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
            "[bench] iter=1/3\n",
            "[bench] iter=2/3\n",
            "[bench] iter=3/3\n",
            "[bench] wrote summary â†’ results/multi_chroma/20251204-170039-89c498df4b7d/bench_summary.json\n",
            "[bench] wrote hits jsonl â†’ results/multi_chroma/20251204-170039-89c498df4b7d/hits.jsonl\n",
            "{\n",
            "  \"backend\": \"chroma\",\n",
            "  \"index_key\": \"multi_chroma\",\n",
            "  \"config_hash\": \"89c498df4b7d\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 64,\n",
            "  \"alpha\": 3,\n",
            "  \"shard_count\": null,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 3,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 13.18,\n",
            "  \"p50_ms\": 13.07,\n",
            "  \"p90_ms\": 13.52,\n",
            "  \"qps\": 74.56,\n",
            "  \"source_mix\": {\n",
            "    \"pubmed\": 0.2333,\n",
            "    \"stackexchange\": 0.3667,\n",
            "    \"wikipedia\": 0.4\n",
            "  },\n",
            "  \"filter_source\": null,\n",
            "  \"total_elapsed_ms\": 482.8,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_chroma/20251204-170039-89c498df4b7d\",\n",
            "  \"hits_written\": true\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run per-domain slices (to confirm filtering & mix):"
      ],
      "metadata": {
        "id": "SL74qe5c6UBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for src in [\"wikipedia\",\"pubmed\",\"stackexchange\"]:\n",
        "  !python bench.py --config configs/index/config.json --backend faiss --index multi_default \\\n",
        "    --qfile data/queries/all_sources_eval.txt --top_k 10 --iters 3 --warmup 1 \\\n",
        "    --filter_source {src} --results_dir results"
      ],
      "metadata": {
        "id": "h2F_hYwC4gff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1c3f97-7adf-4615-bf2f-5e7ef7d304f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867653.871291   14150 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867653.877919   14150 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867653.894589   14150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867653.894621   14150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867653.894624   14150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867653.894628   14150 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[bench] config=configs/index/config.json index=multi_default backend=faiss\n",
            "[bench] queries=12 top_k=10 iters=3 warmup=1 filter_source=wikipedia\n",
            "[bench] run_dir=results/multi_default/20251204-170059-c96953b290c4\n",
            "[FAISS] Using 12 threads\n",
            "[bench] pipeline loaded in 3558.2 ms\n",
            "[bench] warmup passes=1\n",
            "[bench] iter=1/3\n",
            "[bench] iter=2/3\n",
            "[bench] iter=3/3\n",
            "[bench] wrote summary â†’ results/multi_default/20251204-170059-c96953b290c4/bench_summary.json\n",
            "[bench] wrote hits jsonl â†’ results/multi_default/20251204-170059-c96953b290c4/hits.jsonl\n",
            "{\n",
            "  \"backend\": \"faiss\",\n",
            "  \"index_key\": \"multi_default\",\n",
            "  \"config_hash\": \"c96953b290c4\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 128,\n",
            "  \"alpha\": null,\n",
            "  \"shard_count\": null,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 3,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 12.21,\n",
            "  \"p50_ms\": 12.17,\n",
            "  \"p90_ms\": 12.96,\n",
            "  \"qps\": 80.15,\n",
            "  \"source_mix\": {\n",
            "    \"wikipedia\": 1.0\n",
            "  },\n",
            "  \"filter_source\": \"wikipedia\",\n",
            "  \"total_elapsed_ms\": 449.2,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_default/20251204-170059-c96953b290c4\",\n",
            "  \"hits_written\": true\n",
            "}\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867671.843617   14292 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867671.849958   14292 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867671.865575   14292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867671.865603   14292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867671.865606   14292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867671.865608   14292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[bench] config=configs/index/config.json index=multi_default backend=faiss\n",
            "[bench] queries=12 top_k=10 iters=3 warmup=1 filter_source=pubmed\n",
            "[bench] run_dir=results/multi_default/20251204-170116-c96953b290c4\n",
            "[FAISS] Using 12 threads\n",
            "[bench] pipeline loaded in 3593.6 ms\n",
            "[bench] warmup passes=1\n",
            "[bench] iter=1/3\n",
            "[bench] iter=2/3\n",
            "[bench] iter=3/3\n",
            "[bench] wrote summary â†’ results/multi_default/20251204-170116-c96953b290c4/bench_summary.json\n",
            "[bench] wrote hits jsonl â†’ results/multi_default/20251204-170116-c96953b290c4/hits.jsonl\n",
            "{\n",
            "  \"backend\": \"faiss\",\n",
            "  \"index_key\": \"multi_default\",\n",
            "  \"config_hash\": \"c96953b290c4\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 128,\n",
            "  \"alpha\": null,\n",
            "  \"shard_count\": null,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 3,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 11.97,\n",
            "  \"p50_ms\": 12.03,\n",
            "  \"p90_ms\": 12.37,\n",
            "  \"qps\": 81.83,\n",
            "  \"source_mix\": {\n",
            "    \"pubmed\": 1.0\n",
            "  },\n",
            "  \"filter_source\": \"pubmed\",\n",
            "  \"total_elapsed_ms\": 439.9,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_default/20251204-170116-c96953b290c4\",\n",
            "  \"hits_written\": true\n",
            "}\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764867689.789410   14432 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764867689.795991   14432 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764867689.811829   14432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867689.811858   14432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867689.811861   14432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764867689.811864   14432 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[bench] config=configs/index/config.json index=multi_default backend=faiss\n",
            "[bench] queries=12 top_k=10 iters=3 warmup=1 filter_source=stackexchange\n",
            "[bench] run_dir=results/multi_default/20251204-170134-c96953b290c4\n",
            "[FAISS] Using 12 threads\n",
            "[bench] pipeline loaded in 4026.4 ms\n",
            "[bench] warmup passes=1\n",
            "[bench] iter=1/3\n",
            "[bench] iter=2/3\n",
            "[bench] iter=3/3\n",
            "[bench] wrote summary â†’ results/multi_default/20251204-170134-c96953b290c4/bench_summary.json\n",
            "[bench] wrote hits jsonl â†’ results/multi_default/20251204-170134-c96953b290c4/hits.jsonl\n",
            "{\n",
            "  \"backend\": \"faiss\",\n",
            "  \"index_key\": \"multi_default\",\n",
            "  \"config_hash\": \"c96953b290c4\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 128,\n",
            "  \"alpha\": null,\n",
            "  \"shard_count\": null,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 3,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 12.31,\n",
            "  \"p50_ms\": 12.2,\n",
            "  \"p90_ms\": 13.4,\n",
            "  \"qps\": 79.23,\n",
            "  \"source_mix\": {\n",
            "    \"stackexchange\": 1.0\n",
            "  },\n",
            "  \"filter_source\": \"stackexchange\",\n",
            "  \"total_elapsed_ms\": 454.4,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_default/20251204-170134-c96953b290c4\",\n",
            "  \"hits_written\": true\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity peek at what got saved"
      ],
      "metadata": {
        "id": "X9O10lQk6hLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "base = Path(\"results\")\n",
        "index_key = \"multi_default\"  # change if you want a different index\n",
        "\n",
        "# 1) Prefer timestamped run directories (results/<index>/<YYYYMMDD-HHMMSS>-<hash>/)\n",
        "cand = base / index_key\n",
        "if not cand.exists():\n",
        "    raise SystemExit(f\"No path: {cand}\")\n",
        "\n",
        "# Filter to directories only\n",
        "runs = sorted([p for p in cand.iterdir() if p.is_dir()], reverse=True)\n",
        "\n",
        "if runs:\n",
        "    run_dir = runs[0]\n",
        "    print(\"Inspecting run directory:\", run_dir)\n",
        "\n",
        "    summ_path = run_dir / \"bench_summary.json\"\n",
        "    if summ_path.exists():\n",
        "        summary = json.load(open(summ_path, \"r\", encoding=\"utf-8\"))\n",
        "        print(json.dumps(summary, indent=2)[:2000], \"...\\n\")\n",
        "    else:\n",
        "        print(\"No bench_summary.json in this run dir.\")\n",
        "\n",
        "    # Prefer JSONL, else CSV, if present\n",
        "    hits_jsonl = run_dir / \"hits.jsonl\"\n",
        "    hits_csv   = run_dir / \"hits.csv\"\n",
        "    if hits_jsonl.exists():\n",
        "        display(pd.read_json(hits_jsonl, lines=True).head())\n",
        "    elif hits_csv.exists():\n",
        "        display(pd.read_csv(hits_csv).head())\n",
        "    else:\n",
        "        print(\"No detailed hits file in this run directory.\")\n",
        "\n",
        "else:\n",
        "    # 2) No run directories found â€” maybe you only have a flat CSV/JSONL like all_sources_top10.csv\n",
        "    print(\"No run directories under\", cand, \"â€” looking for flat filesâ€¦\")\n",
        "\n",
        "    flat_jsonl = sorted(cand.glob(\"*.jsonl\"), reverse=True)\n",
        "    flat_csv   = sorted(cand.glob(\"*.csv\"), reverse=True)\n",
        "\n",
        "    if flat_jsonl:\n",
        "        f = flat_jsonl[0]\n",
        "        print(\"Showing flat JSONL file:\", f)\n",
        "        display(pd.read_json(f, lines=True).head())\n",
        "    elif flat_csv:\n",
        "        f = flat_csv[0]\n",
        "        print(\"Showing flat CSV file:\", f)\n",
        "        display(pd.read_csv(f).head())\n",
        "    else:\n",
        "        raise SystemExit(\"Found neither run directories nor flat hits files to inspect.\")"
      ],
      "metadata": {
        "id": "XMzVJkpQYg56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64b537f5-89eb-4a29-ff27-df05e4e97a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting run directory: results/multi_default/20251204-170134-c96953b290c4\n",
            "{\n",
            "  \"backend\": \"faiss\",\n",
            "  \"index_key\": \"multi_default\",\n",
            "  \"config_hash\": \"c96953b290c4\",\n",
            "  \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"score_type\": \"similarity\",\n",
            "  \"efSearch\": 128,\n",
            "  \"alpha\": null,\n",
            "  \"shard_count\": null,\n",
            "  \"n_queries\": 12,\n",
            "  \"iters\": 3,\n",
            "  \"top_k\": 10,\n",
            "  \"avg_ms\": 12.31,\n",
            "  \"p50_ms\": 12.2,\n",
            "  \"p90_ms\": 13.4,\n",
            "  \"qps\": 79.23,\n",
            "  \"source_mix\": {\n",
            "    \"stackexchange\": 1.0\n",
            "  },\n",
            "  \"filter_source\": \"stackexchange\",\n",
            "  \"total_elapsed_ms\": 454.4,\n",
            "  \"quality\": {\n",
            "    \"recall_at_k\": null,\n",
            "    \"mrr\": null\n",
            "  },\n",
            "  \"environment\": {\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"numpy\": \"1.26.4\",\n",
            "    \"faiss\": \"1.13.0\",\n",
            "    \"faiss_threads\": 12,\n",
            "    \"chromadb\": \"0.4.24\",\n",
            "    \"sentence_transformers\": \"5.1.2\",\n",
            "    \"cpu_count\": 12\n",
            "  },\n",
            "  \"run_dir\": \"results/multi_default/20251204-170134-c96953b290c4\",\n",
            "  \"hits_written\": true\n",
            "} ...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                      query  iter  rank     score  \\\n",
              "0  How to fix ModuleNotFoundError in Python     1     1  0.613627   \n",
              "1  How to fix ModuleNotFoundError in Python     1     2  0.607274   \n",
              "2  How to fix ModuleNotFoundError in Python     1     3  0.605724   \n",
              "3  How to fix ModuleNotFoundError in Python     1     4  0.605515   \n",
              "4  How to fix ModuleNotFoundError in Python     1     5  0.600347   \n",
              "\n",
              "                                     id  \\\n",
              "0  d7553950-bccc-47a2-acd7-9b389835edfb   \n",
              "1  295167ea-5826-4ba3-b808-f32d2254cda7   \n",
              "2  c3e108d0-54ec-425f-819e-a3f02b288020   \n",
              "3  66fbaa8b-6915-4ddd-a731-08566066975e   \n",
              "4  c1067325-5e06-416a-b590-33b94c647bc6   \n",
              "\n",
              "                                        uid         source  \\\n",
              "0  d7553950-bccc-47a2-acd7-9b389835edfb::c8  stackexchange   \n",
              "1  295167ea-5826-4ba3-b808-f32d2254cda7::c0  stackexchange   \n",
              "2  c3e108d0-54ec-425f-819e-a3f02b288020::c1  stackexchange   \n",
              "3  66fbaa8b-6915-4ddd-a731-08566066975e::c2  stackexchange   \n",
              "4  c1067325-5e06-416a-b590-33b94c647bc6::c0  stackexchange   \n",
              "\n",
              "                                               title  \\\n",
              "0  Xwayland gradually consumes all RAM when runni...   \n",
              "1         FastAPI won&#39;t recognize module changes   \n",
              "2  Error while deploying, but not in local: &quot...   \n",
              "3                  No module named &#39;wx._msw&#39;   \n",
              "4  adding hidden imports in .spec file doesnt see...   \n",
              "\n",
              "                                                 url  chunk_id text  \n",
              "0  https://stackoverflow.com/questions/79750814/x...         8       \n",
              "1  https://stackoverflow.com/questions/79772118/f...         0       \n",
              "2  https://stackoverflow.com/questions/79782221/e...         1       \n",
              "3  https://stackoverflow.com/questions/79784586/n...         2       \n",
              "4  https://stackoverflow.com/questions/79757171/a...         0       "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f7d8734-9d41-4e84-ac70-361987be1681\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>iter</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>id</th>\n",
              "      <th>uid</th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How to fix ModuleNotFoundError in Python</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.613627</td>\n",
              "      <td>d7553950-bccc-47a2-acd7-9b389835edfb</td>\n",
              "      <td>d7553950-bccc-47a2-acd7-9b389835edfb::c8</td>\n",
              "      <td>stackexchange</td>\n",
              "      <td>Xwayland gradually consumes all RAM when runni...</td>\n",
              "      <td>https://stackoverflow.com/questions/79750814/x...</td>\n",
              "      <td>8</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How to fix ModuleNotFoundError in Python</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.607274</td>\n",
              "      <td>295167ea-5826-4ba3-b808-f32d2254cda7</td>\n",
              "      <td>295167ea-5826-4ba3-b808-f32d2254cda7::c0</td>\n",
              "      <td>stackexchange</td>\n",
              "      <td>FastAPI won&amp;#39;t recognize module changes</td>\n",
              "      <td>https://stackoverflow.com/questions/79772118/f...</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How to fix ModuleNotFoundError in Python</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.605724</td>\n",
              "      <td>c3e108d0-54ec-425f-819e-a3f02b288020</td>\n",
              "      <td>c3e108d0-54ec-425f-819e-a3f02b288020::c1</td>\n",
              "      <td>stackexchange</td>\n",
              "      <td>Error while deploying, but not in local: &amp;quot...</td>\n",
              "      <td>https://stackoverflow.com/questions/79782221/e...</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How to fix ModuleNotFoundError in Python</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.605515</td>\n",
              "      <td>66fbaa8b-6915-4ddd-a731-08566066975e</td>\n",
              "      <td>66fbaa8b-6915-4ddd-a731-08566066975e::c2</td>\n",
              "      <td>stackexchange</td>\n",
              "      <td>No module named &amp;#39;wx._msw&amp;#39;</td>\n",
              "      <td>https://stackoverflow.com/questions/79784586/n...</td>\n",
              "      <td>2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How to fix ModuleNotFoundError in Python</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.600347</td>\n",
              "      <td>c1067325-5e06-416a-b590-33b94c647bc6</td>\n",
              "      <td>c1067325-5e06-416a-b590-33b94c647bc6::c0</td>\n",
              "      <td>stackexchange</td>\n",
              "      <td>adding hidden imports in .spec file doesnt see...</td>\n",
              "      <td>https://stackoverflow.com/questions/79757171/a...</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f7d8734-9d41-4e84-ac70-361987be1681')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f7d8734-9d41-4e84-ac70-361987be1681 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f7d8734-9d41-4e84-ac70-361987be1681');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-902faf3f-4f2c-43cb-8e39-8727ad350926\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-902faf3f-4f2c-43cb-8e39-8727ad350926')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-902faf3f-4f2c-43cb-8e39-8727ad350926 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        raise SystemExit(\\\"Found neither run directories nor flat hits files to inspect\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How to fix ModuleNotFoundError in Python\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rank\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0047651881021183095,\n        \"min\": 0.600347340106964,\n        \"max\": 0.613627433776855,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.607274055480957\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"295167ea-5826-4ba3-b808-f32d2254cda7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"295167ea-5826-4ba3-b808-f32d2254cda7::c0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"stackexchange\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"FastAPI won&#39;t recognize module changes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://stackoverflow.com/questions/79772118/fastapi-wont-recognize-module-changes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3.6: Spark fan-out skeleton**"
      ],
      "metadata": {
        "id": "kU3LmEzhanjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"    # 0=all, 1=filter INFO, 2=filter WARNING, 3=filter ERROR\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda\"\n",
        "# Also keep Spark quiet:\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--conf spark.ui.showConsoleProgress=false pyspark-shell\""
      ],
      "metadata": {
        "id": "EGvhoqgedGPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colab Spark + Java setup ---\n",
        "!apt -qq install openjdk-17-jdk-headless -y  # Java 17 (good for Spark 3.5+)\n",
        "import os\n",
        "\n",
        "# set env vars\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.12/dist-packages/pyspark\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin:{os.environ['JAVA_HOME']}/bin\"\n",
        "\n",
        "# optional silencing\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# verify\n",
        "!java -version\n",
        "!echo $JAVA_HOME"
      ],
      "metadata": {
        "id": "yHrn9FBOecv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5694b7-9b11-4d58-c791-ec1ba40b461e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-17-jdk-headless is already the newest version (17.0.16+8~us1-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "openjdk version \"17.0.16\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 17.0.16+8-Ubuntu-0ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.16+8-Ubuntu-0ubuntu122.04.1, mixed mode, sharing)\n",
            "/usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/spark_fanout.py\n",
        "import os, json, time, argparse, math\n",
        "from typing import List, Tuple, Dict, Any, Iterator\n",
        "\n",
        "# Quieter HuggingFace tokenizers in forked workers\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "try:\n",
        "    import faiss  # faiss-cpu\n",
        "except Exception:\n",
        "    import faiss_cpu as faiss  # fallback name if env aliases it\n",
        "\n",
        "\n",
        "def _set_efsearch(index, ef: int) -> None:\n",
        "    try:\n",
        "        ps = faiss.ParameterSpace()\n",
        "        ps.set_index_parameter(index, \"efSearch\", int(ef))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _encode_queries(qfile: str, model_name: str, normalize: bool) -> Tuple[List[str], np.ndarray]:\n",
        "    queries: List[str] = []\n",
        "    with open(qfile, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            ln = ln.strip()\n",
        "            if ln:\n",
        "                queries.append(ln)\n",
        "    if not queries:\n",
        "        raise SystemExit(f\"No queries found in {qfile}\")\n",
        "\n",
        "    model = SentenceTransformer(model_name)\n",
        "    qvecs = model.encode(queries, normalize_embeddings=normalize)\n",
        "    qvecs = np.asarray(qvecs, dtype=\"float32\")\n",
        "    return queries, qvecs\n",
        "\n",
        "\n",
        "def _row_to_rd(row: pd.Series) -> Dict[str, Any]:\n",
        "    src_raw = row.get(\"domain\", \"\") or row.get(\"source\", \"\")\n",
        "    src = str(src_raw).strip().lower()\n",
        "    cid = row.get(\"chunk_id\", None)\n",
        "    try:\n",
        "        cid = int(cid) if cid is not None and cid == cid else None\n",
        "    except Exception:\n",
        "        cid = None\n",
        "    uid = f\"{row.get('id','')}::c{cid if cid is not None else -1}\"\n",
        "    text = (row.get(\"chunk_text\", \"\") or row.get(\"text\", \"\") or row.get(\"preview\", \"\") or \"\")\n",
        "    return {\n",
        "        \"id\": str(row.get(\"id\", \"\")),\n",
        "        \"uid\": uid,\n",
        "        \"source\": src,\n",
        "        \"title\": str(row.get(\"title\", \"\")),\n",
        "        \"url\": str(row.get(\"url\", \"\")),\n",
        "        \"text\": str(text),\n",
        "        \"chunk_id\": cid,\n",
        "    }\n",
        "\n",
        "\n",
        "def _search_shard_for_batch(\n",
        "    ipath: str,\n",
        "    mpath: str,\n",
        "    qvecs_b: np.ndarray,\n",
        "    top_k_prime: int,\n",
        "    efsearch: int,\n",
        "    filter_source: str | None,\n",
        ") -> List[Tuple[int, float, Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Search one shard for all query vectors in the broadcast batch.\n",
        "    Returns flat (qidx, score, rowdict) tuples.\n",
        "    \"\"\"\n",
        "    index = faiss.read_index(ipath)\n",
        "    _set_efsearch(index, efsearch)\n",
        "    meta = pd.read_parquet(mpath).reset_index(drop=True)\n",
        "\n",
        "    out: List[Tuple[int, float, Dict[str, Any]]] = []\n",
        "\n",
        "    # Vectorized search per shard (one FAISS call per qvec)\n",
        "    for qi in range(qvecs_b.shape[0]):\n",
        "        qv = qvecs_b[qi:qi+1]\n",
        "        D, I = index.search(qv, top_k_prime)\n",
        "        for score, li in zip(D[0], I[0]):\n",
        "            if li < 0:\n",
        "                continue\n",
        "            row = meta.iloc[int(li)]\n",
        "            src_raw  = row.get(\"domain\", \"\") or row.get(\"source\", \"\") or \"\"\n",
        "            src_norm = str(src_raw).strip().lower()\n",
        "            if filter_source and src_norm != filter_source.strip().lower():\n",
        "                continue\n",
        "            out.append((qi, float(score), _row_to_rd(row)))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _map_partitions_search(\n",
        "    it: Iterator[Tuple[str, str]],\n",
        "    qvecs_b: np.ndarray,\n",
        "    top_k_prime: int,\n",
        "    efsearch: int,\n",
        "    filter_source: str | None,\n",
        ") -> Iterator[List[Tuple[int, float, Dict[str, Any]]]]:\n",
        "    \"\"\"\n",
        "    mapPartitions: each partition receives one or more (index_path, meta_path)\n",
        "    tuples; for each shard in the partition, load index ONCE and search all queries.\n",
        "    \"\"\"\n",
        "    results_all: List[Tuple[int, float, Dict[str, Any]]] = []\n",
        "    for ipath, mpath in it:\n",
        "        part_out = _search_shard_for_batch(\n",
        "            ipath=ipath,\n",
        "            mpath=mpath,\n",
        "            qvecs_b=qvecs_b,\n",
        "            top_k_prime=top_k_prime,\n",
        "            efsearch=efsearch,\n",
        "            filter_source=filter_source,\n",
        "        )\n",
        "        results_all.extend(part_out)\n",
        "    yield results_all\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--manifest\", required=True,\n",
        "                    help=\"Path to indices/multi_default_sharded/manifest.json\")\n",
        "    ap.add_argument(\"--qfile\", required=True, help=\"Newline-delimited queries file\")\n",
        "    ap.add_argument(\"--model\", default=\"BAAI/bge-small-en-v1.5\")\n",
        "    ap.add_argument(\"--normalize_query\", type=int, default=1)\n",
        "    ap.add_argument(\"--top_k\", type=int, default=10)\n",
        "    ap.add_argument(\"--alpha\", type=int, default=3, help=\"overfetch factor per shard\")\n",
        "    ap.add_argument(\"--efsearch\", type=int, default=128)\n",
        "    ap.add_argument(\"--filter_source\", default=None, help=\"wikipedia|pubmed|stackexchange\")\n",
        "    ap.add_argument(\"--results_csv\", default=\"results/spark_fanout_hits.csv\")\n",
        "    ap.add_argument(\"--log\", default=\"logs/spark_fanout_test.log\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.results_csv), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(args.log), exist_ok=True)\n",
        "\n",
        "    # ---- Load manifest\n",
        "    man = json.load(open(args.manifest))\n",
        "    # Support both \"index_paths/meta_paths\" and richer shard list\n",
        "    if \"shards\" in man and isinstance(man[\"shards\"], list):\n",
        "        index_paths = [s[\"index_path\"] for s in man[\"shards\"]]\n",
        "        meta_paths  = [s[\"meta_path\"]  for s in man[\"shards\"]]\n",
        "        metric      = (man.get(\"metric\") or man.get(\"index_metric\") or \"ip\").lower()\n",
        "        normalized  = bool(man.get(\"normalized\", True))\n",
        "    else:\n",
        "        index_paths = man.get(\"index_paths\") or []\n",
        "        meta_paths  = man.get(\"meta_paths\") or []\n",
        "        metric      = (man.get(\"metric\") or man.get(\"index_metric\") or \"ip\").lower()\n",
        "        normalized  = bool(man.get(\"normalized\", True))\n",
        "\n",
        "    if not index_paths or not meta_paths or len(index_paths) != len(meta_paths):\n",
        "        raise SystemExit(\"Manifest missing or paths length mismatch.\")\n",
        "\n",
        "    # ---- Metric parity & normalization checks (fail-fast)\n",
        "    if metric not in {\"ip\", \"cosine\"}:\n",
        "        # If shards were built as L2, we can still work but ranking semantics change (lower=better).\n",
        "        # For fan-out we assume IP/cosine (higher=better). Enforce now to keep heap logic consistent.\n",
        "        raise SystemExit(\n",
        "            f\"Manifest reports metric='{metric}'. Expected 'ip' or 'cosine'. \"\n",
        "            f\"Rebuild shards with IP on normalized vectors (see Step 3.3).\"\n",
        "        )\n",
        "    if not normalized:\n",
        "        # IPâ‰ˆcosine requires normalized corpus + (usually) normalized queries\n",
        "        raise SystemExit(\n",
        "            \"Manifest indicates 'normalized=false'. Rebuild corpus vectors as unit-norm for IP/cosine parity.\"\n",
        "        )\n",
        "\n",
        "    shards = list(zip(index_paths, meta_paths))\n",
        "\n",
        "    # ---- Encode on driver (broadcast vectors, not the model)\n",
        "    t0 = time.perf_counter()\n",
        "    queries, qvecs = _encode_queries(args.qfile, args.model, bool(args.normalize_query))\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    # ---- Spark session (local; disable Arrowâ€”FAISS is native)\n",
        "    spark = (\n",
        "        SparkSession.builder\n",
        "        .appName(\"spark_fanout_local\")\n",
        "        .master(\"local[*]\")\n",
        "        .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
        "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    # ---- Broadcast query vectors and log broadcast size\n",
        "    qvecs_b = sc.broadcast(qvecs)\n",
        "    est_bcast_bytes = qvecs.nbytes\n",
        "    # Simple size guard: if > 50MB suggest batch-splitting\n",
        "    bcast_warn = (est_bcast_bytes > 50 * 1024 * 1024)\n",
        "\n",
        "    per_shard = max(args.top_k * args.alpha, args.top_k)\n",
        "\n",
        "    # ---- Map each shard â†’ top-k' per query using mapPartitions (load index once per shard)\n",
        "    t2 = time.perf_counter()\n",
        "    # One shard per partition for clean locality\n",
        "    rdd = sc.parallelize(shards, len(shards))\n",
        "    shard_results_nested = rdd.mapPartitions(\n",
        "        lambda it: _map_partitions_search(\n",
        "            it=it,\n",
        "            qvecs_b=qvecs_b.value,\n",
        "            top_k_prime=per_shard,\n",
        "            efsearch=args.efsearch,\n",
        "            filter_source=args.filter_source,\n",
        "        )\n",
        "    ).collect()\n",
        "    t3 = time.perf_counter()\n",
        "\n",
        "    # ---- Merge to global top-k per query, deterministic tie-break (score desc, uid asc)\n",
        "    flat: List[Tuple[int, float, Dict[str, Any]]] = []\n",
        "    for part in shard_results_nested:\n",
        "        flat.extend(part)\n",
        "\n",
        "    # Dedup by uid with best score kept\n",
        "    by_q: Dict[int, Dict[str, Tuple[float, Dict[str, Any]]]] = {}\n",
        "    for qi, score, rd in flat:\n",
        "        bucket = by_q.setdefault(qi, {})\n",
        "        uid = rd[\"uid\"]\n",
        "        if (uid not in bucket) or (score > bucket[uid][0]):\n",
        "            bucket[uid] = (score, rd)\n",
        "\n",
        "    rows = []\n",
        "    for qi, uid_map in by_q.items():\n",
        "        # stable key: (score desc, uid asc)\n",
        "        candidates = [(score, rd[\"uid\"], rd) for uid, (score, rd) in uid_map.items()]\n",
        "        candidates.sort(key=lambda x: (-x[0], x[1]))\n",
        "        top = candidates[: args.top_k]\n",
        "        for rank, (score, _uid, rd) in enumerate(top, start=1):\n",
        "            rows.append({\n",
        "                \"query\": queries[qi],\n",
        "                \"rank\": rank,\n",
        "                \"score\": float(score),\n",
        "                \"id\": rd[\"id\"],\n",
        "                \"uid\": rd[\"uid\"],\n",
        "                \"source\": rd[\"source\"],\n",
        "                \"title\": rd[\"title\"],\n",
        "                \"url\": rd[\"url\"],\n",
        "                \"chunk_id\": rd[\"chunk_id\"],\n",
        "                \"text\": rd[\"text\"],\n",
        "            })\n",
        "\n",
        "    rows.sort(key=lambda r: (r[\"query\"], r[\"rank\"]))\n",
        "    df = pd.DataFrame(rows)\n",
        "    os.makedirs(os.path.dirname(args.results_csv), exist_ok=True)\n",
        "    df.to_csv(args.results_csv, index=False)\n",
        "\n",
        "    # ---- timings & log\n",
        "    t4 = time.perf_counter()\n",
        "    timings = {\n",
        "        \"n_shards\": len(shards),\n",
        "        \"n_queries\": len(queries),\n",
        "        \"top_k\": args.top_k,\n",
        "        \"alpha\": args.alpha,\n",
        "        \"efsearch\": args.efsearch,\n",
        "        \"encode_ms\": round((t1 - t0) * 1000, 1),\n",
        "        \"spark_map_ms\": round((t3 - t2) * 1000, 1),\n",
        "        \"merge_ms\": round((t4 - t3) * 1000, 1),\n",
        "        \"total_ms\": round((t4 - t0) * 1000, 1),\n",
        "        \"filter_source\": args.filter_source or None,\n",
        "        \"results_csv\": args.results_csv,\n",
        "        \"metric\": metric,\n",
        "        \"normalized\": bool(args.normalize_query),\n",
        "        \"broadcast_bytes\": est_bcast_bytes,\n",
        "        \"broadcast_large_hint\": bcast_warn,\n",
        "    }\n",
        "    os.makedirs(os.path.dirname(args.log), exist_ok=True)\n",
        "    with open(args.log, \"w\", encoding=\"utf-8\") as fh:\n",
        "        fh.write(json.dumps(timings, indent=2) + \"\\n\")\n",
        "\n",
        "    print(\"=== Spark fan-out summary ===\")\n",
        "    print(json.dumps(timings, indent=2))\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Gfm1-sWfbm_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214d1f01-c9be-42fd-a2c0-66e213184d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/spark_fanout.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-rebuild it from your shard folders"
      ],
      "metadata": {
        "id": "R_-V0pchdjSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”§ Rebuild manifest from existing shard files (run in project root)\n",
        "import os, glob, json\n",
        "\n",
        "ROOT = os.getcwd()  # should be \"/content/drive/MyDrive/AMS 560 PROJECT\"\n",
        "idx_glob  = os.path.join(ROOT, \"indices/multi_default_sharded/shard_*.faiss\")\n",
        "meta_glob = os.path.join(ROOT, \"data/embeddings/multi_default_sharded/meta_shard_*.parquet\")\n",
        "\n",
        "index_paths = sorted(glob.glob(idx_glob))\n",
        "meta_paths  = sorted(glob.glob(meta_glob))\n",
        "\n",
        "print(\"#index_paths:\", len(index_paths))\n",
        "print(\"#meta_paths :\", len(meta_paths))\n",
        "print(\"Sample index:\", index_paths[:2])\n",
        "print(\"Sample meta :\", meta_paths[:2])\n",
        "\n",
        "if not index_paths or not meta_paths or len(index_paths) != len(meta_paths):\n",
        "    raise SystemExit(\"Shard discovery failed â€” check that both shard .faiss and meta_shard_*.parquet exist.\")\n",
        "\n",
        "# If you know your shards were built with IP on normalized vectors (Step 3.3 fix):\n",
        "metric = \"ip\"\n",
        "normalized = True\n",
        "M = 32\n",
        "efConstruction = 200\n",
        "\n",
        "man = {\n",
        "    \"index_paths\": index_paths,\n",
        "    \"meta_paths\": meta_paths,\n",
        "    \"metric\": metric,\n",
        "    \"normalized\": normalized,\n",
        "    \"M\": M,\n",
        "    \"efConstruction\": efConstruction,\n",
        "    # (optional richer format that scripts/spark_fanout.py can also read)\n",
        "    \"shards\": [\n",
        "        {\n",
        "            \"id\": f\"{i:03d}\",\n",
        "            \"index_path\": ip,\n",
        "            \"meta_path\": mp,\n",
        "            \"n\": None  # can be filled later if you want\n",
        "        }\n",
        "        for i, (ip, mp) in enumerate(zip(index_paths, meta_paths))\n",
        "    ]\n",
        "}\n",
        "man_path = os.path.join(ROOT, \"indices/multi_default_sharded/manifest.json\")\n",
        "os.makedirs(os.path.dirname(man_path), exist_ok=True)\n",
        "with open(man_path, \"w\") as f:\n",
        "    json.dump(man, f, indent=2)\n",
        "print(\"âœ… Wrote manifest:\", man_path)"
      ],
      "metadata": {
        "id": "lzEcsG-9doAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663aa54c-fa7b-4e88-c311-49fbd07a3e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#index_paths: 8\n",
            "#meta_paths : 8\n",
            "Sample index: ['/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded/shard_000.faiss', '/content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded/shard_001.faiss']\n",
            "Sample meta : ['/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default_sharded/meta_shard_000.parquet', '/content/drive/MyDrive/AMS 560 PROJECT/data/embeddings/multi_default_sharded/meta_shard_001.parquet']\n",
            "âœ… Wrote manifest: /content/drive/MyDrive/AMS 560 PROJECT/indices/multi_default_sharded/manifest.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the script"
      ],
      "metadata": {
        "id": "OnmxKGB0cVjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MANIFEST = \"indices/multi_default_sharded/manifest.json\"   # or absolute path\n",
        "QFILE    = \"data/queries/all_sources_eval.txt\"\n",
        "\n",
        "!python scripts/spark_fanout.py \\\n",
        "  --manifest \"$MANIFEST\" \\\n",
        "  --qfile \"$QFILE\" \\\n",
        "  --top_k 10 \\\n",
        "  --alpha 1 \\\n",
        "  --efsearch 128 \\\n",
        "  --normalize_query 1 \\\n",
        "  --results_csv \"results/spark_fanout_hits.csv\" \\\n",
        "  --log \"logs/spark_fanout_test.log\""
      ],
      "metadata": {
        "id": "YiQvfUhObtmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf5618a-28b1-400a-b1f2-07822bab546f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-04 17:28:20.562872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764869300.584720   21472 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764869300.591311   21472 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764869300.607723   21472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764869300.607749   21472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764869300.607752   21472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764869300.607754   21472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/12/04 17:28:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "=== Spark fan-out summary ===\n",
            "{\n",
            "  \"n_shards\": 8,\n",
            "  \"n_queries\": 12,\n",
            "  \"top_k\": 10,\n",
            "  \"alpha\": 1,\n",
            "  \"efsearch\": 128,\n",
            "  \"encode_ms\": 3628.3,\n",
            "  \"spark_map_ms\": 1991.7,\n",
            "  \"merge_ms\": 13.1,\n",
            "  \"total_ms\": 8564.4,\n",
            "  \"filter_source\": null,\n",
            "  \"results_csv\": \"results/spark_fanout_hits.csv\",\n",
            "  \"metric\": \"ip\",\n",
            "  \"normalized\": true,\n",
            "  \"broadcast_bytes\": 18432,\n",
            "  \"broadcast_large_hint\": false\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify Spark top-k equals your local sharded runner's top-k**"
      ],
      "metadata": {
        "id": "ViXq-9zMg8_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glob.glob(\"results/*.csv\")\n",
        "['results/spark_fanout_hits.csv']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-94zvEkAZnqm",
        "outputId": "b7e6bad0-da0e-4b66-b569-cba1c7ab65f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['results/spark_fanout_hits.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/AMS 560 PROJECT\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wy4a9QsaOwo",
        "outputId": "0c97a069-c288-4750-c754-05f1633031c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AMS 560 PROJECT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "CFG = \"configs/index/config.json\"\n",
        "\n",
        "with open(CFG, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "print(\"Available indices keys:\")\n",
        "print(list(cfg.get(\"indices\", {}).keys()))\n",
        "\n",
        "print(\"\\nactive_index in config:\")\n",
        "print(cfg.get(\"active_index\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyDUKDKZaj5J",
        "outputId": "0638174b-5b94-4913-a4f4-fd4515bdf088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available indices keys:\n",
            "['multi_default', 'multi_sharded', 'multi_chroma']\n",
            "\n",
            "active_index in config:\n",
            "multi_chroma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pipeline.runner import load_pipeline\n",
        "\n",
        "CFG   = \"configs/index/config.json\"\n",
        "QFILE = \"data/queries/all_sources_eval.txt\"\n",
        "\n",
        "# Load the FAISS sharded index locally (same one you used to build shards)\n",
        "P = load_pipeline(CFG, index_key=\"multi_sharded\")\n",
        "\n",
        "# Read queries\n",
        "with open(QFILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "rows = []\n",
        "\n",
        "for q in queries:\n",
        "    hits = P.search_one(q, top_k=10)   # list of dicts\n",
        "    for rank, h in enumerate(hits, start=1):\n",
        "        # h is a dict, so use h[...] / h.get(...)\n",
        "        chunk_id = h.get(\"chunk_id\", None)\n",
        "        uid = h.get(\"uid\", None)\n",
        "\n",
        "        if uid is None:\n",
        "            # robust cid handling\n",
        "            if chunk_id is None or (isinstance(chunk_id, float) and pd.isna(chunk_id)):\n",
        "                cid = -1\n",
        "            else:\n",
        "                cid = int(chunk_id)\n",
        "            uid = f\"{h['id']}::c{cid}\"\n",
        "\n",
        "        rows.append({\n",
        "            \"query\": q,\n",
        "            \"rank\": rank,\n",
        "            \"score\": float(h.get(\"score\", 0.0)),\n",
        "            \"id\": h.get(\"id\", \"\"),\n",
        "            \"uid\": uid,\n",
        "            \"source\": h.get(\"source\"),\n",
        "            \"title\": h.get(\"title\"),\n",
        "            \"url\": h.get(\"url\"),\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"iter\": 1,   # single baseline run\n",
        "        })\n",
        "\n",
        "df_local = pd.DataFrame(rows)\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "out_path = \"results/local_multi_default_sharded_runs.csv\"\n",
        "df_local.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"Wrote local baseline to:\", out_path)\n",
        "print(\"Shape:\", df_local.shape)\n",
        "df_local.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "PFNno7E0aQlx",
        "outputId": "dbdbf18a-7eb9-4506-be5e-80b8836cffc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAISS] Using 12 threads\n",
            "Wrote local baseline to: results/local_multi_default_sharded_runs.csv\n",
            "Shape: (120, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           query  rank     score  \\\n",
              "0  metformin mechanism of action     1  0.736435   \n",
              "1  metformin mechanism of action     2  0.734521   \n",
              "2  metformin mechanism of action     3  0.728445   \n",
              "3  metformin mechanism of action     4  0.724863   \n",
              "4  metformin mechanism of action     5  0.716767   \n",
              "\n",
              "                                     id  \\\n",
              "0  879c2390-c660-4fe3-8a31-f959c9c013c4   \n",
              "1  ac6d737e-edaa-4421-929c-969f67d09e52   \n",
              "2  0e2a1ab5-3c8f-4ffb-80f2-7f3892ea7e5e   \n",
              "3  14d305f3-b590-4ed4-9bba-f14800f2c40f   \n",
              "4  a2fb6d19-85af-4153-be89-e56bb9be99c1   \n",
              "\n",
              "                                        uid  source  \\\n",
              "0  879c2390-c660-4fe3-8a31-f959c9c013c4::c0  pubmed   \n",
              "1  ac6d737e-edaa-4421-929c-969f67d09e52::c0  pubmed   \n",
              "2  0e2a1ab5-3c8f-4ffb-80f2-7f3892ea7e5e::c0  pubmed   \n",
              "3  14d305f3-b590-4ed4-9bba-f14800f2c40f::c0  pubmed   \n",
              "4  a2fb6d19-85af-4153-be89-e56bb9be99c1::c0  pubmed   \n",
              "\n",
              "                                               title  \\\n",
              "0  The beneficial effects of empagliflozin-metfor...   \n",
              "1  Randomized controlled trial of effects of metf...   \n",
              "2  Computational Modelling the Impact of GLP-1 Re...   \n",
              "3  Metformin induces ferroptosis associated with ...   \n",
              "4  Curcumin ameliorates high-glucose-induced lipi...   \n",
              "\n",
              "                                         url  chunk_id  iter  \n",
              "0  https://pubmed.ncbi.nlm.nih.gov/41175191/         0     1  \n",
              "1  https://pubmed.ncbi.nlm.nih.gov/41174662/         0     1  \n",
              "2  https://pubmed.ncbi.nlm.nih.gov/41173332/         0     1  \n",
              "3  https://pubmed.ncbi.nlm.nih.gov/41172231/         0     1  \n",
              "4  https://pubmed.ncbi.nlm.nih.gov/41172806/         0     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c921d818-d91f-417f-bc36-b2eda9907fe1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>id</th>\n",
              "      <th>uid</th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>iter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>metformin mechanism of action</td>\n",
              "      <td>1</td>\n",
              "      <td>0.736435</td>\n",
              "      <td>879c2390-c660-4fe3-8a31-f959c9c013c4</td>\n",
              "      <td>879c2390-c660-4fe3-8a31-f959c9c013c4::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>The beneficial effects of empagliflozin-metfor...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41175191/</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>metformin mechanism of action</td>\n",
              "      <td>2</td>\n",
              "      <td>0.734521</td>\n",
              "      <td>ac6d737e-edaa-4421-929c-969f67d09e52</td>\n",
              "      <td>ac6d737e-edaa-4421-929c-969f67d09e52::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>Randomized controlled trial of effects of metf...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41174662/</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>metformin mechanism of action</td>\n",
              "      <td>3</td>\n",
              "      <td>0.728445</td>\n",
              "      <td>0e2a1ab5-3c8f-4ffb-80f2-7f3892ea7e5e</td>\n",
              "      <td>0e2a1ab5-3c8f-4ffb-80f2-7f3892ea7e5e::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>Computational Modelling the Impact of GLP-1 Re...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41173332/</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>metformin mechanism of action</td>\n",
              "      <td>4</td>\n",
              "      <td>0.724863</td>\n",
              "      <td>14d305f3-b590-4ed4-9bba-f14800f2c40f</td>\n",
              "      <td>14d305f3-b590-4ed4-9bba-f14800f2c40f::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>Metformin induces ferroptosis associated with ...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41172231/</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>metformin mechanism of action</td>\n",
              "      <td>5</td>\n",
              "      <td>0.716767</td>\n",
              "      <td>a2fb6d19-85af-4153-be89-e56bb9be99c1</td>\n",
              "      <td>a2fb6d19-85af-4153-be89-e56bb9be99c1::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>Curcumin ameliorates high-glucose-induced lipi...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41172806/</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c921d818-d91f-417f-bc36-b2eda9907fe1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c921d818-d91f-417f-bc36-b2eda9907fe1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c921d818-d91f-417f-bc36-b2eda9907fe1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d23f793c-06bf-484d-b210-e0d4f48d20d2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d23f793c-06bf-484d-b210-e0d4f48d20d2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d23f793c-06bf-484d-b210-e0d4f48d20d2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_local",
              "summary": "{\n  \"name\": \"df_local\",\n  \"rows\": 120,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"history of backpropagation\",\n          \"definition of entropy in information theory\",\n          \"metformin mechanism of action\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rank\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04073954614893376,\n        \"min\": 0.6656453013420105,\n        \"max\": 0.8709640502929688,\n        \"num_unique_values\": 120,\n        \"samples\": [\n          0.7032290697097778,\n          0.6971383690834045,\n          0.7167673110961914\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 88,\n        \"samples\": [\n          \"9622c9aa-7787-4c94-b014-55ddbdb35d9d\",\n          \"879c2390-c660-4fe3-8a31-f959c9c013c4\",\n          \"6e738fde-2c11-4eda-ba76-284f847dc516\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 117,\n        \"samples\": [\n          \"66fbaa8b-6915-4ddd-a731-08566066975e::c2\",\n          \"a2fb6d19-85af-4153-be89-e56bb9be99c1::c0\",\n          \"40169d95-afd0-4f32-bc69-160b6542efb6::c10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"pubmed\",\n          \"wikipedia\",\n          \"stackexchange\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 88,\n        \"samples\": [\n          \"Bit\",\n          \"The beneficial effects of empagliflozin-metformin combination on cardiovascular risk factors in type 2 diabetes mellitus patients.\",\n          \"Comprehensive impact of Intermittent Hypoxia Training and Intermittent Fasting on metabolic and cognitive health in adults with obesity: an umbrella systematic review and meta-analysis.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 88,\n        \"samples\": [\n          \"https://en.wikipedia.org/wiki/Bit\",\n          \"https://pubmed.ncbi.nlm.nih.gov/41175191/\",\n          \"https://pubmed.ncbi.nlm.nih.gov/41170361/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 0,\n        \"max\": 45,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          45,\n          5,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_spark = pd.read_csv(\"results/spark_fanout_hits.csv\")\n",
        "df_local = pd.read_csv(\"results/local_multi_default_sharded_runs.csv\")\n",
        "\n",
        "print(\"Spark fanout shape:\", df_spark.shape)\n",
        "print(\"Local baseline  shape:\", df_local.shape)\n",
        "\n",
        "df_local_run = df_local[df_local[\"iter\"] == 1].copy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiZwNeAbZjYZ",
        "outputId": "744ed677-fedb-4ab7-dc57-7d3009e7fa4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark fanout shape: (120, 10)\n",
            "Local baseline  shape: (120, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mk_uid(df: pd.DataFrame) -> pd.Series:\n",
        "    if \"uid\" in df.columns and df[\"uid\"].notna().all():\n",
        "        return df[\"uid\"].astype(str)\n",
        "    cid = df.get(\"chunk_id\")\n",
        "    return df[\"id\"].astype(str) + \"::c\" + (\n",
        "        cid.fillna(-1).astype(int).astype(str) if cid is not None else \"-1\"\n",
        "    )\n",
        "\n",
        "df_spark = df_spark.assign(uid=mk_uid(df_spark))\n",
        "df_local_run = df_local_run.assign(uid=mk_uid(df_local_run))"
      ],
      "metadata": {
        "id": "O14OdOs4j_yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cmp_sets(query: str, k: int = 10):\n",
        "    a = df_spark[df_spark[\"query\"] == query].sort_values(\"rank\").head(k)[\"uid\"]\n",
        "    b = df_local_run[df_local_run[\"query\"] == query].sort_values(\"rank\").head(k)[\"uid\"]\n",
        "    aset, bset = set(a), set(b)\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"topk_equal_uid_set\": (aset == bset),\n",
        "        \"spark_only\": sorted(list(aset - bset))[:5],\n",
        "        \"local_only\": sorted(list(bset - aset))[:5],\n",
        "        \"k_compared\": min(len(a), len(b)),\n",
        "        \"spark_dupes\": a.duplicated().sum(),\n",
        "        \"local_dupes\": b.duplicated().sum(),\n",
        "    }\n",
        "\n",
        "queries = sorted(set(df_spark[\"query\"]) & set(df_local_run[\"query\"]))\n",
        "reports = [cmp_sets(q, k=10) for q in queries]\n",
        "mismatch = [r for r in reports if not r[\"topk_equal_uid_set\"]]\n",
        "\n",
        "print(f\"Queries compared: {len(reports)}\")\n",
        "print(f\"Exact top-k UID set match: {len(reports)-len(mismatch)}/{len(reports)}\")\n",
        "\n",
        "if mismatch:\n",
        "    import pprint\n",
        "    pprint.pp(mismatch[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtXU_hIzbCUi",
        "outputId": "8d8f25cc-6005-4dc9-b6b6-b66c5b4ac24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries compared: 12\n",
            "Exact top-k UID set match: 12/12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pandas as pd, pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "\n",
        "MAN=\"indices/multi_default_sharded/manifest.json\"\n",
        "m=json.load(open(MAN))\n",
        "ipaths=m[\"index_paths\"]; mpaths=m[\"meta_paths\"]\n",
        "print(\"Shards:\", len(ipaths), len(mpaths))\n",
        "\n",
        "def uidify(df):\n",
        "    ch = (df[\"chunk_id\"].fillna(-1).astype(\"int64\").astype(str)) if \"chunk_id\" in df else pd.Series([\"row\"+str(i) for i in range(len(df))])\n",
        "    return df[\"id\"].astype(str) + \"::c\" + ch\n",
        "\n",
        "uids_all=[]\n",
        "for i,mp in enumerate(mpaths):\n",
        "    df=pq.read_table(mp).to_pandas()\n",
        "    uids=uidify(df)\n",
        "    print(f\"shard {i}: rows={len(df)}, unique_uids={uids.nunique()}\")\n",
        "    uids_all.append(pd.Series(uids.values, name=f\"s{i}\"))\n",
        "\n",
        "all_uids=pd.concat(uids_all, ignore_index=True)\n",
        "print(\"GLOBAL total rows:\", len(all_uids), \"global unique UIDs:\", all_uids.nunique())\n",
        "dups = all_uids.value_counts()\n",
        "print(\"UIDs appearing in >1 shard:\", (dups>1).sum())"
      ],
      "metadata": {
        "id": "AG1lkjuFwJZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0005cb2d-5a9a-4e8a-f1f2-a2907a769fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shards: 8 8\n",
            "shard 0: rows=10184, unique_uids=10184\n",
            "shard 1: rows=10130, unique_uids=10130\n",
            "shard 2: rows=9622, unique_uids=9622\n",
            "shard 3: rows=9362, unique_uids=9362\n",
            "shard 4: rows=9666, unique_uids=9666\n",
            "shard 5: rows=10011, unique_uids=10011\n",
            "shard 6: rows=10331, unique_uids=10331\n",
            "shard 7: rows=9858, unique_uids=9858\n",
            "GLOBAL total rows: 79164 global unique UIDs: 79164\n",
            "UIDs appearing in >1 shard: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVE DEMO**"
      ],
      "metadata": {
        "id": "L7eRSCaaclsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Run a query in FAISS Single"
      ],
      "metadata": {
        "id": "wYsXODpEcpyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pipeline.runner import load_pipeline\n",
        "\n",
        "P = load_pipeline(\"configs/index/config.json\", index_key=\"multi_default\")\n",
        "P.search_one(\"Explain PageRank algorithm\", top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFIanSFVbUIy",
        "outputId": "6bc1ded1-1d9d-4884-8bc7-b8ca95892180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAISS] Using 12 threads\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rank': 1,\n",
              "  'score': 0.5379228591918945,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c13',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 13},\n",
              " {'rank': 2,\n",
              "  'score': 0.5440587401390076,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c34',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 34},\n",
              " {'rank': 3,\n",
              "  'score': 0.5530006885528564,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c32',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 32},\n",
              " {'rank': 4,\n",
              "  'score': 0.5544021129608154,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c11',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 11},\n",
              " {'rank': 5,\n",
              "  'score': 0.6044724583625793,\n",
              "  'id': '0d986bd6-5034-4824-abbc-054048f6ca1f',\n",
              "  'uid': '0d986bd6-5034-4824-abbc-054048f6ca1f::c0',\n",
              "  'source': 'pubmed',\n",
              "  'title': 'A comprehensive framework for solution space exploration in community detection.',\n",
              "  'url': 'https://pubmed.ncbi.nlm.nih.gov/41173948/',\n",
              "  'text': '',\n",
              "  'chunk_id': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Switch to FAISS Sharded"
      ],
      "metadata": {
        "id": "lp3pRqIuctUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P = load_pipeline(\"configs/index/config.json\", index_key=\"multi_sharded\")\n",
        "P.search_one(\"Explain PageRank algorithm\", top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2AsHzIebZcq",
        "outputId": "25d1fa4f-3de2-4c7b-a527-1f2f6a814f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAISS] Using 12 threads\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rank': 1,\n",
              "  'score': 0.7310386300086975,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c13',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 13},\n",
              " {'rank': 2,\n",
              "  'score': 0.7279707193374634,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c34',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 34},\n",
              " {'rank': 3,\n",
              "  'score': 0.7234997153282166,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c32',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 32},\n",
              " {'rank': 4,\n",
              "  'score': 0.7227989435195923,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c11',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 11},\n",
              " {'rank': 5,\n",
              "  'score': 0.6977638602256775,\n",
              "  'id': '0d986bd6-5034-4824-abbc-054048f6ca1f',\n",
              "  'uid': '0d986bd6-5034-4824-abbc-054048f6ca1f::c0',\n",
              "  'source': 'pubmed',\n",
              "  'title': 'A comprehensive framework for solution space exploration in community detection.',\n",
              "  'url': 'https://pubmed.ncbi.nlm.nih.gov/41173948/',\n",
              "  'text': '',\n",
              "  'chunk_id': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Switch to Chroma"
      ],
      "metadata": {
        "id": "gitQKdJTcvNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P = load_pipeline(\"configs/index/config.json\", index_key=\"multi_chroma\")\n",
        "P.search_one(\"Explain PageRank algorithm\", top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "635XU-Cmbdp6",
        "outputId": "bedd4966-8432-46e0-cd2d-472a40bba937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Chroma collection: multi_default | count=79164 | store=indices/chroma_store\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rank': 1,\n",
              "  'score': 0.46207714080810547,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c13',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 13},\n",
              " {'rank': 2,\n",
              "  'score': 0.45594125986099243,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c34',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 34},\n",
              " {'rank': 3,\n",
              "  'score': 0.44699931144714355,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c32',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 32},\n",
              " {'rank': 4,\n",
              "  'score': 0.44559788703918457,\n",
              "  'id': 'f52fac4b-452a-4449-8cb1-bae763c4f528',\n",
              "  'uid': 'f52fac4b-452a-4449-8cb1-bae763c4f528::c11',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Search engine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Search%20engine',\n",
              "  'text': '',\n",
              "  'chunk_id': 11},\n",
              " {'rank': 5,\n",
              "  'score': 0.39470237493515015,\n",
              "  'id': '0738c7de-d696-436e-9e00-c9400319cf62',\n",
              "  'uid': '0738c7de-d696-436e-9e00-c9400319cf62::c0',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Analysis of algorithms',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Analysis%20of%20algorithms',\n",
              "  'text': '',\n",
              "  'chunk_id': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Domain Hit Differences: Without any domain filter"
      ],
      "metadata": {
        "id": "Rt0g86i-c47p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P.search_one(\"side effects of metformin\", top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiRmsRWCcEhj",
        "outputId": "4c451cbe-93fc-45c8-c323-77c439cca27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rank': 1,\n",
              "  'score': 0.5025807619094849,\n",
              "  'id': '2e01f87d-d8f2-4495-8d94-3fe69606e423',\n",
              "  'uid': '2e01f87d-d8f2-4495-8d94-3fe69606e423::c12',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Amphetamine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Amphetamine',\n",
              "  'text': '',\n",
              "  'chunk_id': 12},\n",
              " {'rank': 2,\n",
              "  'score': 0.4798773527145386,\n",
              "  'id': '2e01f87d-d8f2-4495-8d94-3fe69606e423',\n",
              "  'uid': '2e01f87d-d8f2-4495-8d94-3fe69606e423::c11',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Amphetamine',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Amphetamine',\n",
              "  'text': '',\n",
              "  'chunk_id': 11},\n",
              " {'rank': 3,\n",
              "  'score': 0.4463244080543518,\n",
              "  'id': '8ec8afff-470d-4c86-979d-d8cccb358cb6',\n",
              "  'uid': '8ec8afff-470d-4c86-979d-d8cccb358cb6::c24',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Antipsychotic',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Antipsychotic',\n",
              "  'text': '',\n",
              "  'chunk_id': 24},\n",
              " {'rank': 4,\n",
              "  'score': 0.44114363193511963,\n",
              "  'id': '8ec8afff-470d-4c86-979d-d8cccb358cb6',\n",
              "  'uid': '8ec8afff-470d-4c86-979d-d8cccb358cb6::c22',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Antipsychotic',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Antipsychotic',\n",
              "  'text': '',\n",
              "  'chunk_id': 22},\n",
              " {'rank': 5,\n",
              "  'score': 0.4372667670249939,\n",
              "  'id': '3d4cbfdc-35bc-4e93-ab17-932bccf1d5b3',\n",
              "  'uid': '3d4cbfdc-35bc-4e93-ab17-932bccf1d5b3::c4',\n",
              "  'source': 'wikipedia',\n",
              "  'title': 'Dopamine agonist',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Dopamine%20agonist',\n",
              "  'text': '',\n",
              "  'chunk_id': 4}]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "applying filters"
      ],
      "metadata": {
        "id": "dvmdCBBCc7eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P.search_one(\"side effects of metformin\", top_k=5, filter_source=\"pubmed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex6tLuk9cH9o",
        "outputId": "f114fe10-1a6e-49a1-d5d9-9d24b851a72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rank': 1,\n",
              "  'score': 0.4757671356201172,\n",
              "  'id': '879c2390-c660-4fe3-8a31-f959c9c013c4',\n",
              "  'uid': '879c2390-c660-4fe3-8a31-f959c9c013c4::c0',\n",
              "  'source': 'pubmed',\n",
              "  'title': 'The beneficial effects of empagliflozin-metformin combination on cardiovascular risk factors in type 2 diabetes mellitus patients.',\n",
              "  'url': 'https://pubmed.ncbi.nlm.nih.gov/41175191/',\n",
              "  'text': '',\n",
              "  'chunk_id': 0},\n",
              " {'rank': 2,\n",
              "  'score': 0.469219446182251,\n",
              "  'id': 'ac6d737e-edaa-4421-929c-969f67d09e52',\n",
              "  'uid': 'ac6d737e-edaa-4421-929c-969f67d09e52::c0',\n",
              "  'source': 'pubmed',\n",
              "  'title': 'Randomized controlled trial of effects of metformin in NAFLD patients with newly diagnosed type 2 diabetes treated with an intensive lifestyle: a study protocol.',\n",
              "  'url': 'https://pubmed.ncbi.nlm.nih.gov/41174662/',\n",
              "  'text': '',\n",
              "  'chunk_id': 0},\n",
              " {'rank': 3,\n",
              "  'score': 0.46206068992614746,\n",
              "  'id': '8326cb28-4d8e-441a-a139-7e8b72fb09e6',\n",
              "  'uid': '8326cb28-4d8e-441a-a139-7e8b72fb09e6::c0',\n",
              "  'source': 'pubmed',\n",
              "  'title': 'Inner ear signs and symptoms induced by antidepressants: a disproportionality analysis based on the FAERS database.',\n",
              "  'url': 'https://pubmed.ncbi.nlm.nih.gov/41171397/',\n",
              "  'text': '',\n",
              "  'chunk_id': 0},\n",
              " {'rank': 4,\n",
              "  'score': 0.4568052291870117,\n",
              "  'id': '6a8b59a2-2374-4008-8e8d-2e3a27a3e744',\n",
              "  'uid': '6a8b59a2-2374-4008-8e8d-2e3a27a3e744::c0',\n",
              "  'source': 'pubmed',\n",
              "  'title': 'Benefits of glucagon-like peptide-1 receptor agonists versus pioglitazone for cardio-hepatic outcomes: a territory-wide target trial emulation.',\n",
              "  'url': 'https://pubmed.ncbi.nlm.nih.gov/41174643/',\n",
              "  'text': '',\n",
              "  'chunk_id': 0},\n",
              " {'rank': 5,\n",
              "  'score': 0.3843873143196106,\n",
              "  'id': '7ec43062-6d7a-460b-89af-5affa29e72b3',\n",
              "  'uid': '7ec43062-6d7a-460b-89af-5affa29e72b3::c0',\n",
              "  'source': 'pubmed',\n",
              "  'title': 'A Scoping Review of Glucose Spikes in People Without Diabetes: Comparing Insights from Grey Literature and Medical Research.',\n",
              "  'url': 'https://pubmed.ncbi.nlm.nih.gov/41170150/',\n",
              "  'text': '',\n",
              "  'chunk_id': 0}]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Fan-Out Results"
      ],
      "metadata": {
        "id": "KVJAVsotdKJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"results/spark_fanout_hits.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "QDFH0MPtcUyp",
        "outputId": "6f02e557-a908-489c-cc3e-dadf6184794e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                query  rank     score  \\\n",
              "0                          Explain PageRank algorithm     1  0.731039   \n",
              "1                          Explain PageRank algorithm     2  0.727971   \n",
              "2                          Explain PageRank algorithm     3  0.723500   \n",
              "3                          Explain PageRank algorithm     4  0.722799   \n",
              "4                          Explain PageRank algorithm     5  0.697764   \n",
              "..                                                ...   ...       ...   \n",
              "115  type 2 diabetes lifestyle interventions evidence     6  0.779740   \n",
              "116  type 2 diabetes lifestyle interventions evidence     7  0.774791   \n",
              "117  type 2 diabetes lifestyle interventions evidence     8  0.764265   \n",
              "118  type 2 diabetes lifestyle interventions evidence     9  0.763914   \n",
              "119  type 2 diabetes lifestyle interventions evidence    10  0.762292   \n",
              "\n",
              "                                       id  \\\n",
              "0    f52fac4b-452a-4449-8cb1-bae763c4f528   \n",
              "1    f52fac4b-452a-4449-8cb1-bae763c4f528   \n",
              "2    f52fac4b-452a-4449-8cb1-bae763c4f528   \n",
              "3    f52fac4b-452a-4449-8cb1-bae763c4f528   \n",
              "4    0d986bd6-5034-4824-abbc-054048f6ca1f   \n",
              "..                                    ...   \n",
              "115  18cd8804-db4f-4197-b1e5-b0413a06a388   \n",
              "116  2cba85e0-6ba6-4841-a397-b1d43b8482a2   \n",
              "117  7ec43062-6d7a-460b-89af-5affa29e72b3   \n",
              "118  6e738fde-2c11-4eda-ba76-284f847dc516   \n",
              "119  ac6d737e-edaa-4421-929c-969f67d09e52   \n",
              "\n",
              "                                           uid     source  \\\n",
              "0    f52fac4b-452a-4449-8cb1-bae763c4f528::c13  wikipedia   \n",
              "1    f52fac4b-452a-4449-8cb1-bae763c4f528::c34  wikipedia   \n",
              "2    f52fac4b-452a-4449-8cb1-bae763c4f528::c32  wikipedia   \n",
              "3    f52fac4b-452a-4449-8cb1-bae763c4f528::c11  wikipedia   \n",
              "4     0d986bd6-5034-4824-abbc-054048f6ca1f::c0     pubmed   \n",
              "..                                         ...        ...   \n",
              "115  18cd8804-db4f-4197-b1e5-b0413a06a388::c19  wikipedia   \n",
              "116   2cba85e0-6ba6-4841-a397-b1d43b8482a2::c0     pubmed   \n",
              "117   7ec43062-6d7a-460b-89af-5affa29e72b3::c0     pubmed   \n",
              "118   6e738fde-2c11-4eda-ba76-284f847dc516::c0     pubmed   \n",
              "119   ac6d737e-edaa-4421-929c-969f67d09e52::c0     pubmed   \n",
              "\n",
              "                                                 title  \\\n",
              "0                                        Search engine   \n",
              "1                                        Search engine   \n",
              "2                                        Search engine   \n",
              "3                                        Search engine   \n",
              "4    A comprehensive framework for solution space e...   \n",
              "..                                                 ...   \n",
              "115                            Coronary artery disease   \n",
              "116  Continuous Glucose Monitoring Frequency and Gl...   \n",
              "117  A Scoping Review of Glucose Spikes in People W...   \n",
              "118  Comprehensive impact of Intermittent Hypoxia T...   \n",
              "119  Randomized controlled trial of effects of metf...   \n",
              "\n",
              "                                                   url  chunk_id  text  \n",
              "0        https://en.wikipedia.org/wiki/Search%20engine        13   NaN  \n",
              "1        https://en.wikipedia.org/wiki/Search%20engine        34   NaN  \n",
              "2        https://en.wikipedia.org/wiki/Search%20engine        32   NaN  \n",
              "3        https://en.wikipedia.org/wiki/Search%20engine        11   NaN  \n",
              "4            https://pubmed.ncbi.nlm.nih.gov/41173948/         0   NaN  \n",
              "..                                                 ...       ...   ...  \n",
              "115  https://en.wikipedia.org/wiki/Coronary%20arter...        19   NaN  \n",
              "116          https://pubmed.ncbi.nlm.nih.gov/41171276/         0   NaN  \n",
              "117          https://pubmed.ncbi.nlm.nih.gov/41170150/         0   NaN  \n",
              "118          https://pubmed.ncbi.nlm.nih.gov/41170361/         0   NaN  \n",
              "119          https://pubmed.ncbi.nlm.nih.gov/41174662/         0   NaN  \n",
              "\n",
              "[120 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88e60eef-cda9-4738-9cc6-6a819e9c6478\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>id</th>\n",
              "      <th>uid</th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explain PageRank algorithm</td>\n",
              "      <td>1</td>\n",
              "      <td>0.731039</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528::c13</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Search engine</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Search%20engine</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Explain PageRank algorithm</td>\n",
              "      <td>2</td>\n",
              "      <td>0.727971</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528::c34</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Search engine</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Search%20engine</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Explain PageRank algorithm</td>\n",
              "      <td>3</td>\n",
              "      <td>0.723500</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528::c32</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Search engine</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Search%20engine</td>\n",
              "      <td>32</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Explain PageRank algorithm</td>\n",
              "      <td>4</td>\n",
              "      <td>0.722799</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528</td>\n",
              "      <td>f52fac4b-452a-4449-8cb1-bae763c4f528::c11</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Search engine</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Search%20engine</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Explain PageRank algorithm</td>\n",
              "      <td>5</td>\n",
              "      <td>0.697764</td>\n",
              "      <td>0d986bd6-5034-4824-abbc-054048f6ca1f</td>\n",
              "      <td>0d986bd6-5034-4824-abbc-054048f6ca1f::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>A comprehensive framework for solution space e...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41173948/</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>type 2 diabetes lifestyle interventions evidence</td>\n",
              "      <td>6</td>\n",
              "      <td>0.779740</td>\n",
              "      <td>18cd8804-db4f-4197-b1e5-b0413a06a388</td>\n",
              "      <td>18cd8804-db4f-4197-b1e5-b0413a06a388::c19</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Coronary artery disease</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Coronary%20arter...</td>\n",
              "      <td>19</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>type 2 diabetes lifestyle interventions evidence</td>\n",
              "      <td>7</td>\n",
              "      <td>0.774791</td>\n",
              "      <td>2cba85e0-6ba6-4841-a397-b1d43b8482a2</td>\n",
              "      <td>2cba85e0-6ba6-4841-a397-b1d43b8482a2::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>Continuous Glucose Monitoring Frequency and Gl...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41171276/</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>type 2 diabetes lifestyle interventions evidence</td>\n",
              "      <td>8</td>\n",
              "      <td>0.764265</td>\n",
              "      <td>7ec43062-6d7a-460b-89af-5affa29e72b3</td>\n",
              "      <td>7ec43062-6d7a-460b-89af-5affa29e72b3::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>A Scoping Review of Glucose Spikes in People W...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41170150/</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>type 2 diabetes lifestyle interventions evidence</td>\n",
              "      <td>9</td>\n",
              "      <td>0.763914</td>\n",
              "      <td>6e738fde-2c11-4eda-ba76-284f847dc516</td>\n",
              "      <td>6e738fde-2c11-4eda-ba76-284f847dc516::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>Comprehensive impact of Intermittent Hypoxia T...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41170361/</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>type 2 diabetes lifestyle interventions evidence</td>\n",
              "      <td>10</td>\n",
              "      <td>0.762292</td>\n",
              "      <td>ac6d737e-edaa-4421-929c-969f67d09e52</td>\n",
              "      <td>ac6d737e-edaa-4421-929c-969f67d09e52::c0</td>\n",
              "      <td>pubmed</td>\n",
              "      <td>Randomized controlled trial of effects of metf...</td>\n",
              "      <td>https://pubmed.ncbi.nlm.nih.gov/41174662/</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows Ã— 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88e60eef-cda9-4738-9cc6-6a819e9c6478')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88e60eef-cda9-4738-9cc6-6a819e9c6478 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88e60eef-cda9-4738-9cc6-6a819e9c6478');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-59269334-8160-480f-b04e-04200a83f4fa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59269334-8160-480f-b04e-04200a83f4fa')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-59269334-8160-480f-b04e-04200a83f4fa button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 120,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"randomized controlled trial definition\",\n          \"metformin mechanism of action\",\n          \"Explain PageRank algorithm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rank\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.040739540782760264,\n        \"min\": 0.6656452417373657,\n        \"max\": 0.8709640502929688,\n        \"num_unique_values\": 120,\n        \"samples\": [\n          0.6707258224487305,\n          0.6676146984100342,\n          0.6977638006210327\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 88,\n        \"samples\": [\n          \"93e80910-4723-4c34-8bc4-0b9185163696\",\n          \"f52fac4b-452a-4449-8cb1-bae763c4f528\",\n          \"88e02f10-e55d-4e96-8087-33c5b94c83bb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"uid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 117,\n        \"samples\": [\n          \"e13de485-bf87-445f-b69e-aaf52c18a0e9::c3\",\n          \"0d986bd6-5034-4824-abbc-054048f6ca1f::c0\",\n          \"93d03c2a-84c0-4514-a14b-0466c9ec8a45::c0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"wikipedia\",\n          \"pubmed\",\n          \"stackexchange\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 88,\n        \"samples\": [\n          \"Research waste in randomized clinical trials of degenerative spinal diseases: a cross-sectional study.\",\n          \"Search engine\",\n          \"How to define API in header and private fields in source in c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 88,\n        \"samples\": [\n          \"https://pubmed.ncbi.nlm.nih.gov/41173912/\",\n          \"https://en.wikipedia.org/wiki/Search%20engine\",\n          \"https://stackoverflow.com/questions/79774517/how-to-define-api-in-header-and-private-fields-in-source-in-c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 0,\n        \"max\": 45,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          14,\n          24,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ]
}